# 吴恩达机器学习系列课程

## 1 Introduction

### 1.1 Welcome 20210530

### 1.2 What is machine learning 

**Machine learning** algorithms:

**Supervised learning** ：给出正确的答案，让其去模仿

**Unsupervised learning**

Others: Reinforcement learning, recommender learning

### 1.3 Supervised learning

### 1.4 Unsupervised learning



## 2 Linear regression with one variable

### 2.1 Model Represation 20210607

Supervised learning

最常见的就是 classification problem

We have a data set. -> training set

本课程的Notation：

 m= Number of training examples

 x's= input variable / features

 y's= output variable / target variable

弄了个上标表示索引值，不是指幂...

### 2.2 Cost function 20210611

线性回归 Linear regression

> 什么是回归分析呢？这是一个来自统计学的概念。回归分析是指一种预测性的建模技术，主要是研究自变量和因变量的关系。通常使用线/曲线来拟合数据点，然后研究如何使曲线到数据点的距离差异最小。
>
> 线性回归是回归分析的一种。
>
> 1. 假设目标值（因变量）与特征值（自变量）之间线性相关（即满足一个多元一次方程，如：f(x)=w1x1+…+wnxn+b.）。
> 2. 然后构建损失函数。
> 3. 最后通过令损失函数最小来确定参数。（最关键的一步）

最小二乘法

> 误差，当然是真实值与拟合值的差
>
> 而误差平方和
>
> 但为啥要平方（2范数）呢？绝对值（1范数）不可以吗？
>     我们从几何角度来解释这个问题，平方方式的话，则可以视为两点间距离（这里只是没有开方），最小化就是点到直线间距离最短，当然就是垂线

用来拟合数据

Cost function is also called squared error function 

### 2.3 Cost function intuition I

拟合时，最小二乘法公式是个U型函数，最底部就是最好的拟合点。

就是说去拟合的直线称为Hypothesis  : h(x)= a+bx

然后这个最小二乘法就是Cost function : J(b) =...

### 2.4 Cost function intuition II

U型函数只考虑了一个影响变量b，如果考虑两个，即J(a,b), 最后成型的就是三维图像，像一个碗，碗底就是最佳拟合，这个图像被称为contour plots (contour figures)。

进一步思考，我们还可更多J(a,b,c...), 显而易见，之后就很难做可视化处理了。

### 2.5 Gradient descent 20210813

想象J(a,b)所代表的一个三维图像，就像一个起伏的山丘，任选一点，看周围，哪一条路带领你走山丘的最低点，然后就能得到一个局部最低点，多选几个点重复算局部最低点（贪心算法？），得到最优解。

```
a:=b  assignment
a=b   truth assertion
```

> ## 偏导数 Partial derivative
>
> 在一元函数中，导数就是函数的变化率。对于二元函数研究它的“变化率”，由于自变量多了一个，情况就要复杂的多。
>
> 在 xOy 平面内，当动点由 P(x0,y0) 沿不同方向变化时，函数 f(x,y) 的变化快慢一般说来是不同的，因此就需要研究 f(x,y) 在 (x0,y0) 点处沿不同方向的变化率。
>
> 在这里我们只学习函数 f(x,y) 沿着**平行于 x 轴和平行于 y 轴**两个特殊方位变动时， f(x,y) 的变化率。
>
> 偏导数的表示符号为:∂。
>
> 偏导数反映的是函数沿**坐标轴正方向**的变化率
>
> ## 几何意义
>
> 表示固定面上一点的切线斜率。
>
> 偏导数 f’x(x0,y0) 表示固定面上一点对 x 轴的切线斜率；偏导数 f’y(x0,y0) 表示固定面上一点对 y 轴的切线斜率。

### 2.6 Gradient descent intuition

### 2.7 Gradient descent for linear regression

convex function(凸函数)，形象说就是个碗，这就不用担心局部最优解带来的片面性了。



## 3 Linear Algebra review(optional)

### 3.1 Matrices and vectors 20210816

Matrix: 矩阵

Vector: 向量，An n x 1 matrix.

### 3.2 Addition and scalar multiplication

scalar just mean true number. => Matrix和数字相乘

### 3.3 Matrix-vector multiplication

Multiply a matrix by a vector:

m x n 与 n x 1相乘，第一个矩阵的column和第二个向量的row相等是前提，最后得出一个 m x 1的矩阵，降维？

（最后讲了一个预测房子的价格的实例来说明此种运算效率更高，代码更为简洁）？

### 3.4 Matrix-matrix muitiplication 20210820

矩阵为什么叫线性代数：矩阵可以用来计算多个线性方程，大概？

### 3.5 Matrix multiplication properties

矩阵不支持交换律（not commutative）

支持结合律（associative）

Identity matrix : 相当于数字1在乘法中的作用。

### 3.6 Inverse and transpose

在乘法中，1/12与12相乘为Identity number，所以它们inverse.

transpose:   M x N => N x M  转置

（感觉最基础的基础，就很容易理解，至于复杂的证明计算就可以调用相关库函数，所以这么感觉，Linear Algebra就不难，到头来，也就这些知道就行...数学也是站在巨人肩膀上的科目，不必穷尽每个枝干。）
