# 吴恩达机器学习系列课程

## 1 Introduction 20210530

### 1.1 Welcome 

### 1.2 What is machine learning 

**Machine learning** algorithms:

**Supervised learning** ：给出正确的答案，让其去模仿

**Unsupervised learning**

Others: Reinforcement learning, recommender learning

### 1.3 Supervised learning

### 1.4 Unsupervised learning



## 2 Linear regression with one variable

### 2.1 Model Represation 20210607

Supervised learning

最常见的就是 classification problem

We have a data set. -> training set

本课程的Notation：

 m= Number of training examples

 x's= input variable / features

 y's= output variable / target variable

弄了个上标表示索引值，不是指幂...

### 2.2 Cost function 20210611

线性回归 Linear regression

> 什么是回归分析呢？这是一个来自统计学的概念。回归分析是指一种预测性的建模技术，主要是研究自变量和因变量的关系。通常使用线/曲线来拟合数据点，然后研究如何使曲线到数据点的距离差异最小。
>
> 线性回归是回归分析的一种。
>
> 1. 假设目标值（因变量）与特征值（自变量）之间线性相关（即满足一个多元一次方程，如：f(x)=w1x1+…+wnxn+b.）。
> 2. 然后构建损失函数。
> 3. 最后通过令损失函数最小来确定参数。（最关键的一步）

最小二乘法

> 误差，当然是真实值与拟合值的差
>
> 而误差平方和
>
> 但为啥要平方（2范数）呢？绝对值（1范数）不可以吗？
>     我们从几何角度来解释这个问题，平方方式的话，则可以视为两点间距离（这里只是没有开方），最小化就是点到直线间距离最短，当然就是垂线

用来拟合数据

Cost function is also called squared error function 

### 2.3 Cost function intuition I

拟合时，最小二乘法公式是个U型函数，最底部就是最好的拟合点。

就是说去拟合的直线称为Hypothesis  : h(x)= a+bx

然后这个最小二乘法就是Cost function : J(b) =...

### 2.4 Cost function intuition II

U型函数只考虑了一个影响变量b，如果考虑两个，即J(a,b), 最后成型的就是三维图像，像一个碗，碗底就是最佳拟合，这个图像被称为contour plots (contour figures)。

进一步思考，我们还可更多J(a,b,c...), 显而易见，之后就很难做可视化处理了。

### 2.5 Gradient descent 20210813

想象J(a,b)所代表的一个三维图像，就像一个起伏的山丘，任选一点，看周围，哪一条路带领你走山丘的最低点，然后就能得到一个局部最低点，多选几个点重复算局部最低点（贪心算法？），得到最优解。

```
a:=b  assignment
a=b   truth assertion
```

> ## 偏导数 Partial derivative
>
> 在一元函数中，导数就是函数的变化率。对于二元函数研究它的“变化率”，由于自变量多了一个，情况就要复杂的多。
>
> 在 xOy 平面内，当动点由 P(x0,y0) 沿不同方向变化时，函数 f(x,y) 的变化快慢一般说来是不同的，因此就需要研究 f(x,y) 在 (x0,y0) 点处沿不同方向的变化率。
>
> 在这里我们只学习函数 f(x,y) 沿着**平行于 x 轴和平行于 y 轴**两个特殊方位变动时， f(x,y) 的变化率。
>
> 偏导数的表示符号为:∂。
>
> 偏导数反映的是函数沿**坐标轴正方向**的变化率
>
> ## 几何意义
>
> 表示固定面上一点的切线斜率。
>
> 偏导数 f’x(x0,y0) 表示固定面上一点对 x 轴的切线斜率；偏导数 f’y(x0,y0) 表示固定面上一点对 y 轴的切线斜率。

### 2.6 Gradient descent intuition

### 2.7 Gradient descent for linear regression

convex function(凸函数)，形象说就是个碗，这就不用担心局部最优解带来的片面性了。



## 3 Linear Algebra review(optional)

### 3.1 Matrices and vectors 20210816

Matrix: 矩阵

Vector: 向量，An n x 1 matrix.

### 3.2 Addition and scalar multiplication

scalar just mean true number. => Matrix和数字相乘

### 3.3 Matrix-vector multiplication

Multiply a matrix by a vector:

m x n 与 n x 1相乘，第一个矩阵的column和第二个向量的row相等是前提，最后得出一个 m x 1的矩阵，降维？

（最后讲了一个预测房子的价格的实例来说明此种运算效率更高，代码更为简洁）？

### 3.4 Matrix-matrix muitiplication 20210820

矩阵为什么叫线性代数：矩阵可以用来计算多个线性方程，大概？

### 3.5 Matrix multiplication properties

矩阵不支持交换律（not commutative）

支持结合律（associative）

Identity matrix : 相当于数字1在乘法中的作用。

### 3.6 Inverse and transpose

在乘法中，1/12与12相乘为Identity number，所以它们inverse.

transpose:   M x N => N x M  转置

（感觉最基础的基础，就很容易理解，至于复杂的证明计算就可以调用相关库函数，所以这么感觉，Linear Algebra就不难，到头来，也就这些知道就行...数学也是站在巨人肩膀上的科目，不必穷尽每个枝干。）



## 4 Linear Regression with multiple variables

### 4.1 Multiple features 20210821

(多元线性回归->multivariate linear regression, 多元线性方程组)

### 4.2 Gradient descent for multiple variables

How to fit parameterof the hypothesis

### 4.3 Gradient descent in practice I: Feature Scaling

scaling -> 缩放

 Feature Scaling不必太精确，它只是让Gradient descent更快下降，收敛的迭代次数更少(converge in a lot fewer iterations)

Mean Normalization : 均值归一化

（不求甚解的心态看一看，了解一下）

### 4.4 Gradient descent in practice II: Learning rate

> 简单的说
> 有极限（极限不为无穷）就是收敛，没有极限（极限为无穷）就是发散。
> 例如：f（x）=1/x 当x趋于无穷是极限为0，所以收敛。
> f（x）= x 当x趋于无穷是极限为无穷，即没有极限，所以发散

Learning rate 形象点可以看成下降的步伐，调大一些，下降快一些。但是过大的话，会导致越过最小值，然后在一个U行中反复横跳，最终无法收敛。

弹幕说有自动调节学习率的算法，先大后小。

### 4.5 Features and polynomial regression 20210822

polynomial -> 多项式

举例来说，一个cost function有两个features：长度和宽度，那你直接可以看成一个feature：面积；

另外，如果是一个三次多项式，比如长度的三次，二次，一次，你可以视为体积，面积，长度三个一次方的features，这就转化为了线性回归。

如上所见，为了拟合数据，定义feature是相对自由的。

### 4.6 Normal equation

Method to solve for 参数 analytically.

相对于Gradient descent的多步迭代，Normal equation类似于二次函数求导为0即为极值来一步到位。

> 解决约束优化问题——拉格朗日乘数法（Lagrange Multiplier Method）
>
> 作为一种优化算法，拉格朗日乘子法主要用于解决约束优化问题，它的基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。拉格朗日乘子背后的数学意义是其为约束方程梯度线性组合中每个向量的系数。
>
> 　　如何将一个含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题？拉格朗日乘数法从数学意义入手，通过引入拉格朗日乘子建立极值条件，对n个变量分别求偏导对应了n个方程，然后加上k个约束条件（对应k个拉格朗日乘子）一起构成包含了（n+k）变量的（n+k）个方程的方程组问题，这样就能根据求方程组的方法对其进行求解。
> ————————————————
> 版权声明：本文为CSDN博主「-柚子皮-」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/pipisorry/article/details/52135854

### 4.7 Normal equation and non-invertibility(optional)

non-invertibility -> 不可逆性



## 5 Octave Tutorial 20210825

### 5.1 Basic operations

现在多用Python甚于Octave.

> **NumPy是一个关于矩阵运算的库**，熟悉Matlab的都应该清楚，这个库就是让python能够进行矩阵话的操作，而不用去写循环操作。
>
> numpy包含两种基本的数据类型：数组和矩阵。
>
> ```python
> #创建矩阵
> >>> m=mat([1,2,3])
> >>> m
> matrix([[1, 2, 3]])
> 
> #取值
> >>> m[0]                #取一行
> matrix([[1, 2, 3]])
> >>> m[0,1]              #第一行，第2个数据
> 2
> >>> m[0][1]             #注意不能像数组那样取值了
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   ...
> IndexError: index 1 is out of bounds for axis 0 with size 1
> 
> #将Python的列表转换成NumPy的矩阵
> >>> list=[1,2,3]
> >>> mat(list)
> matrix([[1, 2, 3]])
> 
> #矩阵相乘
> >>> m1=mat([1,2,3])     #1行3列
> >>> m2=mat([4,5,6]) 
> >>> m1*m2.T             #注意左列与右行相等 m2.T为转置操作
> matrix([[32]])       
> >>> multiply(m1,m2)     #执行点乘操作，要使用函数，特别注意
> matrix([[ 4, 10, 18]])  
> 
> #排序
> >>> m=mat([[2,5,1],[4,6,2]])    #创建2行3列矩阵
> >>> m
> matrix([[2, 5, 1],
>         [4, 6, 2]])
> >>> m.sort()                    #对每一行进行排序
> >>> m
> matrix([[1, 2, 5],
>         [2, 4, 6]])
> ————————————————
> 版权声明：本文为CSDN博主「yqtaowhu」的原创文章，遵循CC 4.0 BY-SA版权协议，
> 转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/taoyanqi8932/article/details/52703686
> ```



## 6 Logistic Regression 20210829

### 6.1 Classification

说明了用linear regression用于分类问题不是 a good idea.

Logistic Regression is actually a classification algorithm. (不要被它的名字弄confused，只是historical reasons)

### 6.2 Hypothesis Representation 

Logistic function让函数收敛到0到1之间。

### 6.3 Decision Boundary

The decision boundary is a property not of the training set, but of the hypothesis and of the parameters. So long as we've given my parameter vector, that defines the decision boundary.

这个decision boundary可视化的话，可以是坐标系一条划分positive features和negative features的直线，也可以是圆，并不一定。

### 6.4 Cost Function

注意 Hypothesis Representation 与 cost function的区别！前者是去拟合数据的，后者是基于最小二乘法基础上去修正前者的，如果预测值错误，要给出cost penalty.

### 6.5 Simplified cost function and gradient descent

### 6.6 Advanced optimization

介绍了gradient descent之外的一些方法来minimize the cost function.

实现细节只有专家清楚，把库拿来用就行。

### 6.7 Multi-class classification: One-vs-all

之前都是binary classification problem.

"one versus rest"



## 7 Regularization 20210904

### 7.1 The problem of overfitting 

过度拟合，高阶函数振荡，七转八弯

### 7.2 Cost Function

不是直接扔掉高阶项features，而是在cost function中加上高阶项features的平方来进行penalizing，从而尽可能弱化高阶项特征。

### 7.3 Regularized linear regression

### 7.4 Regularized logstic regression

