# 吴恩达机器学习系列课程

## 1 Introduction 20210530

### 1.1 Welcome 

### 1.2 What is machine learning 

**Machine learning** algorithms:

**Supervised learning** ：给出正确的答案，让其去模仿

**Unsupervised learning**

Others: Reinforcement learning, recommender learning

### 1.3 Supervised learning

### 1.4 Unsupervised learning



## 2 Linear regression with one variable

### 2.1 Model Represation 20210607

Supervised learning

最常见的就是 classification problem

We have a data set. -> training set

本课程的Notation：

 m= Number of training examples

 x's= input variable / features

 y's= output variable / target variable

弄了个上标表示索引值，不是指幂...

### 2.2 Cost function 20210611

线性回归 Linear regression

> 什么是回归分析呢？这是一个来自统计学的概念。回归分析是指一种预测性的建模技术，主要是研究自变量和因变量的关系。通常使用线/曲线来拟合数据点，然后研究如何使曲线到数据点的距离差异最小。
>
> 线性回归是回归分析的一种。
>
> 1. 假设目标值（因变量）与特征值（自变量）之间线性相关（即满足一个多元一次方程，如：f(x)=w1x1+…+wnxn+b.）。
> 2. 然后构建损失函数。
> 3. 最后通过令损失函数最小来确定参数。（最关键的一步）

最小二乘法

> 误差，当然是真实值与拟合值的差
>
> 而误差平方和
>
> 但为啥要平方（2范数）呢？绝对值（1范数）不可以吗？
>     我们从几何角度来解释这个问题，平方方式的话，则可以视为两点间距离（这里只是没有开方），最小化就是点到直线间距离最短，当然就是垂线  （=》想不大通...）

用来拟合数据

Cost function is also called squared error function 

### 2.3 Cost function intuition I

拟合时，最小二乘法公式是个U型函数，最底部就是最好的拟合点。

就是说去拟合的直线称为Hypothesis  : h(x)= a+bx

然后这个最小二乘法就是Cost function : J(b) =n(h(x)-y)^2...

（所以所谓的feature还是指a,b...吧=>ERROR! feature是指x！）

（而这里的x,y因为有训练集，反而是可以看成常数...）

### 2.4 Cost function intuition II

U型函数只考虑了一个影响变量b，如果考虑两个，即J(a,b), 最后成型的就是三维图像，像一个碗，碗底就是最佳拟合，这个图像被称为contour plots (contour figures)。

进一步思考，我们还可更多J(a,b,c...), 显而易见，之后就很难做可视化处理了。

### 2.5 Gradient descent 20210813

想象J(a,b)所代表的一个三维图像，就像一个起伏的山丘，任选一点，看周围，哪一条路带领你走山丘的最低点，然后就能得到一个局部最低点，多选几个点重复算局部最低点（贪心算法？），得到最优解。

```
a:=b  assignment
a=b   truth assertion
```

> ## 偏导数 Partial derivative
>
> 在一元函数中，导数就是函数的变化率。对于二元函数研究它的“变化率”，由于自变量多了一个，情况就要复杂的多。
>
> 在 xOy 平面内，当动点由 P(x0,y0) 沿不同方向变化时，函数 f(x,y) 的变化快慢一般说来是不同的，因此就需要研究 f(x,y) 在 (x0,y0) 点处沿不同方向的变化率。
>
> 在这里我们只学习函数 f(x,y) 沿着**平行于 x 轴和平行于 y 轴**两个特殊方位变动时， f(x,y) 的变化率。
>
> 偏导数的表示符号为:∂。
>
> 偏导数反映的是函数沿**坐标轴正方向**的变化率
>
> ## 几何意义
>
> 表示固定面上一点的切线斜率。
>
> 偏导数 f’x(x0,y0) 表示固定面上一点对 x 轴的切线斜率；偏导数 f’y(x0,y0) 表示固定面上一点对 y 轴的切线斜率。

### 2.6 Gradient descent intuition

### 2.7 Gradient descent for linear regression

convex function(凸函数)，形象说就是个碗，这就不用担心局部最优解带来的片面性了。



## 3 Linear Algebra review(optional)

### 3.1 Matrices and vectors 20210816

Matrix: 矩阵

Vector: 向量，An n x 1 matrix.

### 3.2 Addition and scalar multiplication

scalar just mean true number. => Matrix和数字相乘

### 3.3 Matrix-vector multiplication

Multiply a matrix by a vector:

m x n 与 n x 1相乘，第一个矩阵的column和第二个向量的row相等是前提，最后得出一个 m x 1的矩阵，降维？

（最后讲了一个预测房子的价格的实例来说明此种运算效率更高，代码更为简洁）？

### 3.4 Matrix-matrix muitiplication 20210820

矩阵为什么叫线性代数：矩阵可以用来计算多个线性方程，大概？

### 3.5 Matrix multiplication properties

矩阵不支持交换律（not commutative）

支持结合律（associative）

Identity matrix : 相当于数字1在乘法中的作用。

### 3.6 Inverse and transpose

在乘法中，1/12与12相乘为Identity number，所以它们inverse.

transpose:   M x N => N x M  转置

（感觉最基础的基础，就很容易理解，至于复杂的证明计算就可以调用相关库函数，所以这么感觉，Linear Algebra就不难，到头来，也就这些知道就行...数学也是站在巨人肩膀上的科目，不必穷尽每个枝干。）



## 4 Linear Regression with multiple variables

### 4.1 Multiple features 20210821

(多元线性回归->multivariate linear regression, 多元线性方程组)

### 4.2 Gradient descent for multiple variables

How to fit parameter of the hypothesis

### 4.3 Gradient descent in practice I: Feature Scaling

scaling -> 缩放

 Feature Scaling不必太精确，它只是让Gradient descent更快下降，收敛的迭代次数更少(converge in a lot fewer iterations)

Mean Normalization : 均值归一化

（不求甚解的心态看一看，了解一下）

### 4.4 Gradient descent in practice II: Learning rate

> 简单的说
> 有极限（极限不为无穷）就是收敛，没有极限（极限为无穷）就是发散。
> 例如：f（x）=1/x 当x趋于无穷是极限为0，所以收敛。
> f（x）= x 当x趋于无穷是极限为无穷，即没有极限，所以发散

Learning rate 形象点可以看成下降的步伐，调大一些，下降快一些。但是过大的话，会导致越过最小值，然后在一个U行中反复横跳，最终无法收敛。

弹幕说有自动调节学习率的算法，先大后小。

### 4.5 Features and polynomial regression 20210822

polynomial -> 多项式

举例来说，一个cost function有两个features：长度和宽度，那你直接可以看成一个feature：面积；

另外，如果是一个三次多项式，比如长度的三次，二次，一次，你可以视为体积，面积，长度三个一次方的features，这就转化为了线性回归。

如上所见，为了拟合数据，定义feature是相对自由的。

（所以多个种类的x可以转化为同一个x的多项式）

### 4.6 Normal equation

Method to solve for 参数 analytically.

相对于Gradient descent的多步迭代，Normal equation类似于二次函数求导为0即为极值来一步到位。

> 解决约束优化问题——拉格朗日乘数法（Lagrange Multiplier Method）
>
> 作为一种优化算法，拉格朗日乘子法主要用于解决约束优化问题，它的基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。拉格朗日乘子背后的数学意义是其为约束方程梯度线性组合中每个向量的系数。
>
> 　　如何将一个含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题？拉格朗日乘数法从数学意义入手，通过引入拉格朗日乘子建立极值条件，对n个变量分别求偏导对应了n个方程，然后加上k个约束条件（对应k个拉格朗日乘子）一起构成包含了（n+k）变量的（n+k）个方程的方程组问题，这样就能根据求方程组的方法对其进行求解。
> ————————————————
> 版权声明：本文为CSDN博主「-柚子皮-」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/pipisorry/article/details/52135854

### 4.7 Normal equation and non-invertibility(optional)

non-invertibility -> 不可逆性



## 5 Octave Tutorial 20210825

### 5.1 Basic operations

现在多用Python甚于Octave.

> **NumPy是一个关于矩阵运算的库**，熟悉Matlab的都应该清楚，这个库就是让python能够进行矩阵话的操作，而不用去写循环操作。
>
> numpy包含两种基本的数据类型：数组和矩阵。
>
> ```python
> #创建矩阵
> >>> m=mat([1,2,3])
> >>> m
> matrix([[1, 2, 3]])
> 
> #取值
> >>> m[0]                #取一行
> matrix([[1, 2, 3]])
> >>> m[0,1]              #第一行，第2个数据
> 2
> >>> m[0][1]             #注意不能像数组那样取值了
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   ...
> IndexError: index 1 is out of bounds for axis 0 with size 1
> 
> #将Python的列表转换成NumPy的矩阵
> >>> list=[1,2,3]
> >>> mat(list)
> matrix([[1, 2, 3]])
> 
> #矩阵相乘
> >>> m1=mat([1,2,3])     #1行3列
> >>> m2=mat([4,5,6]) 
> >>> m1*m2.T             #注意左列与右行相等 m2.T为转置操作
> matrix([[32]])       
> >>> multiply(m1,m2)     #执行点乘操作，要使用函数，特别注意
> matrix([[ 4, 10, 18]])  
> 
> #排序
> >>> m=mat([[2,5,1],[4,6,2]])    #创建2行3列矩阵
> >>> m
> matrix([[2, 5, 1],
>         [4, 6, 2]])
> >>> m.sort()                    #对每一行进行排序
> >>> m
> matrix([[1, 2, 5],
>         [2, 4, 6]])
> ————————————————
> 版权声明：本文为CSDN博主「yqtaowhu」的原创文章，遵循CC 4.0 BY-SA版权协议，
> 转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/taoyanqi8932/article/details/52703686
> ```



## 6 Logistic Regression 20210829

### 6.1 Classification

说明了用linear regression用于分类问题不是 a good idea.

Logistic Regression is actually a classification algorithm. (不要被它的名字弄confused，只是historical reasons)

### 6.2 Hypothesis Representation 

Logistic function让函数收敛到0到1之间。

（所以Logistic function是Hypothesis Representation，而不是cost function）

### 6.3 Decision Boundary

The decision boundary is a property not of the training set, but of the hypothesis and of the parameters. So long as we've given my parameter vector, that defines the decision boundary.

这个decision boundary可视化的话，可以是坐标系一条划分positive features和negative features的直线，也可以是圆，并不一定。

### 6.4 Cost Function

注意 Hypothesis Representation 与 cost function的区别！前者是去拟合数据的，后者是基于最小二乘法基础上去修正前者的，如果预测值错误，要给出cost penalty.

### 6.5 Simplified cost function and gradient descent

### 6.6 Advanced optimization

介绍了gradient descent之外的一些方法来minimize the cost function.

实现细节只有专家清楚，把库拿来用就行。

### 6.7 Multi-class classification: One-vs-all

之前都是binary classification problem.

"one versus rest"



## 7 Regularization 20210904

### 7.1 The problem of overfitting 

过度拟合，高阶函数振荡，七转八弯

### 7.2 Cost Function

不是直接扔掉高阶项features，而是在cost function中加上高阶项features的平方来进行penalizing，从而尽可能弱化高阶项特征。

### 7.3 Regularized linear regression

### 7.4 Regularized logstic regression



## 8 Neural Networks: Representation

### 8.1 Non-linear hypothesis 20210905

所谓的神经网络

识别汽车图像需要上百万个features，这对于基于logistic regression上的classifier来说，hypothesis representation的polynomial term会无比复杂，并不现实。

### 8.2 Neurons and the brain

神经元

### 8.3 Model representation I

最简单的计算单元，两层：input layer和output layer，就是features和parameters的矩阵乘积而来的多项式。

多层的神经网络的话，中间的就被称为hidden layer.

### 8.4 Model representation II 20210910

最简单的两层神经网络就等同于Logistic Regression，多层的神经网络也可以看成features由隐藏层计算而来的Logistic Regression.  理解这个计算过程的Forward Propagation.

### 8.5 Examples and intuitions I

用来拟合非线性分类问题

How single nuerons in a nueral network can be used to compute logical function.

就是神经元函数怎么去模拟逻辑上的AND和OR.

### 8.6 Examples and intuitions II

同样可以实现NOT，用神经元函数实现输入1，输出0.

最后举例一个识别手写数字的神经网络，每一层神经网络通过提取features层层递进使成像更为清晰，最终被Logistic Regression Classifies识别出来。

### 8.7 Multi-class classfication

把output layer搞成多个输出口。



## 9 Neural Networks: Learning

### 9.1 Cost function 20210912

神经网络的cost function基本上就是一般化Logistic regression的cost function.

### 9.2 Backpropagation algorithm

顾名思义，就是从out layer开始计算误差，然后依次往回推到input layer.

这个算法是用来求解上一节的cost function. 看起来步骤繁多，相当复杂，如果编程实现，你都没法理解基本设计...

### 9.3 Backpropagation intuition

对神经网络的数学模型，先有一个training set，挑取两个input值，肯定可以越过数学模型先确定output值，然后通过数学模型计算output值（最后最终输出函数跟logistic regression一样是sigmoid函数），这两个output值之间一开始肯定有相当大的误差，这时候就要调图上层与层之间的权重了。那么如何求这个权重就是backpropagation algorithm的作用了，从输出层往后推，各种求偏导，计算每个权重对结果误差的影响，反复迭代，最后得出基于training set的数学模型。

### 9.4 Implementation note: Unrolling parameter

advanced optimization

相对于logistic regression，神经网络的参数不是向量，而是matrices。

### 9.5 Gradient checking 20210920

用backpropagation推出来的cost function可能有bug，这时候需要用一种近似方法去验证，也是涉及求导，然而对于两者求导的细节不甚清楚，所以也就云里雾里... 后者应该跟之前提到的gradient descent有关联。(identical)

拉格朗日中值定理：连续可导函数，通过求一段区域的平均值来近拟其中一点的导数。

backpropagation算法 相对 gradient decent算法而言，对于计算机计算量小很多，所以后者一般只用来验证一次前者有没有bug，然后就关闭掉。

### 9.6 Random initialization

开始学习前，神经网络初始的参数matrices也是有要求的...

### 9.7 Putting it together

对于一个neural architecture而言，一开始要选择default的各项数据，比如hidden layer，普遍来说就一层。如果是多层的情况，那么每一层的hidden unites数应该相同。（在数学之美中也提到，神经网络相对贝叶斯网络而言，模型标准建构上更为完善）

Training a neural network:

1. Randomly initialize weights.
2. Implement forward propagation to get f(x) for any x.
3. Implement code to computer cost function J(...).
4. Implement backprop to compute partial derivatives of J(...).
5. Use gradient checking to compare J(...) computed using backpropagation vs. using numerical estimate of gradient of J(...). Then disable gradient checking code.
6. Use gradient descent or advanced optimization method with backpropagation to try to minimize J(...).

以上6看来，9.5节就理解错误了... gradient descent不是和backpropagation同个级别的概念，而是说，backpro用来计算cost function，然后gradient descent基于backpro的计算结果来引导优化backpro计算迭代的方向。

 backpropagation跟linear regression和 logistic regression算是同一个级别的概念，而gradient descent是用来优化这些算法的。

### 9.8 Autonomous driving example

展示了一段1992年的训练视频...



## 10 Advice for applying maching learning

### 10.1 Decide what to try next 20210922

training set不是越多越好，要把时间用在刀刃上

### 10.2 Evaluating a hypothesis

按7：3讲data set分成training set和test set.

用来防止overfitting.

### 10.3 Model selection

polynomial的次数选择上，最简单的就是一次的linear regression.

关于model selection, 就是基于training set来minimize各个次数的polynomial model, 一般来说都能minimize的一个相同的误差(不是的，次数越高误差越小，然而会存在overfitting问题)，然后再依次用test set去验证，选取一个表现较好的model.

但是以上选择还是不够generalization，对于从未见过的数据呢？

另一种分法：按6：2：2将data set分成training set, cross validation set, test set.

相应可以定义 training set error, cross validation set error, test set error.

cross validation set用来select model, 而test set用来estimate generalization error.

### 10.4 Diagnosis bias vs. variance

随着order增大，training set error 越来越小， 而cross validation error 会是一个U型，两端误差很大，分别是underfit 和 overfit 问题导致的。

high **Bias** problem：low order polynomial 导致的underfit 问题；

high **Variance** problem：high order polynomial 导致的overfit 问题.

> 偏差：度量了学习算法的期望预期与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。把模型比喻成一支猎枪，预测的目标是靶心，假设射手不会手抖且视力正常，那么这支枪(模型)的能力就可以用多次射击后的中心(相当于预测值的期望，即和靶心的距离来衡量(偏离了靶心有多远)。当猎枪(模型)和子弹(样本)质量都很好时，就能得到方差和偏差都比较低的结果。但是如果猎枪是没有校准的或者目标超出有效射程，那么偏差就会更高(击中点离靶心比较远)。子弹(样本)也可能出问题，比如子弹的形状、重量等因素，导致瞄准一个点多次射击在靶上会散开一片，这就是高方差的情况。
>
> 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，反映了在不同样本集上模型输出值的变异性，方差的大小反应了样本在总体数据中的代表性，或者说不同样本下模型预测的稳定性。即刻画了数据扰动所造成的的影响。比如现在要通过一些用户属性去预测其消费能力，结果有两个样本，一个样本中大多数都是高等级活跃会员，另一个则是大部分是低质量用户，两个样本预测出来的数据的差异就非常大，也就是模型在两个样本上的方差很大。如果模型在多个样本下的训练误差(经验损失)“抖动”比较厉害，则有可能是样本有问题。
>
> 噪声：表达了在当前任务上学习算法所能达到的期望泛化误差的下界（即模型学习的上限），即刻画了学习问题本身的难度。不可控的错误很难避免，这被称为不可约偏差(irreducible error)，即噪声无法通过模型来消除。噪声通常是出现在“数据采集”的过程中的，且具有随机性和不可控性，比如采集用户数据的时候仪器产生的随机性偏差、或者在实验中受到其他不可控因素的干扰等。
>
> low bias and low variance：又准又稳
> low bias and high variance： 准但不稳
> high bias and low variance：不准但稳
> high bias and high variance：不准又不稳
>
> ————————————————
> 版权声明：本文为CSDN博主「bigdata老司机」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/yawei_liu1688/article/details/107790461

### 10.5 Regularization and bias/variance 20210925

不是直接扔掉高阶项features，而是在cost function中加上高阶项features的平方来进行penalizing，从而尽可能弱化高阶项特征。

=>对上面这句话的理解：cost function里加上高阶项features的平方的总和，会导致如果这个总和值大的话，cost function就会变大，所以拟合模型的时候要尽量去减小这些features值，高阶项features值变小的话，就会避免overfitting的问题，这就是所谓的regularization.

因为以上关系：Bias/variance as a function of the regularizatiom parameter lambda.

### 10.6 Learning curves

error与training set size 相关的曲线，前者分为 training ser error和 cross validation error,  high bias和high variance的这个learning curves都有相关特点，由此可以判断 if a learning algorithm are suffering high bias or hign variance.

### 10.7 Revisited



## 11 Machine learning system design 20211002

### 11.1 Prioritizing what to work on 

Spam classification example

### 11.2 Error analysis

### 11.3 Error metrics for skewed classes

不对称分类的误差评估：比如说预测癌症误差1%，然后癌症率本身只有0.5%，事实上做一个总是预测false的非机器学习模型它的误差反而只有0.5%，一般概念上的误差评估法就不适合这种情况，所以需要引入以下新的评估方式：

precision and recall

precision: 预测为true的值中正真为true的值所占的比例

recall：正真为true的值中被 预测为true的值所占的比例

precision and recall都高，说明算法可信度高

### 11.4 Trading off precision and recall

precision: 宁可放过一百也不错杀一个

recall: 宁可错杀一百也不放过一个

关键就是预测概率临界值（超过这临界概率->threshold就调成true），调高，precision高，相反recall高，两者相反。

可以用PR/(P+R)的类似并联电阻的数学式来权衡两者，评价算法优劣

### 11.5 Data for machine learning

有这样一个观点：

It's not who has the best algorithm that wins, it's who has the most data.



## 12 Support Vector Machines

### 12.1 Optimization objective 20211010

在Logistic Regression基础上推出了SVM的代价函数？？

### 12.2 Large Margin Intuition

SVM又被称为 Large Margin Classifier.

何谓Margin，就是间距，对于一些training set, 跟Logistic Regression一样，需要一条decision boundary来划分正负，而SVM一般趋向于选择样本群间隔比较中间的一条，即既不过分靠近positive，也不过分靠近negitive，从而来说Margin比较大。

在SVM的Cost function上来体现就是，不同于Logistic Regression的非黑即白，SVM是一个缓冲区来代替那条非黑即白的逻辑线的，这就使其更为Robust.

### 12.3 The mathematics behind  large Margin Classifier(optional)

(那就skip吧...)

### 12.4 Kernels I 20211101

Gaussian kernel(高斯核函数)

举例说面对一个比较刁钻的分界线，拟合时有可能回去追求单feature的high order polynomial, 这个时候可以提出来另一种思路，即创造更多的feature的低阶函数去拟合会不会更好？

这里的关键在于如何基于既存的一个feature去凭空创造多个feature呢？

GIven x, compute new feature depending in proximity to landmark.

利用similarity function，以既存的x和landmark为因变量计算新的features。

而视频中所采用的similarity function被称为Gaussian kernel(高斯核函数)。

这里的landmark实质上指既存的假设是true的点的坐标，换句话说，当新的坐标接近于其中一个landmark，那么它也被认为大概率接近于true。

（留下一个问题，先前的features还保留，还是直接遗弃？）=>被替换掉了

以上都是基于SVM模型算法来说的，当然也可以用于Logistic Regression等其它算法，似乎效果不太好

### 12.5 Kernel II 

### 12.6 Using a SVM

SVM不用自己写，using software packeage, call a libary function.



## 13 Clustering

### 13.1 Unsupervised learning introduction 20211105

Clustering algorithm 物以类聚，没有额外的诸如true/false的标签信息

### 13.2 K-means algorithm

1. cluster assignment step   =>计算各点到初始化的聚集点的距离总和
2. move centroid step =>将第一步中的总和最小化，从而使聚合点（多个）移动

### 13.3 Optimization objection 20211107

### 13.4 Random initialization

### 13.5 Choosing the number of clusters 

###  

## 14 Dimensionality Reduction

### 14.1 Motivation I: Data Compression 20211108 

reduce data from 2D to 1D:  xy平面斜线，投射到x轴，去掉y轴信息，起节约内存作用

扩展开来，其实从1000维开始减...（虽然没法可视化，原理一样）

以上就是Dimensionality Reduction

### 14.2 Motivation II: Data Visualization

50D的数据集，通过降维算法降到2D，你就可以可视化。

通过降维算法形成的新features，需要去理解

### 14.3 Principal Component Anylasis(PCA) problem formulation 20211110

The goal of PCA if we want to reduce data from two-dimensional to one dimensional is we're trying to find a vector, I'm going to find direction onto which to project the data so as to minimize the squared projection error.

核心思想：最小化投影误差。PCA算的是垂直低维向量的距离

 二维到一维画个坐标很容易理解，进一步扩展到从M维降到N维的PCAsuanfa。



注意：PCA is not a linear regression. 看上去形式类似，实则目的截然不同

从坐标图可以理解，PCA是点投射到斜线的垂直距离，而线性回归（即用斜线去拟合数据集）算的是点到线平行于y轴的高度差。

### 14.4 Principal Component Anylasis(PCA) algorithm

奇异值分解：singular value decomposition(SVD)

> 一、特征值与特征向量的几何意义
>
> 1.     矩阵乘法
>
> 在介绍特征值与特征向量的几何意义之前，先介绍矩阵乘法的几何意义。
>
> 矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度的新向量。在这个变化过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某些向量只发生伸缩变换，不产生旋转效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。
>
> 2.   特征值分解与特征向量
>
> 如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：
>
> ```
> Av=λv 
> ```
>
> λ为特征向量 v 对应的特征值。特征值分解是将一个矩阵分解为如下形式：
>
> ```
> A=QΣQ^-1
> ```
>
>
> 其中，Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵A的信息可以由其特征值和特征向量表示。
>
> 对于矩阵为高维的情况下，那么这个矩阵就是高维空间下的一个线性变换。可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。
>
> 总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
>
> 二、奇异值分解
>
> 1.     奇异值
>
> 特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法
>
> 2.   奇异值与特征值
>
> 那么奇异值和特征值是怎么对应起来的呢？
>
> 奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r（ r远小于m、n ）个的奇异值来近似描述矩阵，即部分奇异值分解
>
> 三、PCA主成份分析
>
> 主成分分析(PrincipalComponents Analysis。即PCA，也称为K-L变换)，是图像压缩中的一种最优正交变换。PCA用于统计特征提取构成了子空间法模式识别的基础。它从图像整体代数特征出发，基于图像的总体信息进行分类识别。PCA的核心思想是利用较少数量的特征对样本进行描述以达到降低特征空间维数的目的。
>
> 上面讲了一大堆，就是为了下一篇PCA人脸识别做铺垫的，给你一副图像，要从图像库中得到匹配的图像，怎么弄？如果是两两做像素点比较是不可能完成的任务，时间上废掉了。如果用其他特征点代替也许可以，但容易漏检吧，这边不扩展。我们必须对图像数据的协方差矩阵进行降维，所以用到了PCA。
>
> 而具体如何实现PCA呢？关键是特征值及相应特征向量的求取。matlab有个eig函数，opencv也有相应的函数。由于不想被别人牵制，我自己查了资料，发现QR算法可以用来求实对称矩阵的全部特征值和特征向量。【雅可比算法也可以，就是速度太慢了；而上面介绍的SVD实现PCA还没见过，文献上说SVD和PCA是等价的】
> ————————————————
> 版权声明：本文为CSDN博主「taotao1233」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
> 原文链接：https://blog.csdn.net/jinshengtao/article/details/18448355

### 14.5 Choosing the number of principle components 20211111

### 14.6 Reconstruction from compressed representation

所谓解压缩，低维到高维，只能近似还原。比如说一维还原到二维，点全还原到原来的斜线上，但是点本来距离斜线的距离这个信息就被丢失了。

### 14.7 Advice for applying PCA

Application of PCA

1. Compression

   Reduce memory/disk needed to store data

   Speed up learning algorithm

2. Visualizatiom

Using PCA to prevent over-fitting ,that is not a good use of PCA.



## 15 Anomaly detection

### 15.1 Problem motivation 20211115

比如网站察觉异常登录，然后给这些异常登录发验证信息；

又比如数据中心machine cluster中察觉哪一台电脑运行不正常，预防宕机...

### 15.2 Gaussian distribution

 is also called the normal distribution(正态分布)=》就是概率论中的概念

### 15.3 Algorithm

Anomaly detection algorithm

1. Choose features that you think might be indicative of anomalous example
2. Fit parameters of Gaussian distribution
3. Given new example, compute if it is amomalous

 （推测：x结合feature得到y，然后对于y，来fit正态分布）

### 15.4 Developing and evaluating an anomaly detection system 20211117

training set, cross validation set, test set  (只针对 unsupervised learning？不是吧... =>是因为Anomaly detection归类于unsupervised learning吧？不像...但至少从下节标题看，跟supervised learning就不搭界)

### 15.5 Anomaly detection vs. Supervised learning

|                     |                                                              |                                                              |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Anomaly detection   | Very small number of positive examples(异常) and large number of negative example | Fraud detection, Manufacturing(e.g.aircraft engineer), Monitoring machines in a data center |
| Supervised learning | Large number of positive and negative examples               | Email spam classfication, Weather prediction, Cancer classfication |

(看异常数据够不够训练Supervised learning algorithm，不够的话就只能应用Anomaly detection algorithm)

### 15.6 Choosing what features to use

 数据天然符合Gaussian distribution最好，如果不符合，就做一定的transformation使其符合，会让algorithm表现更好。

然后通过error anaylysis for anomaly detection来选取features.

(当你没有参数可调时，创造参数也要调) 

### 15.7 Multivariate Gaussian distribution 20211118

（多元高斯是为了解决特征之间有线性关系的情况下，简单anomaly detection结果有误差的问题，比如数据中心，计算机的CPU负载和内存占用率）

And the key advantage of it is it allows you to capture when you'd expect two different features to be postively correlated or maybe negatively correlated.

(利用对角矩阵，比如一个二维feature，在坐标上，没有线性关系的话，两个正态分布，一结合就是个中间突起的正圆山丘，有线性关系，调节这个对角矩阵的参数，这个正圆山丘就会被拉成斜方向的椭圆山丘)

### 15.8 Anomaly detection using the multivariate Gaussian distribution

original model vs. multivariate Gaussian model

|                             |                                                              |                                                              |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| original model              | Manually create features to capture anomalies where x1, x2 take unusucal combinations of values. | Computationally cheaper( alternatively, scales better to large n) |
| multivariate Gaussian model | Automatically captures correlations between features         | Computationally more expensive                               |



## 16 Recommender Systems

### 16.1 Problem formulation 20211121

Example：Predicting movie ratings

### 16.2 Content-based recommendations

Content-based recommender systems:

 each of movie have two features:  romance, action => can be represented with a feature vector

predicting the rating of each user as a separate **linear regression** problem

问题：itercept term究竟是怎么回事? 弹幕说是偏置项...不懂

A(T):  矩阵转置的定义（矩阵的行和列互换位置）

问题：regularization term也忘记是干啥用的了...



本节：假定已经给出了一部电影的一维三纵的特征向量，比如爱情元素多少，动作元素多少...然后又假定了一个用户的三维一纵向量对各种类型的喜好，转置相乘就可以得到一个用户对一部电影的喜好程度。到最后归根到底还是对每一个用户的**linear regression** problem.

 

### 16.3 Collaborative filtering 20211123

### 16.4 Collaborative filtering algorithm

从预先直到用户的喜好，然后根据他们的评分来反推电影的feature.

### 16.5 Vectorization: Low rank matrix factorization 20211124

### 16.6 Implemantational detail: mean normalization 



## 17 Large scale machine learning 

### 17.1 Learning with large datasets 20211125

### 17.2 Stochastic gradient descent

(随机梯度下降法)

之前比较传统的叫Batch gradient descent (批量梯度下降法)

而为了应对 large datasets，新算法就是 Stochastic gradient descent

主要区别在于前者cost function是计算各个点方差的平均值，而后者只是取一点的方差来判断

1. randomly shuffle datas
2. repeatedly scan through training examples and update gradient terms

(之前是遍历所有样本梯度下降一次，随机是每输入一个样本梯度下降一次)

不像Batch那样顺滑连续地曲线到最小值并停在那里，Stochastic各种跳跃迂回，最终converge到最小值周围晃悠。

### 17.3 Mini-batch gradient descent 20211127

相对于Batch gradient descent遍历全部样本和Stochastic gradient descent遍历一个样本，Mini-batch gradient descent遍历一部分样本，介于两者中间。

Use b examples in each iteration.

(把上述理解为醉汉下山，样本越少，醉汉越醉，下山的路越曲曲折折)

### 17.4 Stochastic gradient descent convergence

check the algorithm is converging

(调参侠，贵在知道怎么调，learning rate 和 features去把握收敛方向)

To summarize in this video, we talk about a way for approximately monitoring how the Stochastic gradient descent is doing in terms of optimizing the cost function. And this is a method that does not require scanning the entire training set periodically to compute the cost funtion on the entire training set. But instead it looks at say, only the last thousand examples or so.

### 17.5 Online learning 20211128

If you have a continuous stream data, you don't have to keep a  fixed training set.

Online learning algorithm can adapt to changing user preferences

### 17.6 Map-reduce and data parallelism

(Map-reduce是云计算的基础)

The Map-reduce approach to parallelizing machine learning 
