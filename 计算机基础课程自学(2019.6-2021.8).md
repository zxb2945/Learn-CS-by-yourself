# 计算机基础课程自学回顾

## 书籍：编码的奥秘 2019.6.30

本周花了两天上班空闲时间看完同事推荐的《编码的奥秘》。从摩尔斯电码讲到现代计算机的组成，科普好书。怪不得科班出身的程序员对买电脑如数家珍，毕竟是专业嘛。

书中讲哲学与数学的交际就是逻辑，现代计算机理论就是从布尔代数引出的集合论与串并联的电路技术为基础发展起来的。将人类选择十进制更像是个偶然，二进制才是最为简洁的。但也正因为这个二元性，数字计算机不能处理连续事件，像浮点型数据只能是近似值，精确表达只能通过存储空间更大的BCD码。“每个字节都可以用两个十六进制表示”，也就是说，“一个十六进制是半个字节”。可以用继电器，电线，开关，灯泡组成一个巨大简陋的原始计算机，甚至可以用纸条上打卡来存储数据。到微观层面，CPU构成可以从加法器与锁存器讲起，像RAM为什么是易失性存储器，因为它本质上是电流与金属弹簧片的作用。硬盘能够断电后仍存储，是因为它是磁盘，利用磁介质长时间存储数据。据说要用500万个继电器，才能构成一个64KB的RAM阵列，显然不实用。上世纪六七十年代用晶体管为代表的半导体来取代，此即芯片为首的集成电路产业。微处理器(CPU)即芯片上的计算机，Intel和Motorola首先涉足，各自建立了高低位有别的存储系统。微处理器结构采用RISC，为了提高处理器速度，CPU与RAM之间还有Cache。书中还对各种最基础的执行机器语言的指令有介绍，可以逐一翻译成汇编语言，逐渐往上到高级语言，可知这个体系已经是非常抽象了。锁存器就是寄存器register，像堆栈stack就是中存储器技术，是不被别的东西使用的正常的RAM的一部分。（感慨自己对内存的理解不足很大一部分原因是缺少硬件理论的知识。）现在回想起来对书中出现的ROM，DMA，BIOS，DAC，ADC等术语有些熟悉是自己在工作之初有在杭州图书馆看过嵌入式系统的相关书籍，可见保持读书习惯还是很重要的。ASCII码是计算机界最重要的标准，A即美国，想这个信息时代的基础计算机的发展史前期基本全在美国，这是十几年的积累，无怪乎美国半导体产业这么强。至此计算机硬件最重要的几个部分CPU，RAM，I/O就讲完了，叫做冯诺依曼体系。早期的计算机理论就是出自冯诺依曼，图灵等人的硕士论文，人家这硕士含金量杠杠。并且，ROM会放些初始化程序，如OS。UNIX就是用C写的，有移位操作，按位逻辑操作以及指针是有别于其他语言最显著的特点。DOS，LINUX是文字输入类型OS，Windows是图形界面OS。计算机部件中还有个显卡，提供信号给显示器的电子部件，计算能力据说很强大，比特币矿机就会搞很多显卡来挖矿。

## 网课：C++ 2019.8.14

之前六月份利用早起看完清华大学郑莉的C++课程，一直想做个总结来着，刚才回头又过了遍笔记，觉得有点难。C++几年前自己看过书，尚有点记忆，这次算是系统地重温最基本的概念了。与C通用的不讲，新增了如reference，bool，new/delete等算是语法细节补充。而类/对象包含的知识只能是了解面向对象设计的基本思想，其语法细节关乎多态继承等的虚函数什么的只能说有概念。又比如友元，运算符重载等。To be honest，就算是C语言，目前项目开发中，我发现自己对字符串的理解，库函数的熟悉程度都令人汗颜。想要精通C++，除了最基本的语法，还有对泛型设计思想即模板，STL的熟悉程度无疑是难上加难。即便是科班出身的程序员不用来开发，也会忘得差不多。那么我看这门课的意义主要在于了解面向对象与泛型设计的思想，对C++语法系统过一遍，直到有这个知识点，有迹可循。在这基础上，以后工作有相关项目可以去尝试一下，可以先业余写个魔塔游戏？什么形式的语言只是工具，关键是编程思想。

## 网课：离散数学 2019.9.1

八月中旬历时一个半月在B站上看完北大陈斌老师的《离散数学概论》。分成五分部分：数理逻辑，集合论，图论，抽象代数，形式语言与自动机。

起头很有意思，讲中国古代数学始终停留在计算层面，不像西方经历了三次数学危机，使数学成为脱离观察，直觉与经验的纯粹思维。第一次数学危机是数轴无法表示的无理数，第二次是微积分创立时关于无限无穷的概念，第三次是集合论引出的自我相关，从中看到数学之奇妙！我高中时企图以经验出发把我实质，那时物理是够用的，对数学却是捉急，因为后者内在要求就是超越人类经验的。所以经典物理学尚可以用经验归纳出规律，待相对论及量子力学这种理论非借助数学这个梯子就无以触及，数学真是打破了求知欲的天花板。

讲数理逻辑，最基础的是命题逻辑和谓词逻辑。简而言之，前者的逻辑联结词为∧，∨，->等，做抽象形式化处理后成为命题公式，本质上是真值函数（布尔）。通过逻辑等价，有一套演算系统，可以确定一个主范式。但其无法表现不同命题之间的联系，比如三段论，只能是对确定的对象做出真值判断。在这基础上，加上量词ᗄ，∃就是谓词公式，谓词逻辑就可以讨论不确定性。数理逻辑还有其它方向，但基本建立在以上两者基础上，比较实用的就是德摩根定律【非(P 且 Q) = (非 P) 或 (非 Q)；非(P 或 Q) = (非 P) 且 (非 Q)】。

集合是现代数学中比数更为基础的概念，不做定义，只通过公理来描述的初始概念，是整个数学的基础。集合的集合即是集合族。将{{a},{a,b}}称为二元有序组，简记为<a,b>，定义笛卡尔积A1xA2={<u,v>|u∈A1,v∈A2}，是有序组的集合。若R是笛卡尔积A1xA2的子集，则R为A1到A2的二元关系，简记为A1RA2。特殊的关系有等价关系和函数，前者可举例三角形的相似关系：自反，对称，传递。可以引出划分这个概念。有限集合一个不空不漏不交的子集族，集合族关系与划分均可以定义出一系列运算来，可在各自定义域内封闭。

图论起源于科尼斯堡七桥问题，即一笔画问题，抽象为点线组合。两点之间的连接方式即拟路径，可分为边各不相同的路径walk与顶点各不同的通路path。两者出入点相同则称为闭路径与回路，此概念而来著名的图有欧拉图与汉密尔顿图。此外，二分图常用于匹配问题。平面图用于电路板线路不交叉问题。树tree即使二分图也是平面图，二元树对应数据结构中的二叉树，其遍历问题比较经典。

抽象代数是抛弃代数结构（例如自然数上的加法运算可记为<N,+>,又比如矩阵的运算）中对象集合与运算的具体意义，研究运算的一般规律。举例说，比如<R,+>与<R,x>为同态映射，此正是加法与乘法许多相似的原因。代数结构有许多类型如群环域，但课程中没有细讲，与幺元（如乘法中的1），零元（乘法中的0），反元（1/2x2=1，互为反元）等基本概念有联系。

形式语言与自动机这一章节有意思，以前中文专业中有语言学的内容嘛。Chansky对语言的定义为按照一定规律构成的句子和符号串有限或无限的集合。形式语言研究语言描述的问题，比如语法描述，通过有限替换规则，生成合格的句子。典型有短路结构语法（四元组）。又比如自动机，像有限状态自动机（五元组）就可以检验正则语言，常用来定义检索模式。正则语言也可以用正则表达式表示，如ab*描述{a,ab,abb,...}，是编程中很常用的概念。最后讲图灵机（分格无限长的纸带），可用来检验0型语言，其停机问题不可解。

总的而言，离散数学是计算机专业相当基础的一门课，有其垫着，才能进一步去了解数据结构与算法，编译原理等课程。跟之前C++一样有十七八小时。

## 书籍：程序员的自我修养 2019.12.31

过去几个月在上班闲暇时间看了这本副标题为“链接，装载与库”的书。把以前认识比较模糊之处系统铺陈分析起来，真是本好书，算是对之前Linux的一遍巩固。比较底层的软件共通的知识体系。

计算机软件体系结构可分：Hardware，OS，Runtime Library，APP与Development tools。Thread和Process的区别当然理所当然（汗颜之前混为一谈）；程序编译过程：预编译，编译，汇编，链接，编译器做的词法，语法，语义分析，链接器LD做的重定位工作；Linux上的目标文件为ELF格式，比如GCC缺省编译出来的a.out，其分代码段，数据段，只读数据段，BSS段等，清楚认识这个对代码开发过程中许多细节处理大有裨益；静态链接，空间与地址分配，符号解析与重定位。这里可以将运行库的概念在知识体系中的位置，GNU操作系统以Linux为内核，它的C运行库为glibc(GNU C Library)。所谓C运行库其实是C程序与各个平台的中间层，其主要功能是程序入口出口函数实行（堆，I/O初始化）。C语言标准库的实行（主要是数学函数，字符处理，I/O函数）。因为Linux早期不大支持多线程，有版本更新，像`strtok()`就更新为`strtok_r()`，这在今年开发时就遇到的问题！！GNU除glibc外还包括BFD开发库，如GCC，Id，GDB等。所以常说的Linux系统指的就是GNU操作系统，Linux只是内核而已；程序的装载，以Linux内核装载ELF文件为例讲解，理解虚拟地址的概念，对Segmentation与Paging也要有概念，加深对OS的理解。之后动态链接为何，C语言动态库libc.so放在哪，命名方式如何，即Linux共享库的组织方式也有介绍。动态链接时对解决绝对地址引用的方案，其实讲得非常细，但自己肯定消化不了，只是不求甚解而过，但也远胜于年初啥也不懂的状态。之后内存这一块，跟之前B站孟宁的视频对照看，大同小异。函数调用对栈变化的讲解，堆如何管理，系统调用是怎样一个过程等。最后还实现了一个简易的运行库，读下来，回顾副标题“链接，装载与库”，脑子里就留点东西了，所以读书真好！

## 网课：Linux 2019.10.20

年初面试时，对方问程序实在Linux上编译的吗？竟回答不上来，回想之前用了一年半的Linux系统，但却对它一无所知，连.so和.a区别都不知道，虚拟机vmware和测试盘有什么区别？写TCL自动化脚本时用了awk，但回头想什么也没记起来。过去两个月在B站上看了尚硅谷韩顺平的Linux教程以及中国科大孟宁的《Linux内核讲解》，加之新工作入职大半年的钻研，对Linux才有了个基本的了解。

我认为对Linux的认识可以分为三个层次。首先是内核层面的。孟宁在六个小时多的视频中从x86寄存器和常用汇编指令讲起，着重讲了函数调用时栈的变化过程，接着分析内核源代码（提到C中内嵌汇编代码，操作系统基本用C写），以sys.time为例说明了system call的使用过程，此外需清楚用户态和内核态，通过中断处理的切换过程，以及层次状况。然后开始讲进程，PCB的结构体介绍（讲道理2016年刚参加工作时候相关内容的书也看过，但没与工作结合，故而印象浅薄）。进程创建时，fork同时返回父子进程两个信号，特殊点。另一个特殊的系统调用是可执行程序装载时，execve进入内核后再出来已是另一个进程这个替换。最后讲进程的调度，进程上下文的切换与硬件上下文切换的区别（前者是两个进程间的行为，后者一个）。以上大部分操作系统的知识点，从Linux角度讲解，受益匪浅。

其次，可以结合韩顺平的视频讲，即交互层面。首先Linux下一切皆文件，其根目录命令行式的交互界面，像工作中用TeraTerm与WinSCP来进行Windows和Linux的交互，之前工作也一样。一些基本的操作指令就不说了，总数有250条左右。然后对vim的使用像yy，p这种复制拷贝以及高亮搜索翻页至少要知道，跟sakura以及SourceInsight一样是代码文本编辑器。还有用户管理，网络配置这部分的介绍（对开发要求不高吧，运维会比较关心）。一般来说，Windows能做到的Linux也行，上网收发邮件下载软件（YUM）等，甚至装Eclipse等IDE。在这个层面上最关键的当是shell脚本开发。Linux命令行界面本身就是一个交互式shell。那些shell script是非交互式的。在这基础上，理解各种变量，尤其是export环境变量（之前依样照葫芦敲进去却完全不知道其背后原理，惭愧），位置参数变量$n，预定义变量$!,$?...运算式的格式要求，判断式，流程控制等及函数部分：系统函数，自定义函数。在操作Linux过程中，会写shell脚本很基础，可以用于批量操作，定时运维等。

最后，是对C/C++开发而言的一些工具程序，GDB与GCC的操作，makefile的编写。并且，Linux文本处理三剑客grep，sed与awk。各自长处领域得有概念啊。以上只是些基础，说熟悉Linux仍然是不够，只能讲基本的Linux框架OK了，面试时吹个牛也知道个方向。

## 网课：数据结构与算法 2020.1.12

去年底三四个月时间看B站清华邓俊辉的Data Structure & Algorithm. 是计算机专业的核心课程，leetcode上刷算法基础就在这。大学学了门C，直到冒泡排序和链表...如此而已。之前前辈有跟我提vector和list各自优缺点以及归并算法，我就觉得很新奇，这边一看，是课程中最为基础的知识点，汗颜！

课程开篇先高屋建瓴地讲算法分析概念，n^2->2^n就是算法是否有效的分水岭，讲大O的估算模型怎样去繁就简，以最长公共子序列与斐波那契计数问题为例讲递归如何通过动态规划消去重复操作成为迭代，阐述了分而治之和减而治之两个重要思想。

之后就分篇介绍各种containers了。最基本的线性结构vector和list，说明归并排序如何优于冒泡排序；以两者为基础的stack和queue，尤其是stack。因为电脑与人脑机制不同，必须借助stack来实现进制转化，括号匹配以及中缀表达式（即1+1=2形式）；接着讲半线性结构binary tree，同事经常挂嘴边的二叉树。着重说明其三种遍历算法以及重构条件；非线性结构graph其实就简略提了一下；然后就花大篇幅讲balanced binary search tree，结合了向量与列表的优点，大致顺BST->BBST->AVL（两边平衡值可差1）理了遍；扩展到其它各具用途的树，利用某些数据访问具有局部性，将常用数据聚集于根部的伸展树，为实现更高级的IO操作的B树，支持对历史版本访问的红黑树；树之后讲hash（散列），借助hash函数，如除余法，对空间进行压缩，关键点在于冲突排解的方式；priority queue（优先级队列），就是get max问题，采用的是完全二叉堆，即以向量为形BBST为神，实现PQ，另外为方便堆合并过程，有左式堆的概念；之后讲串匹配，依此介绍KMP算法，通过构造next表避免重复匹配前缀。BM算法，从末字符开始匹配。Karp-Robin算法，凡物皆数的思想；最后是一些排序算法介绍：Quicksort, Quickselect, Linearselect, Shellsort, 大体是分而治之和减而治之思想的体现。

以上是非常简略过了遍30小时左右课程的主要内容，目的是脑中留个概念，往后碰到相关词汇有据可查，即索引表。

## 网课：计算机网络 2020.3.29

年初两个月在B站上学习了韩立刚的《计算机网络》，之前公司前辈推荐的。那老师讲高校中讲时偏理论，这门课就很抽象，他本身是网络工程师，结合设备就更清晰，加之自己之前交换机开发经验，这门课过一遍，自己综合性理了遍头绪，许多地方有了提纲挈领的认识。上次公司前辈问我IP地址与mac地址有什么区别？我居然没回答上来，惭愧。IP地址决定最终去哪里，mac地址决定下一条去哪，可见之前工作缺乏网络通信知识系统学习。

《计算机网络》主要就是讲解了OSI分层模型。这分层思想你不能局限于数据头嵌套：以太头嵌套IP头嵌套TCP头...否则你就没法理解物理层的电流与链路层帧的区别，电流与包与段是不同层次识别的对象，互相间区别划分标准是一样的。

物理层讲什么呢？什么信道复用技术，编码调制方式，奈式准则，香农公式...不感兴趣，无关紧要跳过（自学就是目的性学习，效率就高）。最简单的就是光纤，以前机房看得多吧。还有一点就是物理层中继设备集线器，就是现在工作台前那网线接头，即hub与链路层设备交换机的区别，前者蠢，只会广播，后者是有mac储存学习能力的。之前面试就把hub与交换机搞混淆了。并且，与无线路由也不一样，WIFI那个实质上有AP(Access Point，相当于有线网路集线器)，交换机，路由三个层次的功能，因此称作路由器没问题。

数据链路层，就是之前我做交换机芯片驱动层coding处，数据帧抓过不少。Ethernet协议之外，还有PPP协议（两个Router通信，ADSL拨号等）。前者才有MAC地址概念。Cisco建网三层模型，接入层（hub？），汇聚层（二层交换机），核心层（三层交换机），脑子里要有图。之前做的就是二层交换机一个盘上一块转发芯片上的编程工作，基于MPLS的VPN。MPLS是2.5层，路由器转发就不必解析IP数据包，凭标签就可以找下一跳， L2VPN(VPWS,VPLS)，L3VPN等概念当初学习得够久。MAC地址学习机制，QoS： ACL(Access Control List), Meter(流量控制)，线性保护技术FRR，PW1:1等就是那一年半主要开发维护的技术嘛，你说你自己究竟多清楚也不尽然，只知概念，惭愧，脑筋还是少动。当然课程中没涉及到MPLS。

网络层，得益于之前做过的一份各式报文的构造过程总结，对ARP，ICMP(比如ping)，IGMP，OSPF，RSVP等耳熟，除了ARP(通过IP找MAC地址)，其余均是承载在IP上的，只是IP头协议号不同，均是网络层协议。有这个知识听课过程对课件中图例就会更深的理解。了解一下ARP攻击，怎样窃取数据包，之前进公司给电脑配置网关，子网掩码就不会像无头苍蝇那样了；

传输层，TCP协议，三次握手，可靠传输，分段，课程里对怎样用窗口划动技术去保证流量控制，网络拥塞讲得很详细。与之相对的就是UDP协议，不可靠传输，如QQ消息。值得留意的是套接字（socket）概念：比如TCP头部前两行就是源端口，目的端口，将不同应用层协议发到对面地址的不同应用，这个端口号+IP地址即是socket，相当于TCP的两个端点之一。防火墙机制就源于这个端口号。

应用层，OSI中实际上还细分为会话，表示，应用三层，各种加密解锁等层次，课程没细讲。比如访问网站过程，其实分解开了有许多步骤，首先ARP找网关出去，DNS解析网址，TCP三次握手建立会话，最后HTTP接受数据。像douban.com是域名，movie，music等前缀就是其中的主机。自己电脑可利用DHCP服务器动态配置IP。DNS和DHCP都是应用层协议，对应不同的端口。此外，FTP协议，像现在在使用的WinSCP实际上就是个图形化SFTP客户端，同时支持SCP协议。Telnet就不讲了，天天去连接远程服务器，RDP远程桌面登录。收发邮件时配置的SMTP，POP3/IMAP也是应用层协议，要理解邮箱服务器就是两个邮局，从163.com下载文本到本地客户端。最后是HTTP协议，现在工作上的通信协议，网站多是URL，HTML，工作上数据部分用的是Json格式，但协议均是HTTP。关于几种操作手段GET，POST，DELETE，PUT，各种接口操作就是这些的包裹，反信200，204，410，500等在工作测试中各种确认，可以说是比课程讲解要了解的更深一点。

网络安全，有介绍病毒，蠕虫，木马，逻辑炸弹等多种恶意程序。

最后两章讲IP电话与无线网络，讲得略简，但恰好是与自己当下工作密切相关的。因此，《计算机网络》这门课没学，我之前的工作几乎是摸着石头过河，但反过来说，工作后重温，互相佐证，就看得特别有意思。话说当前得工作内容，框架性地说明下：首先，是通信发展史，2G/3G时代，日欧与北美用的是两套标准，到4G时代才统一成LTE。2G/3G时代是电路交换CS与分组交换PS并存的，到4G是全网IP化时代，日本2G采用的是GSM标准，过渡到3G时引入了GPRS，我推测是在这个节点导入IMS的，即IP化的核心网。4G时代音声从CS变成了PS，IP化，即VoLTE，粗略讲VoLTE=LTE+IMS。我认为LTE应该是接入网。

再者，我所参与开发的就是这LTE网，其核心框架成为EPC，即LTE交换局，从接受手机信号的基站eNode到接入核心网PDN的Gateway的各种设备，用于地址登录的MME，用户签约信息的HSS等。与之并存的3G无限接入收容网中对应设备为NodeB，SGSN，HLR/VLR。

我所参与开发的设备称为SCP，HSS相当，用于处理用户的契约，在圈信息，全日本有二十多套吧。操作系统和数据库是分离的，分别叫做FSCP和DSCP。其中FSCP又分成FS/FEP，USP两部分。FS用来接收HTTP/SO协议，FEP是收发Diameter和MAP协议。我所开发的程序位于USP上（各种分离主要是确保故障发生时可以切换）。

所参与开发的程序用于各种业务的呼处理。架构上是两条线程，线程间用队列消息通信（这里认识错误，仅仅是用mutex_lock来调度）。数据收发采用HTTP协议，数据部分用Json格式。有契约创建删除，用户信息更新，电话号码获取各种API提供出去。测试时从模拟呼出设备收到HTTP，访问DSCP，用Diameter协议跟4G设备通信，MAP协议跟3G设备通信共同完成任务处理。

去年一整年全组十几个人就是从0开始写代码，与之前那种增量开发比较，对整个程序开发流程还是会有更深的理解。

## 网课：数据库原理 2020.7.4

B站上看了尚硅谷李玉婷的《MySQL》课程以及东南大学徐立臻的《数据库原理》。之前面试人家就有问懂不懂SQL？对我而言是块切实的知识盲区。另外有意思的是可以了解培训机构与大学课程讲解的不同处：前者相当于Linux中的GNU，后者就是内核；前者止于表面，但实用，后者深潜海底，却晦涩。两者一联系，容易理解恍然大悟。我觉得先接触表面后了解实质的吸收效率就高很多。

徐立臻一开始讲数据库的发展史，目前基本是关系型数据库，讲基本构架原理，比如数据独立性，就提到视图view，以及最后讲数据库文件组织，提到的事务transaction，从原理上讲很重要的两个概念在李玉婷课中就一小节。徐立臻讲SQL语言前先讲了离散数学中的代数关系和关系演算，提到了笛卡尔积，但讲SQL是怎么一回事就没李玉婷直观。一结合就更容易明白SQL语句的设计思路对吧。

李玉婷课中的重点就是SQL语言。先介绍了MySQL这款DBMS。SQL实质上是增删改查，类似于shell脚本语言有流程控制if，case，系统用户变量等概念，还可以自定义函数等。又尤其以DQL查询语句的介绍内容为大头，其形式类似：

```
select 查询列表 from 表1 别名 连接类型 join 表2 on 连接条件 where 筛选 group by 分组列表 having 分组后筛选 order by 排序列表
```

这部分内容占了一半以上。DML就是对余下的数据的增insert，删delete，改update操作。而DDL是以表为对象的增create，删drop，改alter操作。创建表时要添加各种约束constraint，如主键，外键。

徐立臻还有讲他之前设计数据库时遇到的坑。Anyway，他是讲数据库是什么，而李玉婷讲数据库怎么用，各自角度还是不同。回想起来之前韩立刚讲《计算机网络》就把两者结合，广受人们喜爱。

## 网课：计算机构造与解释 2020.10.2

7.16到9.13这两个月利用起床后的空闲时间在B站上看完了SICP课程：Structure Interpretation of Computer Program. 是八十年代作者讲给惠普工程师听的课，差不都二十多个小时。To be honest, 所用的LISP语言全程也不大明白，兼之又刻意听英文，又不反刍做题，估计就听懂两三成。亏弹幕大神，也只了解他全程讲了哪一些主题。

先讲了iteration与recursion，几乎是algorithm的核心概念了。铺成了Fibonacci，根号5的计算过程，rectangle的缩扩变化等，说There is no real difference in some sense between procedure and data.(不明觉厉) 又搞了embedding a language inside another language，怎么去make up a language. 延申开来就讲的object-oriented programming，像函数重载，多态等概念，不求甚解，又如stream概念。像Lisp中传参默认传指针，传值反而要特别feature下，听起来操作还挺复杂，要dynamic binding，delayed parameter什么的。最后几章讲stack，压栈什么的，我稍微懂一些halting problem什么。其实SICP讲得范围很广，此外有讲Logic和Relationship(SQL)  Programming的区别，Compiler和Interpretation的不同。Anyway，都说这本书非常好，我听完因为不大懂也品啧不出来，希望以后有机会反刍。

## 网课：编译原理 2021.1.8

10月下旬到昨天将近两个半月主要在B站看编译原理，本来是想听英文课，所以特地找了Stanford的Compliers看，看到十分之七Code generation实在是听不下去了，因为他以COOL语言为例在讲，尽管比上次听SICP认真不少，但还是放弃了。然后接着看哈工大的编译原理，从二分之一处Code generation开始看起，因为前半部分讲Lexical analysis，Parsing，Semantic analysis两个课程基本对得上，到Code generation就完全不同了。

Lexical analysis主要就是讲将program text分割成tokens，tokens编译时经常出现对吧？有许多种类，Number[digit+]，Keyword[ 'if' + 'else' +...] ,Identifier [letter (letter + digit)*],Operator 什么的。这些种类都是通过Regular Languages来定义的，正则表达式是CS中一个重要概念，来日本前我一无了解，惭愧！Compliers中介绍了五种：'C' ，''(空字符串)，AUB (union)，AB (concatenation)，A\* (iteration)。具体没深入去了解，之前离散数学中也提到过。回想呢，其实看这个课程基本抱着一个不求甚解，浅尝辄止的态度，先不去说到科班生写一个简单的编译器，就大学中去理解去做题去考试那种程度也没有，有个框架概念能去索引，也就这样了，否则自己也看不下去。Lexical analysis中还有一个概念就是读入program text时的有限自动机Finite Automata，NFA 与 DFA 区别在于有无空跳，DFA就更大更快，其实是一个空间与时间之间的取舍。

Parsing就是针对Context Free Grammar利用各种分析算法生成一棵Abstract Syntax Trees的过程。Parse algorithm有很多，比如最直观的从上而下，从左往右的Recursive Descent Parsing；从左往右扫描，产生最左推导式的LL(1)Parsing ，这里注意First Sets与Follow Sets(向前查看时用得着)的概念；还有Bottom-Up Parsing，利用stack自下往上分析做SLR分析表等。讲道理分析文法与自动机的结合什么的，也没看懂，但以上这些算法确实是可以代码实现的，编译原理某种程度上是教你如何去设计一个编译器的课程，我自然没这个需求，但是学习新的编程语言时可以事半功倍，接下来想看Python，可以检验一下这种不求甚解的了解是否有帮助。

Semantic Analysis就是The scope of definition，Types Checking（C是statically typed，而Python是Dynamically typed）等一类的东西，讲得相对不算多，接下来就讲Code generation，正是这个节点就转去看哈工大一个叫陈鄞的老师所讲的编译原理。上来就讲语法制导翻译SDD（语法分析+语义分析+中间代码生成），事实上中国的大学课程大多不是按照一个初学者正常探索的思维顺序，而是经过编书者再理解后的一个瀑布铺陈式顺序，这就容易造成学生一个概念上来不知道干嘛用的，抽象费解，没头没脑，只能被填鸭式接受。SDT是在产生式右部嵌入了程序片段的CFG，是对SDD的具体实现，将语义动作中抽象定义改写成具体可执行的栈操作，比如输入：3*5+4，依照SLR自动机走，并根据产生式和语义动作，来完成对输入内容的语义分析。其实语法制导翻译SDD可以理解成编译器代码设计的一个框架，比如为每一个终结符设计一个函数，继承属性是参数，综合属性是返回值这类的，为SDT中的语义动作编写执行代码。一个比较基础的概念是三地址码，就是最简单的算式。中间代码生成就是利用SDT介绍声明，赋值，控制流，过程调用等各种语句的翻译过程，看栈如何变化，汇编层面上的动作。这就要求以最简单的三地址语句来举例对运行存储分配，寄存器选择有个基本概念。回想起来之前看《一个程序员的自我修养》，Linux内核介绍等还是很有帮助的，CS各门课程之间都是你中有我互相穿插的关系。至于代码优化，说起来删除无用代码，更高效率地利用寄存器容易，其实实现起来还是很费解的，有什么到达定值分析方程等十分抽象的内容，怪不得人家说编译原理难。

综合所上，编译五个过程大概是什么，应当有一个了解，编不了编译器，但前人造了很多轮子这件事应该知道，也就如此而已了。

## 书籍：SICP 2021.3.19

是上个月看完 Structure and Interpretation of Computer Programs的Python版本。前天又花了一天时间回顾。真是大开眼界，给你一种完全不同的视角看待程序语言，正如书中所言的magic，art，比如我们都知道program是由procedure和data构成，但我从没想过procedure就是data，data就是procedure。就Python入门而言，对一个C语言程序经验者，这本书非常难懂，难懂在于它的讲述方式跟传统的填鸭式介绍控制语言，数据结构...完全不同，但我反过来想如果对于初心者而言，或许就没有别样的难。当然本书与其说讲解Python，还不如讲是一种通用的程序语言概念，最核心的无非就是抽象和递归，只是拿Python举例了。

第一章讲procedure。程序语言中最基本的概念是什么？表达式（最基础的就是数值本身）和 表达式的绑定。在Python中，在一定范围内的自然数，都是有独立地址的，比如

```
a = 5，b = 5
```

a和b地址就相同，所以相对于C，`=`更确切讲是绑定而不是赋值，`=`提供了最基础的抽象手段。然后是调用表达式（数值+运算符）。运算符本质上就是函数，这里就引出了第一章的主题，函数。进一步稍微复杂一点，函数可以嵌套，提供了组合操作手段。函数由什么组成？语句。表达式、返回语句return和赋值语句等是最简单语句，跟C相较，因为Python可以是屏幕上交互式语言，所以像123这样单独数值的表达式就是一条独立的语句，逗号在赋值语句中分隔了多个名称和值，比如

```
a,b=1,2
```

（这本书很多东西都讲到C基础Python语言学习者的盲点上，有时候正是受困于既定思维，所以阅读此书时方才更为困难）。简单语句其基础上有def（定义），if（条件控制），while（重复）等复合语句。Python 的代码是语句的序列（语句的序列实质上也是个first+rest的递归结构）。一条简单的语句是一行不以分号结束的代码，复合语句一般占据多行，并且以一行以冒号结尾的头部开始，它标识了语句的类型。同时，一个头部和一组缩进的代码叫做子句（借助简单复合语句的概念，去理解Python与C书写格式上的异同）。函数作为一个整体可以作为参数，返回值，跟最基本的表达式一样是语言的一等元素，可以被嵌套定义，高阶函数可以像操作数据一样去操作函数，这里就可以理解为什么procedure是data。Python中对一些没有赋值和控制语句单个返回表达式的函数可以用Lambda 表达式，有点类似于C中的define，但进一步具有匿名性（即不存在函数名）。此外，还顺道讲了一个语法糖：装饰器，结构上非常费解然而又精巧，用于不修改高阶函数原有代码的前提下对高阶函数功能追加。

第二章讲data。Python 包含了三个原始数值类型：整数（`int`）、实数（`float`）和复数（`complex`）。之后所有的抽象数据类型都是基于此构建起来，最终形成一个Python大厦。首先书中通过有理数的构造过程，来展示抽象的层次。表现为几分之几的有理数数据结构由二元元组(偶对的形式)来实现，而偶对的下标访问方式以及有理数数据结构的运算都能以函数操作原始数值类型来实现（从这个角度上看，偶对或者所构造的有理数就是一个函数，这就说明了为什么data是procedure）。偶对到多元元组就一回事。

这个节点，书中引入了`nonlocal`语句，我们知道函数也是一种数据类型，书中特地分了纯函数和非纯函数，它们之间最本质的区别就是是否可变。前者只要特定的参数，return肯定是不变的，后者假若引进nonlocal变量，return值就是可变的。这里隐含了一个赋值语句的双重作用：创建新的绑定，或者重新绑定现有名称。为了去补救重新绑定的操作引起的认识论问题：它对于两个相同的值意味着什么。（如果清楚C中指针的概念的话，其实就很好理解）。对于不可变类型，值相同对象就相同，对于可变类型就不尽然。所以Python 引入了两个比较运算符，叫做`is`和`is not`，测试了两个表达式实际上是否求值为同一个对象，`is`和是个比相等性`==`更强的比较运算符。

偶对可以嵌套，满足封闭性（封闭性在任何组合手段中都是核心能力），去实现

```
(1, (2, (3, (4, None))))
```

这样的嵌套结构。这个嵌套的结构通常对应了一种非常实用的序列思考方式，一个非空序列可以划分为：它的第一个元素，以及序列的其余部分。这说明了列表是可以基于偶对递归实现，并用函数实现元素的变更（ps Python 的内建list序列类型以不同方式实现）。(传统Python介绍中先介绍list，再是tuple，然后要去解释为什么有了list后还要创造tuple？因为list基于tuple基础上实现嘛，由此可见本书编排精妙之一斑。) 

字典的目的是提供一种抽象，用于储存和获取下标不是连续整数，而是描述性的键的值。所以字典实现上可以通过列表改造而来。至此就完成了从偶对到字典的主要序列容器的构建。（书中还顺手组合了非局部赋值、列表和字典来构建一个基于约束的系统，支持多个方向上的计算。这真是开了我的眼界，理解连接器和约束，把它们看成key为函数名，value为函数过程去理解，最新颖在于这个这个基于约束网络特征的无方向计算。）

我们知道序列的许多操作都是基于迭代的概念，这里就可以引进迭代器。有序序列其实隐含着迭代器接口`__iter__`和`__next__`，Python 拥有额外的控制语句来处理序列数据：`for`语句。

```
>>> counts = [1, 2, 3]
>>> for item in counts:
        print(item)
1
2
3
```

如上例，counts可以通过`__iter__`回应给`for`一个迭代器，然后`for`反复调用`__next__`来实现对序列的遍历。可以通过`while`显式地配合`__next__`来实现`for`语句。程序中的一个常见模式是，序列的元素本身就是序列，`for`语句可在头部中包含多个名称，将每个元素序列“解构”为各个元素。这个绑定多个名称到定长序列中多个值的模式，叫做序列解构。它的模式和我们在赋值语句中看到的，将多个名称绑定到多个值的模式相同。如果不需要解构序列元素，常见的惯例是将单下划线字符用于`for`头部，要注意对解释器来说，下划线只是另一个名称，但是在程序员中具有固定含义，它表明这个名称不应出现在任何表达式中。（这本书说明Python完全是另一套思维，不是既存的`for`去操作序列，而是序列的某些特性需要引入`for`语，而且说`for`语法的每一个点都说到心坎上去了，相较于C，Python中`for`的改造在哪里？妙不可言！其实深入思考一下，C语言中`while`和`for`在汇编层面是一样的，但Python中不然，就因为这个迭代器概念。）

`for`语句常跟`range`配合使用，如

```
>>> for _ in range(3):
        print('Go Bears!')

Go Bears!
Go Bears!
Go Bears!
```

`range`事实上是一种隐式序列，迭代器提供了一种机制，可以依次计算序列中的每个值，但是所有元素不需要连续储存。反之，当下个元素从迭代器获取的时候，这个元素会按照请求计算，而不是从现有的内存来源中获取。这是惰性计算的一个例子。计算机科学将惰性作为一种重要的计算工具加以赞扬。

生成器表达式组合了过滤`filter`和映射`map`的概念，并集成于单一的表达式中，以下面的形式：

```
<map expression> for <name> in <sequence expression> if <filter expression>
```

表达式包含了`map`和`filter`的大部分功能，但是避免了被调用函数的实际创建，实际上也是一种惰性计算。

生成器函数是一种特殊的迭代器，不同于普通的函数，因为它不在函数体中包含`return`语句，而是使用`yield`语句来返回序列中的元素。`yield`与`return`的区别在于，它返回值出去之后仍然可以在当前位置继续执行下去，利用这点可以去构建Python中一种轻量级的线程，即协程（coroutine）。

流提供了一种隐式表示有序数据的最终方式，流是惰性计算的递归列表，就像之前自偶对嵌套构建而来的`list`那样，`Stream`类实例化后可以响应对其第一个元素和剩余部分的获取请求，同样，`Stream`的剩余部分还是`Stream`，然而不像`list`，流的剩余部分只在查找时被计算，而不是事先存储，也就是说流的剩余部分是惰性计算的。（惭愧惭愧，C++中多次接触流，但始终没有理解其本质）。

讲流时提到了对象，而对象正是基本数据抽象最上层的概念，事实上可以通过在实例、类和基类之间发送含有属性的字典作为消息来实现对象系统，只是没法去实现点运算符，书中讲点运算符是 Python 的语法特征，它形成了消息传递的隐喻。（类与方法之间的点可以视作为一个运算符。其实我这个浸淫C多年的coder刚看Python就对这个点运算符比较懵逼。）点运算符隐式地把方法的第一个参数设为了self。可以在pycharm中使用debugger来追踪书中所列出来的例子看一下类与对象是如何建立起来的。先创建类，从外层的类到基类，调用层层深入进去，然后实例化对象，从基类开始往外依此调用初始化函数最终抵达实例。比喻来说，创建时先一层层进去拿到最核心的字典，即基类的字典，然后实例化时在从这个核一层层出来，包裹各层次之间的颜料，最后形成实例。每一个层次的字典（可以理解为方法集）都不一样，就可以理解类与对象各自有相应的方法，某种程度上类似于类与基类，从而也能理解各个层次间方法重载的机制，先是调用实例中的方法，如果找不到，就去类中寻找，并绑定到实例中执行。（另一方面我们也可以理解为什么C的编译器可以用C来编写，因为就如同Python的某些机制如类与对象可以由更为低层次的抽象概念字典与函数来实现一样，语言的建构是一个不断抽象累积的过程。）

第三章实现了一个解释器，前面两章分别讲了编程的两个基本元素procedure和data，事实上，procedure就是data这一点，也可以从用户的程序即是解释器的数据的角度来理解。本章最后有介绍Lsip的两种方言Scheme和Logo的解释器构造：数据求值粗略地来说可以得到基本表达式和函数，这个函数就需要调用过程，而过程中去读取每一行的代码有需要递归去调用数据求值来确定值，从而数据求值与过程互相递归，最终停止于基本表达式。

关于这两种抽象语言的解释器框架，我确实也只一知半解，但能够意识到其核心就是递归：可以通过C写一个Fibonacci数列的递归调用，从汇编层面去理解递归函数：函数前半段不断重复进去递归函数内层，然后后半段不断重复出来到最外层。书中说我们不应该关心`fun(n-1)`如何在`fun`的函数体中如何实现，只需要相信它计算了`n-1`的阶乘，将递归调用看做函数抽象叫做递归的“信仰飞跃”（leap of faith），这说得真好！递归与迭代思考方式上是截然不同的，通常，迭代函数必须维护一些局部状态，它们会在计算过程中改变。在任何迭代的时间点上，状态刻画了已完成的结果，以及未完成的工作总量。

书中以较抽象语言更为简单的计算器语言的解释器构造为例来抛砖引玉，介绍之前先需介绍两个概念：表达式树和字符串。就像第二章介绍的递归列表，树同样可以通过偶对嵌套的封闭性来构造，这里引入Python除tuple，list，dictionary之外第四种容器set来说明树的高效性，set可以由无序集合，有序集合和平衡二叉树集合来实现，它们对于集合的交并等计算而言，效率差别很大。（这些事实上是算法和数据结构的内容，加上编译原理，本书内容其实非常丰富。）

然后是字符串，字符串满足基本的序列容器条件，数据值的字符串表示在类似 Python 的交互式语言中尤其重要，其中“读取-求值-打印”的循环需要每个值都拥有某种字符串表示形式。Python中主要有两种字符串构造函数`str`和`repr`，略有区别。与之紧密相关的概念是接口，响应`__repr__`和`__str__`特殊方法的对象都实现了通用的接口，它们可以表示为字符串，这某种程度上是函数多态的一种表现。（JAVA中抽象类ADT与接口的区别不是常提的嘛）。

回到计算器语言的构造，仍然是可以用pycharm中的debugger来看书中的例子，其基础是构建一个表达式树的类来作为计算器语言的基本对象，比如`add(1,2)`,由operator和operand构成(注意这个类本身可以递归表示为operand，形成复合计算式）。在“读取-求值-打印”循环的交互模式中（一个loop函数），利用词法分析器lexical analysis切割所输入的字符串（代码文本）产生一个token列表，然后利用语法分析器parse去生成表达式树（这里要构建一个递归逻辑去实现复合计算式的实现），中间通过异常机制健壮语法错误检查，最终将表达式树输入计算逻辑函数即semantic analysis得出结果，即输入符合语法的字符串（代码文本）被相应执行。（就C编译器而言，相对Python解释器，多optimization和code generation两个步骤，但前三步原理一样，这比看编译原理简单明了多了！）

之前有看过Lisp版本SICP的英文视频，就是完全云里雾里，也看过浙大python的教学视频，感觉就填鸭死记硬背，等到看了这本Python版本的SICP，非常惊喜，回味起来就感慨这本书的编排之美丽，果真是magic，art。明白别人所讲的：永远不要做一个不知道递归和抽象概念的程序员。

## 网课：Python 2021.3.31

之前有讲过Python版的SICP，它本身是可以作为Python教材来看待的，而且是以剖析Python展开编程思维的角度，非常之棒。其实这之前的一月份花了一礼拜先在B站看了浙大翁恺等人八小时左右长的Python程序设计课程，讲实话就是填鸭式顺序介绍Python的基本数据类型，条件循环控制语句，五种容器：list，tuple，set，dictionary，字符串；函数，以及面向对象六部分内容，非常常规。我估计JAVA，C#这些语言无非也是这样一个框架，细节处变化而已。虽然吐槽这个念PPT，单纯看这个就跟好久以前看C#一样一个月忘得一干二净。但是我也必须说，有了这个入门，再去看SICP，发现入门时候的一些困惑处它那边都给你寻根究底地解释出来，一比较更能发现SICP的妙处。比如说缩进层次表示，为什么需要tuple，装饰器，点运算符等等。

同时，两月份的时候，在Github上下载了骆昊的Python-100-Days来对照着看。他里面相对浙大版教程要更细一点，诸如`if __name__ == '__main__'`的作用，Python中定义类时，class, class()和class(object)的区别，理解python之self，enumerate可以同时返回index和value值，使用property对类方法进行修饰转变为属性等等（其中也不乏自己去CSDN搜索的结果）。讲实话呢，Python基础知识大框架跟浙大教程一样，跟SICP相比也是表层的一些介绍。它有意思的地方在于，对计算机领域很多宏观方向做了一定介绍，但是像大数据，AI与机器学习呢只是列出了框架，没有深入，加上区块链，5G，云计算等差不多就是近十来年来甚嚣尘上的概念了，按照原有想法把基础夯实之后，就一块前沿技术想深入了解一下。骆昊这里对我最有价值的还是Python的Web框架Django的介绍。Web开发在IT行业占据很大一个比例，陈磊就是做后端的。所谓Web框架，就是用于开发Web服务器端的一系列封装好的模块和工具。事实上，即便没有Web框架，我们仍然可以通过socket来开发Web服务器端应用，比如目前所从事开发的HLR/HSS就是典型直接用socket构建起来的服务器，Web服务器最底层也应该是一样收发基于TCP/IP的HTTP报文。一般来说以浏览器为界，分前后端，浏览器中为用户进行页面展示的部分称之为前端，而将运行在服务器为前端提供业务逻辑和数据准备的所有代码统称为后端。前端主要就是HTML、CSS、JavaScript语言。HTML就是网页内容的载体，使用Ajax技术可以在不重新加载整个页面的情况下对页面进行局部刷新。在使用Ajax技术时，浏览器跟服务器通常会交换XML或JSON格式的数据，所以XML是HTML的补充，XML 把数据从 HTML 分离，通过使用几行 JavaScript，你就可以读取一个外部 XML 文件，然后更新 HTML 中的数据内容，而JSON是相对于XML更为轻量的传输格式；CSS是HTML的化妆师，比如，标题字体、颜色变化等控制，被成为前端渲染；JavaScript是嵌入在HTML中的脚本语言，由浏览器解释执行代码，不进行预编译，用来向HTML页面添加交互行为，如鼠标按一下显示动画，JQuery是一个优秀的JavaScript框架，一个轻量级的JS库。CSS用link（或style）， JS用script标签，在html文件里都可以找到。后端最简单的搭建就是LAMP： Linux+Apache+MySQL+PHP。Apache就是用来收发浏览器HTTP请求的socket包裹程序，如果是静态资源就直接返了，如果是动态资源，就用HTTP或共享内存跟PHP进程进行交互，如果需要相关数据，就再去MySQL取。如果用JAVA来开发，通常用用Tomcat服务器来辅助Apache解析动态资源JSP(JAVA+HTML)。Django当然是基于Python的，MySQL是关系型数据库，它通过Django提供的ORM模块将数据转化为Python的对象形式（也就是说Python不用直接操作SQL语言），然后Python再通过相关函数库转化为JSON格式发给Web API。服务器并发量很大的情况下，会使用非关系型数据库Redis来作为缓存服务器提高网站性能。还有Session这个概念也有意思，联系目前的工作，其实可以理解为一个请求对象。骆昊还讲了网络爬虫，其实百度搜索引擎本质上就是个 web crawler。大多数网站都会定义robots.txt文件，淘宝在这个文件里不允许百度访问它的产品，因为是Open API，所以只能是百度自觉遵守。

## 网课：操作系统 2021.5.2

哈工大李治军的操作系统号称B站第一OS课程，花了两个月时间看了一下，还是很不错的！讲述逻辑清晰，让自己意识到之前许多细节上的认识不清，CS真的是一个整体学科，各门基础课互相交叉佐证，比如操作系统跟计算机组成和原理关系就很紧密。

首先，操作系统是如何启动的呢？为什么讲计算机一定要溯源到图灵机？因为即便是21世纪的计算机执行的核心也无非是“取指执行”，与图灵机无异。当一台计算机Power On，它首先去读位于内存ROM中的BIOS，基本输入输出系统代码，检查各种硬件设备。然后它去磁盘读取操作系统第一段代码，是汇编代码，向屏幕打印出开机LOGO，完成OS启动前的设置，获取机器的内存大小，OS开始接管各种硬件。然后才到OS的C文件的main函数，而main中最后调用fork函数，执行一个桌面shell，其逻辑就是scanf一个cmd，然后再fork一个新进程，用exec(cmd)去替换掉这个进程。这样启动基本就完成了。这里需要搞清楚的是操作系统作为一个程序，它特殊在哪里？它是个程序，但它不是个进程。内存分为内核空间和用户空间（与CPU的内核态与用户态概念要区别），前者放着OS最核心的代码与数据。其它程序，作为进程运行其上，必须与其合作来完成对硬件的操作。内核空间大部分对所有进程都是共享的（除了各个进程各自的内核栈），所以某种意义上说，操作系统像个服务器，给作为客户的各个进程处理各种难题。这样你就可以去理解socket建立TCP连接的三次握手为什么可以在server端accept阻塞的时候完成，因为OS在内核里处理了。进程如何从用户态进入内核态？就是System Call，软件中断指令int 0x80，拿`printf`举例: 用户调用`printf ` -> 展开成int 0x80 -> system_call -> 查table -> 调用sys_write。所以往屏幕上输出字符，无非就是普通C语言函数+操作系统接口，后者表现为sys_write的函数调用形式。

李治军在课程中讲OS最核心的两样东西就是多进程图谱和文件操作视图，前者密切相关的硬件是CPU和内存，后者是磁盘和各种终端设备以及IO操作。

讲多进程，就得先介绍线程，多进程无非是指令切换和内存资源切换，如果知道了线程切换，多进程的指令切换这一部分就懂了。这里呢，我头次听说线程还有用户级线程和核心级线程的区别。ThreadCreate 的核心就是用程序做出这三样东西：TCB（类似于进程的PCB），栈， 切换指令寄存器的逻辑。用户级线程的创建和切换，并不进去内核态，事实上可以不涉及操作系统，就是应用程序层面的。内核级线程相对于用户级线程并发性要更好，为什么呢？因为后者比如应用于浏览器，接受URL时候，就要调用硬件IO，穿过内核时，内核直接调度到其它进程去了，导致浏览器所有线程都阻塞住，所有页面都卡住（如果浏览器是用户级线程来表示每一个页面的设计的话，然后chrome似乎是多进程设计，就没有这个问题，但是超耗内存）。对于核心级线程而言，不仅在用户空间有一个栈，内核空间也要有一个栈（用户态栈跟内核态栈存储的信息组成一个完整的中断栈信息，不是单纯copy一份）。内核线程切换：先从用户栈进入内核态，再到TCB，然后切换TCB，根据TCB，先切换内核态栈，然后根据内核态栈，再出来切换用户态栈。进程没有用户态进程这一说法，所有进程都是内核态的，因为进程需要分配内存资源，这是与硬件打交道的，必须进入内核态。每个进程都有自己的一套映射表MMU和缓存cache，所以对于多核处理器（与多处理器要区别）而言，它虽然有多个CPU，但它们都是相同的地址映射和缓存，所以多进程没法体现其优势，而多线程则可以，所以说多核要发挥作用，必须支持核心级线程。之前看《程序员的自我修养》时没法理解Linux内核没有实现完全符合POSX标准的多线程机制的这个说法。有了以上的概念去了解Linux如何实现多线程之后就会明白些。Linux事实上在内核中用轻量级进程去模拟核心级线程，尽量优化轻量级进程切换开销，在内核中完成调度，然后在核外线程库中完成线程的取消同步等机制，所以进程：线程其实是1：1的，这就造成Linux的线程有独立的进程id，以及线程数量受到整个系统的总进程数的限制。课程还介绍了进程调度的算法，不过对IPC通信机制基本没什么展开，就是简单说了下Semaphore共享数据策略，如何避免死锁。

讲文件系统，就要去深刻理解“Linux下一切皆文件”这句话的意思。Linux下主要有普通文件，存储普通数据，一般就是字符串，比如存在磁盘上的txt，可执行文件等；目录文件，存储了一张表，该表就是该目录文件下所有文件名和inode的映射关系表；链接文件，比如软连接，文件内容就是要链接原文件路径的信息；设备文件，又主要分为三种，块设备文件，就是磁盘自身，字符设备文件，各种键盘鼠标等终端，网络设备文件，网卡。（用来实现pipe的管道文件就不表了）。对于储存在磁盘上的普通文件而言，从读写磁盘到呈现给用户的字符流之间有好几层抽象，所谓的格式化硬盘，其实在磁盘上建立文件系统，便于索引（整个磁盘变成一个目录树， 目录树这个结构自身也放在磁盘上，所以磁盘拔下来插到另一个操作系统中，就直接可以用了）。在这些抽象层中，inode是比较关键的，它是一个记录对应文件相关信息的C结构体，普通磁盘数据文件与设备文件映射过程中inode往后路径就分叉了，前者去调用相关函数操作磁盘，后者去调用相关函数去操作其它设备，无论哪种文件，对用户而言都是字符流，只是设备文件并不真正存储指向的数据，只存储了类型和参数信息。而fd(File Description)是另一个关键点，它其实是一个数组下标数字。当一个进程打开某一文件，就在PCB中建立一个fd去指向该文件的inode，close该文件的话，就将其fd移除。当一个程序启动，它自动打开标准输入stdin，标准输出stdout，标准错误stderr三个文件，并在PCB创建三个文件描述符0，1，2。（这3个文件都与终端相联系。）所有I/O操作都通过调用fd来执行，包括socket。网络 IO 的本质是 socket 的读取，socket 在 Linux 系统被抽象为流，IO 可以理解为对流的操作。对于一次 IO 访问 (以 read 举例)，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。在Linux下一切皆文件，虽然各种类型不同，但是对其提供的却是同一套操作界面：open，read，write，close...（网络设备文件用socket接口。）比如向屏幕输出“hello world”，调用`printf`，系统调用接口其实就是write，在write函数里根据fd找到字符设备文件的inode，判断文件类型进入相应设备驱动代码分支，最后由CPU执行指令，从内存直接mov相应字符流到显存寄存器，完成输出，最后由屏幕控制器返给CPU一个硬件中断告知完成输出。操作系统中的设备驱动代码其核心就是给用户一个简单的文件视图。

聊完外围设备，讲内存，其实就一句话“程序分段，内存分页”，段面向用户，页面向硬件。程序员希望用段，内存希望用页，段和页的结合就很重要，其核心就是虚拟内存， 将段以页的形式打散到物理内存中，代码中是逻辑地址，然后加上基址就成虚拟地址，最后映射成物理地址。（因为用软件实现非常费时间，所以CPU里设计了MMU这个硬件来负责这两次映射）程序执行请求地址，MMU查页表，发现缺页，就打个中断，内存就从磁盘换入，这里介绍了一个内存如何从磁盘换入换出数据的算法。

李治军在课程中基本上是结合着代码来讲解OS的，那操作系统主要有哪几块代码呢？OS初始化代码块，进程调度切换以及核心级线程切换代码块，文件系统和设备驱动代码块，管理内存代码块，差不多了吧？了解了这几部分，基本上也就知道操作系统用来干嘛的。有时候确实感慨，计算机行业是不断复杂累积，不断站在巨人肩膀上前进的一门科学，基础就是概括性地了解肩膀下面的学科层次。

## 书籍：CSAPP 2021.7.11

本周中算是正式看完了CSAPP: Computer Systems A Programmer's Perspective. 是去年八月份海淘了英文原版，因为CS领域，但凡有书单，言必及之。但是初看时还是颇为艰难，前半年也就看了开头两章，直到今年二月份开始，笃定每天早起来作为routine，即便每日只一点点时间（其实近来，如若在宅工作，动不动一个小时往上走），日积月累，这本一千多页砖头厚的原版CS领域书籍也能够顺利啃完。除去大学里看的菲茨杰拉德的The Great Gatsby, 这是我第一本读完的英文原著，并且来说，CS领域大家力推的也都是原版，就像最近在看的MIT的分布式系统，都是英文字幕，相较之前的SICP课程，配合CSDN上的其他人的理解文章，还是可以顺利看下来的。英语正真做到学以致用，也是致力于口语提升之外另一个值得自我肯定的事情。CSAPP有些学校是作为计算机组成和原理的教材来使用的，它同时也可以作为我CS基础领域温习的一部分，但你深入去看，他其实包括了汇编语言，计算机操作系统，计算机网络等诸多方面的内容，是一本综合性CS著作，而且相当系统，另一个角度看，作为C语言的进阶教材也再合适不过。

至少就C语言而言，一个重要的观点是：The only thing that distinguishes different data objects is the context in which we view them.  本质都是不变的那些二进制，区别在于如何去阐述它们。就存储方式来分，Integer和Floating point两类。仅对C语言而言，Integer分为signed和unsigned，前者采用Two-complement encoding，字节首bit来表示正负，在bit右移操作填1还是0就没有定论，一些嵌入式程序为了便于平台移植就只能采用后者（JAVA就规定了，所以并不需要unsigned），Two-complement encoding可以联想时钟转圈来理解正负之间的变化。因为历史遗留，诸如byte层面的大小端问题，long类型的位数问题，以及类型cast问题就有点tricky，但是总归想，不管计算机底层怎样去实现，目的就是给表层以逻辑自洽，某种程度上，无需关心，你一般也不会去操作Integer中单独一个Byte位。Floating point的存储形式跟Integer完全不同，参考科学计数法，浮点数表示很大的值是一方面，更关键在于其在小数点上的表现力，因为二进制本来就没法完全表示十分位，分数只能是近似值。整形cast 到浮点数是要变bit位的，这跟整数型互转截截补补本质就不同，而且有可能被四舍五入，因为不得不表示为科学计数法，变成小数形式嘛。讲实话，浮点数的存储，分数的近似表示原因我毕业后前几年就完全没有意识，认识实在是太肤浅，而这本书真的是系统性地刨根究底，非常好。

读之前我还不能理解ISA这个概念，ISA就是汇编语言指令集，是一个CPU设计的框架，所以x86和ARM的汇编语言应该是不同的（联系烽火时候的类汇编语言微码），而AT&T or Intel仅仅是同一种汇编语言的不同书写方式罢了。与编译原理侧重编译的过程不同，这里直接讲各个运算符在汇编层面的结果是什么，细到讲各个register的bit构成，来理解为什么寄存器之间，寄存器与地址之间可以赋值，而地址间则不行（超出寄存器bit位了）。通过各个汇编指令如`jmp`的讲解，理解switch基于goto实现的，可读性与效率兼备。所谓的逻辑实现，说到底不就是if/else加loop嘛。各种数据结构也提及，比如a\[i][j\]中\*(a+i)与\*(a[0]+i)不同在machine code上的原因。另外需要注意的是浮点数相对于整型就另用一套寄存器了。

书里自己设计了一套只支持整型从x86-64简化而来的ISA，由15个Program registers, 3个single-bit的conditions codes(CC), PC, 一个status code Stat和Memory五部分组成，来讲解CPU运行一条指令从Fetch到PC update 的6个stages. 细到电路层面的ALU以及register file内部，感慨计算机体系大厦千里之堤，累于毫末。之后在理解CPU分步运行指令基础上理解CPU设计中的Pipeline，尽管只是O(n)量级的速度优化，可见其锱铢必较。因为Pipeline，所以遇到分支，势必需要事先预测，预测失败反而降低效率。

除了CPU自身效率优化策略，编译器层面的优化也很重要。但是编译器毕竟是机器，存在许多优化上的逻辑困难，比如难以判断是否为纯函数，所以对程序员写的代码质量就有要求，比如尽量把函数放到loop外，在loop内尽量少用指针，因为指针相对于变量在汇编层面重复了memory到register的交互过程，简直有点手把手教写代码的感觉...从最底层的ALU来看，乘除相对加减需要花费更大数量级的clock cycle，所以对于编译器而言，针对乘除可以去实现Instrcution-level Parallelism来优化。对于循环，可以使用Loop Unrolling，比如单偶各加来充分利用Pipeline。因为CPU与编译器层面的优化相对于算法就不在一个量级上，略微有点屠龙之技的感觉，理解起来也费劲，所以看得就有些趣味阑珊。

Memory Hierarchy这章就能让你理解本书为什么可以做计算机组成和原理的教材了。存储分层，越小越快，从上到下，分别是嵌在CPU中register，由SRAM构成的多级Cache，DRAM构成的内存（微观结构上，SRAM每个bit上有6个transistors，所以比每个bit 1个transistor的DRAM更稳定更快，因而也更贵。而DRAM相较SRAM更容易受干扰，没法像SRAM那样自动恢复，所以需要内存系统需要定期refresh DRAM。也是因此DRAM更显得dynamic），以及本地磁盘（近来多被SSD所代替，某种程度上说，固态硬盘源于ROM，所以它读写次数有上限，但这个上限够用几百年，所以无所谓），在分布式系统中还可以考虑到远端存储。内存和磁盘间可以越过CPU直接通过DMA(Direct Memory Access)来进行通信，DMA不是一个的硬件设备，而是内存与磁盘独立于CPU的一个逻辑处理。在这个体系之中，下层的存储均可以看成上一层的Cache，从而引入Locality的概念，经常访问的数据应当能被更快得访问到，需要从Spatial（被引用数据的周围）与Temporal（同一数据的反复调用）两方面进行考虑，举例来说双loop访问一个二维数组，row by row就要比column by column具有更好的Locality，效率上更快。

书后半部分就从这个效率问题上转向了，主题更为斑驳。讲Linking，linker主要做两个工作：Symbol resolution和Relocation。这里你得去探讨Relocatable object file怎样加上Shared object file去变成Executable object file. 一个程序根本上讲就是只读的代码段和可读写的数据段两部分，但是因为未初始化及初始化为0的全局变量，静态变量等不必事先占空间，载入时甚至不用去磁盘里swap-in，直接初始化为0就行，所以搞出了.bss来区分.data。对于一个C module而言（通常为一个.c文件），有三种Symbol: 

1. 本模块内的Global symbols;
2. 别的模块里所定义的Global symbols；
3. 本模块内的Static symbols.

以上三种会被记录在.symtab中，不同模块的static symbol可以同名，编译器隐式地会在.symtab中作区分。而该模块中的local variables不被包括，它们跟Linker就没什么关系。某模块引用其它模块的public变量时，需extern声明（单个），或者include包含其声明的头文件，否则gcc编译器一般会报“warning: implicit declaration ”的警告。然后这里还有一套强弱symbol的规则，但是只要你规规矩矩写就不必去考虑这些隐式规则。在静态链接中为什么要引入静态库的概念呢？`.a`文件可以看成一个可分解为多个`.o`文件的集合（所谓的图书馆嘛），一般程序只链接其中一部分`.o`文件合并各数据段重定位即可，若将其编译成一个`.o`文件，则连未被参照的部分也需被链接，造成链接时间和磁盘空间两方面的浪费。对于动态链接，首先在Link阶段，它将Relocation and symbol table info编入Relocatable object file组成Partially linked executable object file, 然后在Load阶段，将Code and data部分通过Dynamic linker载入到Fully linked executable in memory。而这个Code and data位置在User stack和Run-time heap之间，共享库无论是数据部分还是代码部分都不嵌入到可执行文件的相应段中，code部分是共享的，而data部分是各自独立的。因为多进程需要共享code部分，所以引入了PIC技术，一般来说，模块内的code部分和data部分的相对地址是固定的，天然就可以做到绝对地址无关。但是不同模块之间，尤其是共享库之间code部分中的全局变量访问以及函数调用，就要借用GOT表做地址无关，即将code部分中的绝对地址引用放到data部分的GOT表（增加了一个跳转层），从而每个进程就可以通过独立的GOT表对共享库进行访问，从而共享库可以在载入时不用顾忌各个进程之间的引用冲突。也是编译时`fPIC`的由来。这部分最后还讲了几种包裹malloc等库函数的技巧。链接部分之前看《程序员的自我修养》时还是有点云里雾里的，这次又重新理解了一遍。

之后讲OS相关的进程和虚拟地址。`fork`和`execve`通力合作运行一个新的进程，它们的区别在于， 前者是同一个program在不同process之间的复制，而后者是同一个process中不同program的替换。从这里也可以看出程序与进程的区别。进程启动时配置的参数以及环境变量都是从main函数传进去的，它们会被率先压入stack中。运行过程中所进行的Exception handling事实上是一种并行处理，有硬件中断，软件中断（系统调用），以及Faults和Aborts（后两者的区别主要在于错误是否可以被re-executing）四种类型，像page faults就是Faults的一种。Low-level hardware exceptions are processed by the kernel's exception handlers and would not normally be visible to user processes. Linux signals provide a higher-level software form of exception mechanism to allow processes and the kernel to interrupt other processes.  Signal handling跟main routine同样是并行，有时候可以打断read/write等system call，像键盘typing Ctrl+C 和 kill进程本质都是Signal这个机制，感觉也可以算进程之间的一种通信形式吧。这部分最后讲了`setjmp`和`longjmp`两个函数，这两个函数配合，可以无视the normal call-and-return sequence, 达到不同函数间跳转的效果。C++和JAVA的异常处理机制就源于C这个特性实现的。事实上，还可以配合抓取Signal，比如Typing Ctrl+C不去终止进程，而是跳到该进程某一位置，从而形成不断Ctrl+C，不断循环的结果。进一步，可以根据此，实现协程。比如在golang语言中的for几个go关键字，相当于设置多个`setjmp`，然后在main routine中，`longjmp`按照分配器算法选择其中一个`setjmp`跳，在所跳到的协程中，根据相应条件`longjmp`到另外一个`setjmp`中，从而完成协程之间的切换。

MMU就像Virtual Memory的边界物理墙，CPU到MMU这一段是Virtual Memory，之后就是实际地址了。一般OS课程讲虚拟地址不会去讲MMU，但本书就深入进去，甚至聊到MMU中cache，这个目前的我有点读不进去了。而MMU另一侧，就是个磁盘换出换入算法，一般来说，因为Locality的关系，Page Faults没有预想中的频繁。OS课程之外，另外有不少细节的东西，比如`copy-on-write`技术和`mmap`函数。进程之间read-only的physical memory基本是共享的，诸如同一程序不同进程的代码区，又如C program requires functions from the standard C library such as `printf`. 但是，值得注意的是，对于某些进程的private area，也存在共享的可能性，先给其标read-only的flag，然后进程一旦试图去写入，就trigger a protection fault，然后it creates a new copy of the page in physical memory. 也就是说不更改不copy，便于节省空间。这被称为`copy-on-write`技术，`fork`函数就是基于此技术；Linux中的两种共享内存方式：IPC(shm)和存储映射I/O(mmap). 传统的read/write对文件的操作：

1. 从disk往内存缓存copy.
2. 再从内核缓存往用户空间copy.

而`mmap`只需要从disk到用户空间一次数据copy，效率更高，并可实现不同进程间共享内存，相互通信的方式。这部分最后是讲Dynamic Memory Allocation，这里隐含着如何malloc与free堆上内存的策略，做得高效率和高利用率，有许多算法事实上是本书最具挑战性的一个实验，没去深入研究这个实验的我果然还是浅尝辄止。进一步，C事实上也可以像JAVA一样做一个Garbage Collectors，但因为C does not tag memory locations with any type information. 所以会过于保守，而没法确保清除干净。

接下来讲I/O，是否可以这样理解，Linux下一切皆文件，而System-Level I/O就是操作这些文件，所以Open, Close, Read，Write理所当然是最主要的几个接口了。I/O里一个重要issue是所谓的`short count`，即`read`或`write`的时候，读到或写入的字节数比所设定参数值要少有时也需要正常返回。之后书中介绍了一种`RIO` Package，在这之前需要了解Standard I/O（`fopen`, `fread`, `fwrite`, `fclose`等）和Unix I/O（`open`, `read`, `write`, `close`等）的区别：前者主要定义在`<stdio.h>`头文件里，倾向于将stream放在内buffer里来减少频繁地system call，这大概是其整体设计策略。而`RIO` Package中的`rio_readnb`与标准IO库一样是可以带有缓冲的，意思是它在内核区可以申请一块缓存区，而且是为每个file descriptor分配，在`rio_reaninitb`中申请，所以是线程安全的。相对于UNIX I/O级别的`read`而言，避免了频繁地在用户态与内核态之间进行切换，不言而喻效率更高。更主要是`RIO` Package中的`rio_writen`跟UNIX I/O级别的`write`一样不带缓冲（只是增加了错误类型修复），所以它相对于带缓冲且支持格式化输入输出的标准IO库的`fwrite`更适合网络编程（因为`short count`机制的原因）。

书中的网络编程部分就利用这个`RIO` Package。先介绍了一遍socket相关函数，然后在此基础上手写了一个Tiny Web Server，让人见识到其核心逻辑是如此简洁。Web services其实也是从去远端服务器拿文献这一行为发展过来的，只是相较于FTP，The main difference is that Web content can be written in a language known as HTML. An HTML program(page) contains instructions(tags) that tell the browser how to display various text and graphical objects in the page.  Web servers provide content to clients in two different ways:

1. Fetch a disk file and return its contents to the client. The disk file is known as **static content** and the process of returning the file to the client is known as **serving static content**.
2. Run an executable file and return its output to the client. The output produced by the executable at run time is known as **dynamic content**, and the process of running the program and returning its output to the client is known as **server dynamic content**.

这个就是存储静态资源的Apache和存储动态资源的Tomcat的由来。而书中的Tiny Web Server就实现了这两类资源处理逻辑，对于动态资源，Arguments for GET requests are passed in the URL. 然后`fork`+`execve`启动一个符合CGI standard的进程, 将URL里的参数通过设置环境变量形式传进去。

本书最后一部分讲Concurrent Programming。Concurrency含义比较广，包括前面的异常处理。但是一般来说，我们讲Application-level concurrency, 主要是以下三种：

1. Processes;
2. I/O multiplexing;
3. Threads.

1和3就跳过，就讲这个2. Suppose you are asked to write an echo server that can also respond to interactive commands that the user types to standard input. In this case, the server must respond to two independent I/O events, which event do we wait for first?  事实上，就是instead of waiting for a connection request by calling the `accept` function, we call the `select` function, which blocks until either the listening descriptor or standard input( 两个不同的fd ) is ready for reading. 建立在I/O multiplexing基础上的 Event-Driven Server 可以被看成一个状态机。相对于process-based designs, I/O Multiplexing各个event之间的通信更为方便，因为有相同的上下文，但其不适合多核处理器。还有一节比较重要的就是Semaphores，概括来说就是两个操作：

1. P(s)：如果s不为0，则减一返回，为0则suspend the thread;
2. V(s):   就是将s增一返回，如果当前thread从0变成非0，则restart;

对于一个两元信号灯，就是就是个`mutex`，可以用于互斥，但Semaphores另一个重要用处在于schedule。比较经典的有生产消费问题和读者作者问题，书里有详细论述。可以理解之前OS网课中IPC机制为什么读讲一个Semaphores，因为进程间通信不是重点，主要是想说明并行时的互斥与调度问题。其中最需要提防的就是死锁问题，规则只有一条：Mutex lock ordering rule: Given a total ordering of all mutexes, a program is dead-free of each thread acquires its mutexes in order and release them in reverse order.

## 网课：分布式系统 2021.8.8

差不多整四个月看完《麻省理工 6.824 分布式系统 2020 春季》课程，教授据说是之前蠕虫病毒的发明者，是继《SCIP》后第二个完整看下来的全英文课程，字幕都是英文，课程主要就是解读分布式发展历程上许多重要的论文，主要就来源于各大巨头科技公司。结合CSDN上相关资料基本上还是能够大概理解的。总共二十节课，三十小时左右。

一开始就介绍了Google闻名于世的分布式运算程序的编程框架MapReduce，用来处理大数据，其核心就是处理上的Parallelism，把输入的大数据切割，对应许多map任务，进行shuffle并行处理，最后归纳reduce，输出。后半段讲的Spark与此类似。Spark这个分布式框架，是每台worker都运行同一段代码，处理不同的数据，中间会有key的集约交互。相比MapReduce，Spark在各个worker交互过程中用的是内存，而不是磁盘，所以I/O操作更少。课程中还专门就一段PageRank代码来逐行解读Spark的运行过程。Google 使用的大数据平台主要包括 3 个相互独立又紧密结合在一起的系统：Google 文件系统（Google File System，GFS），针对 Google 应用程序的特点提出的 MapReduce 编程模式，以及大规模分布式数据库 BigTable。这就是江湖流传的谷歌三宝。GFS 的系统架构主要由一个 Master Server（主服务器）和多个 Chunk Server（数据块服务器）组成。Client 和 Master Server 之间只有控制流，没有数据流，因此降低了 Master Server 的负载，Client 与 Chunk Server 之间可以直接传输数据流。BigTable没有具体介绍，好像是一个非关系型数据库，按照我的理解，GFS 解决Google 海量数据的存储问题，MapReduce解决如何从这些海量数据中快速计算并获取期望结果的问题，而BigTable则是解决这些海量数据的组织问题。

Bigtable没具体说，后半段倒是有介绍谷歌另一个分布式数据库Spanner，与 BigTable 不同的是，Spanner 会把time stamp分配给数据，这种非常重要的方式，使得 Spanner 更像一个多版本数据库。而这个time stamp使用GPS和原子钟，所以即便Spanner全球分布，也能提供统一的更新时间，强一致性来处理read-only commit和一小部分的read-write commit. 视频中先介绍了Spanner基于paxos与Two-Phase commit之上的transaction的流程。Transactions现在多用于database中，但是其实它是一种普遍的设计思想，遵循ACID( Atomic, Consistent, Isolated, Durable ). 而分布式中的Transactions要多加一个concurrency control, 有两种策略：pessimistic，就是获得lock改数据，否则delay，修改成本大；optimistic，就是直接改，最后check有没有其他人修改，有的话abort重来。而Two-Phase commit是一种基于transaction上的protocol，用于协调transaction中不同部分在不同服务器上的分工合作。所以Two-Phase commit是针对一个transaction而言的，而不是针对一个paxos group.  这就意味着所谓的coordinater及其它是针对不同的数据而言，比如存储于两个不同的paxos group的trasaction部分，因为paxos算法机制原因，不用担心Two-Phase commit的availability.  前面讲到乐观并发控制，就不得不提一下FaRM，相较Spanner来说还不完善，并且不同于Spanner拥有多个data centers,  FaRM就只有一个，尽管FaRM也有Primary, Backup的fault tolerance机制，加之其为了提高速度（速度可以比Spanner快上几百倍）使用的都是RAM，所以一旦整个data center断电，它采用的是每台service都有独立备用电池... FaRM为了提高速度，还是非常激进的，直接对硬件层面做了很大的改动，相应OS也要适配，比如Kernel Bypass: Application越过kernel，不用system call，直接访问Network Interface Card的stack告诉它怎么做，然后网卡通过DMA直接访问Application的memory. 还有RDMA( Remote Direct Memory Access):  本地电脑直接通过两个网卡交互，直接访问远端Application的memory，不仅不经过kernel，甚至远端Application也不知道，这就需要两个网卡提供一种类似kernel中的TCP协议栈的protocol来保证reliability. 因为transaction和RDMA看起来不怎么兼容，所以FaRM采用Optimistic Concurrency Control.

Paxos是分布式容错系统的一种一致性算法，大概因为晦涩课程中没专门介绍，而是介绍了paxos的改进版RAFT. 在分布式系统中，两个client可以一对一访问两个server，但每个client不能同时访问两个server（分布式的意义所在）如果network部分线路有问题，就会造成这样叫做Split Brain的问题。面对以上这个Fault，所提出来的策略是：把n个server组成的分布式系统扩展为n+1个server，取其中n个工作，一旦出现以上错误，就可以交替，这好像叫做：majority voting system. RAFT似乎是一种这样的distributed system算法，由一个leader server和多个follower server组成，由leader主要跟client交互，然后发送给follower们让它们进行更新日志等操作，只要有其中一个follower完成response给leader后，leader就给client回复了。这其中最关键的是Leader Election，每个server都有一个竞选时间片，它们要设定为长度不一并且一定间隔，每当其中一个竞选成功，就reset所有的server的timer，然后各个server会在将来不同时间竞选，就很容易得到大多选票，不至于有竞争对手。Zookeeper就是基于RAFT算法基础上的一个分布式服务框架。zookeeper允许client直接访问副本服务器，前提是只读访问，这样可提高效率。但这个也带来一个问题，恰好主机正在做更改，还没通知到followers，事实上这就不是linearizable. 所以zookeeper只能做到linearizable writes，并且保证client request order做到FIFO，这样的话，虽然没法保证client读到最新状态的值，但一定程度上只要等待一定长的时间，总能得到正确的值。顺便提一下课程之外的信息：Hadoop是一个由Apache基金会所开发的分布式系统基础架构，主要解决，海量数据的存储和海量数据的分析计算问题（就是大数据分析），其中的HDFA 、 Spark、 HBase 分别是Google的GFS、MapReduce、BigTable这三个思想的实现。而Zookeeper是其中提供分布式协调一致性服务的一个组件。

Chain replication是相对于RAFT更为 linearizability strong consistency的算法模型，类似链表，就是从head写到tail，读的时候反方向，这样就可以保证强一致性。但是，中间两台server网络断了，就会造成split brain的问题。相对于Chain Replication的扩展的CRAQ流行于读为主的分布式存储架构中，它通过允许向任意的数据节点发送读请求来增强读的读取速率，同时它依旧提供强一致性的保证。但是网上似乎没有特别有名这个算法应用的例子，不知道是这篇论文太新还是有别的实用性上的缺点...

大数据这一块由此看来是Google引导的，不愧是世界首席科技公司。本课程中也提到其它公司的论文，比如Amazon的Cloud Replicated DB, Aurora. 插个题外话，说到云计算，就不得不提AWS(Amazon Web Service)。就其发展历史而言，刚开始是EC2(Elastic Compute Cloud)，用于给客户提供云上虚拟服务器来云计算，但是database一旦crash，没有备份。所以发展出EBS(Elastic Block Store)，同一个data center（比如一幢大楼里许多计算机）中多台计算机去备份这个database，然后EC2来mount这个EBS，解决单台EC2 crash的问题，但是进一步，如果整个data center crash的话怎么办？RDS（给各种关系型数据库，如MySQL，SQL Server，Oracle等设置操作的服务）比EBS更进一步，在多个data center进行备份，问题是通过网络传输的数据量要远大于修改量，就会降低速度。Aurora与RDS相比，多个data center之间只是传输日志文件，这就提高了performance. 另外它还采用了Quorum机制：假设Aurora在三个data center备份了6台计算机，那么只要其中最快的4台完成备份，它就可以返给client OK了，对于读操作而言，读3个副本就能读到此次更新的数据。这是另外一个提高performance的策略。据说，以上两个策略，帮助Aurora比RDS快了35倍。Aurora更GFS类似，分为database server和 storage server,  前者像是管理，后者是真正的储存。因为read request远多于write request的实际情况，除了一台main database server之外，还可以加许多read only database server，可以在不打扰main的情况下直接去读storage server。而Dynamo是Amazon一种的NoSQL数据库的分布式 key/value 存储引擎，好像只被亚马逊内部服务使用。课程中没具体说，只是作为Causal Consistency的一种实现提了一下。Causal Consistency遵循一些操作的因果准则，这些规则在同一个线程内的操作之间以及在与数据存储交互的不同线程的操作之间创建了潜在的因果关系，这个模型，并不允许线程直接通讯，而是通过数据存储进行通讯。如发照片添链接，不能点开链接没照片。能够提供因果+一致性causal+ consistency的集群被称为COPS(Clusters of Order Preserving System)。

除了Google和Amazon, 课程中另一家论文被提到的公司是Facebook. Facebook的Web服务器在Apache等Front-ends服务器和数据库服务器之间，加了层硬件改造更为快速的缓存服务器Memcached( 其实就是利用CPU利用率高，内存利用率低的计算机 )。所以缓存服务器与数据库服务器之间的数据一致性就成了issue，Cache Consistency不用在意秒级的，即稍微不一致也可以，但是对于用户自己所更新的数据，就有更高的要求。另一方面因为缓存服务器承担了绝大部分的访问，一旦crash，Front-ends直接访问数据库的话，数据库服务器承受不住，所以需要在缓存服务器这一层做fault tolerance. 然后众多web服务器和众多Memcached服务器会组成一个个cluster. 然后各cluster之间还有共享的memcached机群,称为regional pool，来存储那些并不那么hot的key. 如果新增一个cluster的时候，一开始其中的memecached没有任何数据，大概率hit不了cache，要去database取，就会有一段cold time，这个时候可以直接去既存的cluster中取来代替。说到Cache Consistency，课程里还提到了一篇Frangipani的架构的论文，它是本地的file system，位于kernel. 可以调用remote procedure call访问Petal(Share virtual disk，on a separate set of machines , like a disk driver used over a network).  所以Frangipani可以看成Petal一个cache，而且是write-back的。因为在group协同工作中，其实大部分users只是read/write他们各自的files，顶多是去read别人的files，这就给Frangipani作为一个write-back cache的速度优势了。这个模型最关键的问题在于Cache coherence, Atomicity和Crash recovery of individual server. Frangipani会适应于一些小型的协同工作的情况，但一般不太用来设计为大型系统，比如大数据计算，因为cache毕竟容量有限。

课程最后一部分是讲区块链的。区块链这个概念是怎么来的呢？之前有man-in-the-middle attack：截获client发往DNS的包，发fake reply获取用户的密码，代替其登录相应server，通俗讲就是钓鱼网站嘛。为此引入了HTTPS，也称作HTTP over TLS。TLS (transport layer security)的前身是SSL (secure sockets layer)，就是在明文的上层和 TCP 层之间加上一层加密，在浏览器和网站首次商定密钥的时候需要使用非对称加密（公钥，私钥的关系就涉及到费尔马小定律了，求解两个极大的互素数），一旦网站收到了浏览器随机生成的密钥之后，双方就可以都使用对称加密来进行通信。浏览器需要去CA(Certificate Authorities) 获取到网站的公钥，CA机构专门用于给各个网站签发数字证书，从而保证浏览器可以安全地获得各个网站的公钥。Web server和CA 商议公钥的时候，这一事件要公之于众才行，就牵扯出 Certificate Transparency: 证书必须记录到可公开验证、不可篡改且只能附加内容的日志中，用户的网络浏览器如谷歌的Chrome才会将其视为有效。为了避免客户下载整个CT log，其采用Merkel Tree进行存储，本质上是一个二叉树，只是各节点是hash函数，从而将CT log分成许多小块。这样客户就可以判断log是否变动以及迅速锁定log的变动位置，这被称为proof of inclusion。这也是 Block Chain的设计框架的由来，proof of inclusion同样应用于bitcoin。对于Bitcoin，个人理解是每台矿机都可以拥有一个完整的Blockchain，Blockchain中的一个Block记载的是一部分交易信息，每十分钟会产生一个新的Block并由这个Block来集约记载新的交易信息（所以Bitcoin支付至少得花费十分钟来确认），然后遍历全部网点，产生这个新的Block需要计算一个hash，计算出这个hash的computer会被奖励一定量的比特币，这就是比特币的发行。比特币确切的说可以理解为一个账本，但没有账户余额概念，只有一笔笔的从一个账户转到另外账户的转账信息。每当发起一比交易的时候，比特币系统会先通过你的比特币地址查到你之前的所有交易记录，看你是否有足够的钱去支付这笔交易。这个账本是一个全网都有的账本，不归属于某个人，而且全网都一样，每个网络节点人手一份，而且都是相同的。Bitcoin主要的问题是double-spending problem：即使用同一货币进行多次支付，从而达到欺诈的目的。实现这种目的最主要的手段就是算力攻击，若恶意攻击者掌握全网51%的算力，那么算力攻击可能实现。为什么呢？视频中讲Block chain会产生Fork现象，其中一条chain如果slightly longer，就会聚集大量的mining，从而形成长的越长的局面。而另一条chain上的信息就会被遗弃，不merge，所以要求只有确定本chain是winning chain后，其上的transaction才会被正式确认。如果malicious attacker有超过51%的CPU power，就可以强行开出一条更快的Fork chain。除了比特币之外，区块链还有去中心化服务器方面应用的前景，就是BlockStack。视频先介绍了现在普遍的中心化的Web sites构成的优缺点(high performance, low privacy)，然后介绍Decentralized Architecture: 终端上的APP将数据存储于用户独立的Cloud上，而不是所有用户的数据存储于服务器Database中，其它终端通过一定的权限认证可以来此用户的Cloud上访问数据。这里用户名需要跟数据位置以及Public key联系在一起，这种联系就需要通过区块链来记录，秉持先登记有效，后登记被忽略的原则。register name need to pay, free lead to waste domain. Certificate Transparency 相对于Bitcoin而言没法解决 name conflicts的问题。教授对Blockstack应用的前景不是太看好，相较于中心化服务器，似乎编程上会更麻烦...

这个课程还是让我大开眼界，感受到Google, Amazon等这些公司在CS科技前沿的探索创造力，同时也略微皮毛地了解到大数据云计算区块链到底是怎么一回事，这些在其它计算机基础课程中就很少提到，算是CS前沿的一些东西，没想到都是基于Distributed System. 课程事实上还有几个实验，需要用Go语言去完成（我当然只是随便听听，即便这样也是花了三四个月），所以课程前面还专门花了一节课介绍Golang语言，我索性就去B站花了一周看了8小时转职Golang工程师--刘丹冰的讲解视频，以有CS基础为前提的讲解，不是动不动从零开始三四十节课那种，所以还是挺适合我的。Go语言可以解释器直接执行，也可以编译器编译后执行，而且语言层面天然支持并发，几行代码就可以实现协程，能充分利用多核。Golang创建goroutine语法上非常简洁，用一个`go`关键字就可以起协程。协程（co-routine)应该就是用户级线程，然后在Golang语言中被称为goroutine, 并且goroutine在内存开销以及切换速度上做了一定的优化。照理来说，用户级线程没什么意思...但Golang采用核心级线程与用户级线程多对多的形式，通过优化它们之间的协程调度器，内部完成多并发的机制。大概是因为协程创建数量上没有限制，程序可以被分为无数多的time slice，然后通过协程调度器分发到多个核心级线程完成并行，所以Golang程序在多核处理器的机器上才能发挥出它的优势来吧。协程之间通过channel通信，感觉类似于进程之间的消息队列，通过`make`就可以创建一个channel，也很简洁。可以用`close`关闭channel，关闭后可以从那里接收数据，但不能往里面写了。单流程下一个go只能监视一个channel的状态，select可以完成对多个channel的监控，类似C语言中select函数监视多个socket等IO状态。除了Golang最有特色的并发机制之外，它的书写方式细节上跟C差异也挺大的，数据类型放在变量名后面，句末可以不加分号，变量声明时可以用`:=`直接表示初始化并赋值，类型等信息自动匹配等。import有点C的include头文件感觉，函数名首字母大小写决定是当前包私有函数还是公有函数。golang也支持指针，虽然不常用。Golang上的面向对象是基于结构体的，通过使用`this`在结构体外绑定方法组合成一个对象表示。数据结构上还有动态数组slice，Golang中的`map`就跟Python中的`dictionary`一样，是一个`key:value`键对形式的数据结构。Golang中实现多态的方法是通过接口interface，interface{}还可以当万能数据类型，相当于C中的void*, 但是有更多功能，比如类型断言机制。我推测：在C语言中，程序运行时应该是没有数据类型的信息的，所以没有相关操作去专门获取数据对应的类型，但是在编译阶段的语义分析阶段，编译器自身（注意不是该程序自身）应该是维护着该程序数据与类型对应的结构体信息，所以编译时类型不匹配时会报warning。进一步讲，程序运行时虽然没有相关数据类型的信息，但是编译器已将将其转化为特定的寻址方式，比如说float类型与int类型存取时的寄存器就不一样，这已经反映在编译后的汇编代码里，也正因为如此，一定程度可以反汇编。与C相比较而言，Golang它程序自身估计也有类似编译器一样的数据与类型对应的结构体信息，这应该就是作者所说的`变量的内置pair结构`，所以interface{}不仅可以通过数据名去判断其值，而且可以通过数据名去断言其类型。此外还有`defer`等特殊关键词，字面上就是推迟执行的意思。这样的新的编程语言一边看下来似懂非懂，像C开发了几年不懂的地方还是有，所以只能便开发边学习，如果有机会的话。

## 总结

随着前两天看完分布式系统，历时整两年时间，我的计算机科学自学旅程完结了一个阶段，就基础性的东西系统性地浏览了一遍，刚来日本第一年还问同事数据结构和数据库这两门课有关联吗？真是汗颜，无怪乎人家专业出身的程序员轻视你嘛...2019年六月份开始着手时，学习方式细节上还在摸索，所以像离散数学，算法和数据库，数据库原理等课程，看过也不做笔记，最后只是总结回忆一下，真的只是有这个概念这样非常浅的了解。2021年以来，一方面是因为疫情的缘故在宅办公，自己有了比之前多好几倍的自由支配时间，另一方面灵活地利用Github来云上记录笔记，所以像SICP,CSAPP,分布式系统等课程相较而言更能深入细节去了解，尽管跟人家科班生结合Lab学习差距还是很大，但对我这样的意图框架理解的自学者还是充分的。

其实刚开始自学CS，还是困囿于拓展会使用的程序语言上，听说新西兰多用C#和C++，就B站上去找视频，然后想汇编语言，数电模电，Matlab看看？多亏当时同事王桑和超哥指引，自己才发现知识体系中到处是漏洞，现在回想就工作上而言2019年还是非常有启发性的一年，在中国工作的最初三年真的就是不学无术，然后这两年其实也没有人跟我讨论技术上的问题。也就是2019年后半年专门参照宁波大学计算机专业培养方案，大致理了一下需要看的课程，而后2020年末在Github上找到一篇某两位加州大学教员写的如何自学计算机的指引如获至宝，最终按那篇指引上的学习list完成了阶段性的CS自学旅程：

第一个科目是编程，推荐教材就是SCIP，我刚开始看英文视频的时候，一遍下来真是云里雾里，首先LISP这门语言就是大难题，之后专门看了Python版的中文书，看完感慨真是magic！怪不得那么多人推崇。看这本书之前专门先预习了B站上浙大的Python语言课和Github上私人写作者所写的Python学习100天，从后者那里了解到了基本的Web开发框架是怎样的，这也很重要，不然都看不懂那些招聘信息。其实一开始我看的就是清华大学的C++录制课，结合自己工作中的C语言，对编程构成了一个完备基础，只是C++学习时，确实还是缺少系统性笔记，比较遗憾，但是最重要的面对对象的设计思想还是有印象的。往后来说，确实应该不会无缘无故去学习某门语言，但是应对工作的不同，用别的语言，比如JAVA开发的情况发生的话，在像C一样着重去研究了解一下。

第二个科目是计算机架构，推荐教材是CSAPP。知乎上一搜，CS领域最有名两本书就是SICP和CSAPP了（另一个版本多加一本算法导论），今年把这两本书终于啃完，尤其是CSAPP还是去年八月份买的英文原版，不得不为自己点赞。讲道理，往后CS学习主要还是聚焦于英文资料，另一方面也算是英语的学以致用。CSAPP事实上是一本综合性教材，其就CPU架构，内存层次等的深入挖掘还是独此一家。前年也看了一本，算是科普读物性质的《编码》，也比较浅显地讲了这个主题。现在别人在面前讲DDR3，SSD，显卡什么的，甚至组装电脑，不至于啥都不懂对吧。

第三个科目是算法与数据结构。所看的是清华邓俊辉的公开课，知乎还是挺有名的，但我顶多就是浅尝辄止了，虽然很多人讲这是CS的核心，硅谷大厂面试你不去Leetcode刷几百道题目都没资格，但是就目前为止我的工作经验来看，实在是没有任何地涉及，大概是需要进入上层设计的研究领域？目前为止对于我而言还是屠龙之技，所以没有什么动力去进一步了解，只能说以后工作真的有涉及的话。当然就单说一个二叉树去理解区块链这种level那还是没什么问题，讲道理B树红黑树一类的看的时候还是能明白的。

第四个科目是数学知识。看了一个北大公开课《离散数学》，这算是计算机科学的数学了，但其还是宽泛介绍了整个数学史，还是相当好看的。之前略微了解了下机器学习，各种途径讲数学之重要性，我目前也在看吴军的《数学之美》，其中介绍谷歌自然语言处理，搜索，大数据等计算机科学领域中数学的应用，有那么些瞬间看得心潮澎湃，都买了大学高数书准备再研究一下来着，觉得数学作为今后的床边阅读材料会不会是有益的尝试...讲实话而言，我的数学知识本来就不怎么样，看编译原理的自动机什么的也总是磕磕绊绊。

第五个科目是操作系统。B站第一OS课，哈工大李治军，看完名不虚传，手把手教你写操作系统。本来以为2019年看了Linux内核和一些Linux操作说明以为已经大有精进了，相对于刚开始进程线程都分不大清楚当然要好多了，之后工作上开发多线程程序，独立调查单体环境还是收获颇丰，但看李治军的课，还是有新的收获，用户级线程以及Linux下一切皆文件的理解上加深一层。

第六个科目是计算机网络。B站上韩顺平的课是超哥推荐的，他当时也是借着现场开发的通信设备软件的契机，而我更是在这之前就是做交换机的，OSI模型刚明白，这个课程之后呢，加上应用层的一些知识和之后工作中对socket的研究，对整个网络知识稍微有些融会贯通。其实计算机网络是计算机质变的东西，像Web开发也离不开这个。我结合目前为止的工作经验结合着理解，一方面也促进自己对所做工作的钻研。

第七个科目是数据库。去年上半年看了MySQL和东南大学的一个数据库原理，起因是跟人面试接触的时候人家问起了动不动SQL语言。因为这五六年来的工作中完全不涉及数据库这一块，但是对于大部分IT工作而言这是很重要的一部分。这就挺矛盾的，算是我这几门课里最薄弱的一块，当时看的时候因为自己也没搞过，印象也比较模糊，顶多只有个概念，只能希望与以后工作时接触的话进一步了解。

第八个科目是编程语言和编译器。年初结合斯坦福和哈工大的课程看，这个呢属于我自己没看懂，编译原理还是有点难，尤其在概念先行的情况下。倒是看SICP时候解析过一个计算器语言的简易编译器，其实这个最有效就是读一个编译器源代码就行，但是似乎也没必要，GCC用得6就行，语言细部编译原理能够理解就好，这就足够为以后学习新的语言打下基础了。另外联系所看的《程序员的自我修养》，虽然说主要是讲链接的，但可以相联系思考。编译原理跟算法一样属于暂时不想深入了解的两门课。

第九个科目是分布式系统。看了推荐的麻省理工的课程。严格意义上上是开眼界的一门课，大数据云计算区块链各方面都有涉及，算是前沿发展趋势，可以作为以后工作方向的参考。

以上就是近两年回顾计算机科学基础课程的list了，基本上是以自己现有的工作为基础，在大范围涉猎的情况下对局部进行加深理解，然后对未来的发展方向有一个认识。之后确实计划看一点机器学习入门，但主要还是当作一个了解方向，我并不觉得自己已经有能力去进入这个领域。今年过后工作方向肯定是要进行调整的，假若工作上有野心，那么安逸绝对不能成为继续工作的理由，这个现场学到了很多东西，但是明显地它越来越少了，无论是CS本身的联系以及业务上的理解。之后有可能去做Web框架，在Cloud的基础上，将数据库与分布式系统学以致用，应该不太会去回到嵌入式往底层挖，那就偏硬件不是我的方向。当然如果能往机器学习区块链方向走，那再好不过。不过目前最为可行的还是云计算，如果能把英语利用上也行啊。All right，今年余下几个月，这方面就要深度探索一下了，不能拖延，比如从猎头公司闻讯入手。