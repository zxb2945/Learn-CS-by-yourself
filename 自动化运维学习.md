# 自动化运维学习

# Ansible

## 历史与安装

历史：

> Ansible 是由 Michael DeHaan 于 2012 年开发的。最初，Ansible 是作为一个个人项目启动的，旨在简化 IT 自动化和配置管理。2013 年，Ansible 被开源，并迅速受到社区的欢迎。
>
> 在随后的几年里，Ansible 逐渐发展，增加了许多功能，支持各种平台和服务。2015 年，Ansible 被 Red Hat 收购，并继续在其基础上发展，成为现代 IT 基础设施管理的重要工具之一。

如何安装:

Ubuntu

```
sudo apt update
sudo apt install ansible
ansible --version
```

CentOS/RHEL

```
sudo yum install epel-release
sudo yum install ansible
ansible --version
```

> Ansible 采用的是一种“无代理”（agentless）的架构，这意味着你不需要在目标服务器上安装 Ansible。Ansible 通过 SSH（对于 Linux/Unix 系统）或 WinRM（对于 Windows 系统）与目标机器通信，直接在控制节点（Ansible 所在的服务器）上执行命令和剧本。

## Ansible运维自动化

[Link](https://www.bilibili.com/video/BV1DY4y137Be?p=2&spm_id_from=pageDriver&vd_source=6fc477a8e79179a3fd30bed2e2ba5fbe)

ansible是新出现的自动化运维工具，基于Python开发，集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，实现了**批量**系统配置、**批量**程序部署、**批量**运行命令等功能。

### Ansible Inventory 20240903

在hosts文件里写各种主机的IP，可以分组，有专门的格式要求

```
[web]
172.16.1.5 
172.16.1.6
[db]
172.16.1.7
[nfs]
172.16.1.11
```

甚至可以将用户名和密码写进主机清单里，但是不推荐

```shell
#在hosts文件里写各种主机的IP，可以分组
#-i inventory list 指定主机清单
#-m 指定模块
#-a 指定动作
ansible -i hosts all -m ping

#从ansible所在主机远程操作
ssh 172.16.1.6 'lis -l .ssh'
```

### Ansible模块 20240904

| 命令/脚本模块 |                                                              |
| ------------- | ------------------------------------------------------------ |
| ping模块      | 检查ansible与主机连通性                                      |
| shell模块     | 批量执行shell命令                                            |
| command模块   | 不指定模块,默认就使用该模块，批量执行简单命令                |
| scripts模块   | 分发脚本并执行，在ansible上写个脚本分发到各个主机执行，比如安装Python |

调用具体模块要什么参数，查阅ansible官方网站

#### 

| 文件相关模块       |                                |                                                              |
| ------------------ | ------------------------------ | ------------------------------------------------------------ |
| file模块           | 文件目录创建，删除...          |                                                              |
| copy模块           | 远程复制，分发文件             | ansible -i hosts all -m copy -a 'src=/etc/hostname dest=/tmp/' |
| **服务管理模块**   |                                |                                                              |
| systemd模块        | 服务的开机自启动，服务开启关闭 |                                                              |
| service模块        |                                |                                                              |
| **软件包管理**     |                                |                                                              |
| yum_repository模块 | 给远端配置一个yum源文件        | ansible -i hosts lb -m yum_repository -a '...'               |
| yum模块            | 安装文件                       | ansible -i hosts lb -m yum -a 'name=nginx state=latest'      |
| get_url模块        |                                |                                                              |
| **系统管理**       |                                |                                                              |
| moute模块          | moute远程挂在nfs               | ansible -i hosts web -m mount -a 'fstype=nfs ...'            |
| cron模块           | 定时任务模块                   |                                                              |
| **用户管理**       |                                |                                                              |
| group管理          |                                |                                                              |
| user模块           |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |



ansible现在有上千的模块...

跳板机/堡垒机: Jump Server, 通过跳板机管理其他所有机器

网站架构-运维角度：Developers => VPN通道 => 跳板机 => 批量管理：Ansible => 监控，日志收集



ansible返回的结果通常是json结构

### Ansible剧本(Playbook) 20240918

对于重复性操作与部署推荐使用剧本（脚本），使用模块+固定格式（对齐）

剧本格式：yaml格式，yml尾缀的文件

Playbook组成

Ansible Playbook 示例：

```yaml
---
- name: 安装和配置 Nginx
  hosts: webservers
  become: yes  # 以超级用户身份运行任务
  vars: #定义在 playbook 中使用的变量
    nginx_port: 80
    nginx_server_name: "example.com"
  tasks:
    - name: 确保 Nginx 已安装
      apt:
        name: nginx
        state: present
      notify:
        - 重启 Nginx

    - name: 配置 Nginx
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
      notify:
        - 重启 Nginx

    - name: 确保 Nginx 服务正在运行
      service:
        name: nginx
        state: started
        enabled: yes

  handlers:
    - name: 重启 Nginx
      service:
        name: nginx
        state: restarted

```

你可以使用 `ansible-playbook` 命令来运行这个 playbook，例如：`ansible-playbook -i inventory playbook.yml`



注意空格缩进，还不能用tab键，有些琐碎的格式要求

Playbook中可以定义变量和使用变量，除了自定义，还有内嵌变量facts，获取主机名,ip，cpu信息

可以将Playbook中这些变量抽出到一个独立文件vars_files中维护

> Ansible 中的 `register` 用于捕获任务的输出并将其存储在变量中，供后续任务使用。

总结：ansible中变量定义就是这三种：自定义，facts，register



剧本流程控制：

1. handler触发器 => 关键词 notify 与 handlers 配合，类似于C语言中的“goto”
2. when判断 => when一般与ansible facts变量一起使用，判断主机/判断系统系类
3. loop循环=> 关键词 with items, 批量启动服务，批量添加用户，感觉这里所谓的循环更像 批量操作，事先列出所有项目，所以无所谓某一条件的break

```yaml
---
- name: Install and configure Nginx on Ubuntu
  hosts: webservers
  become: true  # 以超级用户权限执行

  vars:
    nginx_conf_file: /etc/nginx/nginx.conf
    server_blocks:
      - server_name: example.com
        root: /var/www/example
      - server_name: example.org
        root: /var/www/example_org
    is_ubuntu: "{{ ansible_distribution == 'Ubuntu' }}"  # 声明 Ubuntu 条件

  tasks:
    - name: Install Nginx
      apt:
        name: nginx
        state: present
      notify: Restart Nginx  # 触发 handler
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Ensure the Nginx configuration file exists
      file:
        path: "{{ nginx_conf_file }}"
        state: touch
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Configure Nginx server blocks
      template:
        src: nginx.conf.j2  # 模板文件
        dest: "{{ nginx_conf_file }}"
      notify: Restart Nginx  # 触发 handler
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Create document root directories
      file:
        path: "{{ item.root }}"
        state: directory
        owner: www-data
        group: www-data
        mode: '0755'
      with_items: "{{ server_blocks }}"
      when: is_ubuntu  # 仅在 Ubuntu 上执行

  handlers:
    - name: Restart Nginx
      service:
        name: nginx
        state: restarted
```

简单的 Nginx 配置模板nginx.conf.j2：

```
http {
    {% for block in server_blocks %}
    server {
        listen 80;
        server_name {{ block.server_name }};

        root {{ block.root }};
        index index.html;

        location / {
            try_files $uri $uri/ =404;
        }
    }
    {% endfor %}
}

```

> **Handler**：
>
> - 在 Playbook 的最后部分定义。名为 `Restart Nginx` 的 handler 会在其他任务通过 `notify` 指令触发时被执行。这里，它会重新启动 Nginx 服务
>
> **`with_items` 循环**：
>
> - `with_items` 用于循环创建多个文档根目录。这里通过 `server_blocks` 变量中的每个项目，创建对应的目录



对于比较复杂的剧本，可以进行拆分，分成记录主要步骤的主剧本 以及 一些yml小剧本，即所谓的 include机制

还可以用roles规则对这一些yml小剧本分门别类放在不同的路径中，使文件层次更为清晰



### Ansible项目  20241028

部署架构的时候，拆分不同的组件/模块，通过Ansible roles部署每个模块

1. 基础环境：yum源，用户，关闭防火墙，内核参数... （模板机做了）
2. 应用/服务环境：服务 nginx php 安装配置启动， 通用独立全面，根据不同的服务应用拆分 LNMP
3. 业务环境代码：比如部署Java的代码

=>可以对开发完成的项目一键部署，虽然没使用过，但是NTC现场完全可以用Ansbile来升级的



1通过服务器部署及配置流程-->转换为对应的步骤

2根据步骤转化成模块

3书写剧本，调试剧本

1. 安装：使用什么模块
2. 配置：使用什么模块
3. 启动与使用：使用什么模块
4. 最后书写剧本roles



# Terraform 

### Terraform Crash Course 20250208

[link]([The Terraform Crash Course](https://www.youtube.com/watch?v=4ukf6-S8wL4))

> Terraform 是一个 **基础设施即代码（Infrastructure as Code, IaC）** 工具，由 **HashiCorp** 开发。它用于**自动化管理云基础设施**，可以让你使用代码来定义、部署和管理基础设施资源（如服务器、数据库、网络等）。
>
> ### **Terraform 的核心特点**
>
> 1. **声明式（Declarative）**
>    - 你只需描述最终的基础设施状态，Terraform 会自动计算并执行必要的更改。
> 2. **跨云平台**
>    - 支持 AWS、Azure、Google Cloud、Kubernetes、VMware、阿里云等多种云平台，甚至可以用于本地数据中心。
> 3. **状态管理（State Management）**
>    - Terraform 维护一个 **状态文件（tfstate）** 来跟踪当前的基础设施状态，并确保所有更改都可预测和可追踪。
> 4. **模块化（Modular）**
>    - 可以使用 **模块（Modules）** 复用代码，提高管理效率。
> 5. **可审计 & 可版本化**
>    - 基础设施代码可以存入 Git 进行版本管理，方便审计和协作。
>
> 
>
> ### **Terraform 工作流程**
>
> 1. **编写配置文件**（.tf 文件）
>    - 使用 HCL（HashiCorp Configuration Language） 或 JSON 描述基础设施。
> 2. **初始化（terraform init）**
>    - 下载相关插件和依赖项。
> 3. **执行计划（terraform plan）**
>    - 预览 Terraform 计划做出的更改，不会真正修改基础设施。
> 4. **应用变更（terraform apply）**
>    - 执行变更，创建或更新基础设施。
> 5. **销毁资源（terraform destroy）**
>    - 删除所有配置的资源，释放云平台上的基础设施。
>
> 
>
> ### Terraform 示例
>
> 下面是一个创建 AWS EC2 实例的 Terraform 配置文件（`main.tf`）：
>
> ```
> provider "aws" {
>   region = "us-east-1"
> }
> 
> resource "aws_instance" "example" {
>   ami           = "ami-12345678"
>   instance_type = "t2.micro"
>   
>   tags = {
>     Name = "TerraformInstance"
>   }
> }
> ```
>
> 执行以下命令：
>
> ```shell
> terraform init      # 初始化 Terraform
> terraform plan      # 预览变更
> terraform apply     # 应用变更，创建实例
> terraform destroy   # 删除实例
> ```
>
> 
>
> ### **Terraform vs. 其他 IaC 工具**
>
> | 工具               | 语言                 | 主要特点           | 适用场景               |
> | ------------------ | -------------------- | ------------------ | ---------------------- |
> | **Terraform**      | HCL（声明式）        | 多云支持，状态管理 | 适用于大规模云基础设施 |
> | **Ansible**        | YAML（命令式）       | 偏向配置管理       | 配置服务器和应用部署   |
> | **CloudFormation** | JSON/YAML            | 仅支持 AWS         | 适用于 AWS 生态        |
> | **Pulumi**         | Python/Go/TypeScript | 编程式 IaC         | 适合 DevOps 和开发者   |
>
> 
>
> ### **Terraform 适用于哪些场景？**
>
> - **多云管理**（AWS + Azure + GCP）
> - **自动化基础设施部署**（创建服务器、数据库、网络等）
> - **DevOps CI/CD**（结合 GitOps，自动部署环境）
> - **Kubernetes 资源管理**
> - **灾难恢复（DR）**（一键恢复基础设施）



用vs编辑尾缀为tf的文件，加入相应插件

如何安装 Terraform，这个像一个编译器？terraform plan直接编译tf文件，甚至terraform apply运行它，然后可以得到一个tfstate的描述设施状态的文件

还有其他命令 terraform destroy

接下来Manage Resource on AWS:

创建一个 IAM 得到key和密码可以访问终端，在vs上用export命令注册它？

如果要创建一个EC2，你可以访问[link](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)，查看该如何定义它在tf文件中，定义resource，data等，所以这方面你都不用记忆什么东西，最后通用运行它



### Complete Terraform Course

[Link](https://www.youtube.com/watch?v=7xngnjfIlK4&t=8883s)

#### 简介 20250218

1. Evolution of Cloud + Infrastructure as Code
2. Terraform Overview + Setup
3. Basic Terraform Usage  =>展示了一个main.tf文件用于创建Router53 LoaderBalacner等一个后后端架构的创建
4. Variables and Outputs
5. Language Features =>HCL语言
6. Projest Organization + Modules
7. Mnaging Multiple Environments
8. Testing Terraform Code
9. Developer workflows

Declarative vs. Imperative

Provisioning + Config Management => Terraform+Ansible

Provisioning + Orchestration=> Terraform+Kubernetes

> Terraform 通过 **Provider**（提供者）与云平台进行交互，每个 Provider 都封装了云平台的 API，Terraform 通过声明式配置管理资源。

安装Terraform就是在vscode一行命令的事，拿到AWS的IAM，然后通过tf脚本就可以操控AWS了

#### Terraform State File

> Terraform **State File**（状态文件）是 Terraform 用来跟踪和管理基础设施资源的文件。它存储了 Terraform 管理的所有资源的当前状态，并在 `terraform apply` 运行时用于比较和决定哪些更改需要执行。
>
> State 文件是一个 **JSON 格式** 的文件，包含 Terraform 追踪的所有资源。例如：
>
> ```json
> {
>   "version": 4,
>   "terraform_version": "1.5.0",
>   "serial": 1,
>   "resources": [
>     {
>       "mode": "managed",
>       "type": "aws_instance",
>       "name": "web",
>       "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
>       "instances": [
>         {
>           "schema_version": 1,
>           "attributes": {
>             "id": "i-1234567890abcdef0",
>             "ami": "ami-0abcdef1234567890",
>             "instance_type": "t2.micro",
>             "private_ip": "10.0.1.25",
>             "public_ip": "34.203.120.12"
>           }
>         }
>       ]
>     }
>   ]
> }
> 
> ```
>
> 默认情况下，Terraform 将 `terraform.tfstate` 存储在 **本地目录**（即执行 `terraform apply` 的目录）。但对于团队协作，建议使用 **远程存储**，如：AWS S3 + DynamoDB

#### HCL语言 20250220

> HCL（HashiCorp Configuration Language）是一种专为基础设施即代码（Infrastructure as Code, IaC）设计的声明式配置语言，由 HashiCorp 开发。它主要用于 HashiCorp 的工具，如 **Terraform**、**Consul** 和 **Nomad**，用于定义和管理云基础设施、服务发现和编排任务。
>
> **声明式语法**：HCL 采用类似 JSON 的结构，但比 JSON 更具可读性，同时支持嵌套和复杂配置。

介绍了HCL语言中的Variable Types：Input Variables, Local Variables, Output Variables

tf采用的HCL语言还是有许多内容的...

可以把Sensitive Data存储在一个单独的tfvars文件中，通过pass的手段传递，这还有另外的好处，tf文件相当于一个函数，根据不同的参数可以deploy到不同的云设施中去

Meta-Arguments:

> 在 Terraform 的 HCL 语言中，**Meta-Arguments（元参数）** 是特殊的参数，它们可以用于 **控制资源的创建、依赖关系和循环逻辑**，从而增强 Terraform 配置的灵活性和可复用性。
>
> Terraform 中的 Meta-Arguments 主要包括以下几个：
>
> | Meta-Argument | 作用                                      |
> | ------------- | ----------------------------------------- |
> | `count`       | **创建多个相同资源**（基于整数值）        |
> | `for_each`    | **基于 map 或 set 迭代创建多个资源**      |
> | `depends_on`  | **显式定义资源之间的依赖关系**            |
> | `provider`    | **指定特定的 provider（云提供商）**       |
> | `lifecycle`   | **控制资源的生命周期（防止删除/更新等）** |
>
> 如果你在 CI/CD 和 Cloud 领域工作，Meta-Arguments 在 **自动化基础设施管理** 里非常重要：
>
> - `count` 和 `for_each` 适用于 **批量部署 Kubernetes、Lambda、VM**
> - `depends_on` 确保 **CI/CD 运行顺序正确**
> - `provider` 适用于 **多云环境**
> - `lifecycle` 可以防止 **意外破坏生产环境**

相当于HCL语句中的关键字

Provisioners：

> `Provisioners`（供应器）是 Terraform 用于在 **资源创建后** 或 **销毁前** 执行本地或远程命令的功能。它们通常用于：
>
> - **配置服务器**（如安装软件、修改系统设置）
> - **执行脚本**（如 Bash、PowerShell）
> - **文件传输**（如上传配置文件）
> - **处理资源删除逻辑**（如运行清理脚本）
>
>  **常见的 Provisioners**
>
> | Provisioner 类型 | 作用                                 |
> | ---------------- | ------------------------------------ |
> | `local-exec`     | 在 **本地主机** 上执行 Shell 命令    |
> | `remote-exec`    | 在 **远程主机** 上执行 Shell 命令    |
> | `file`           | 将 **本地文件或目录** 复制到远程主机 |
> | `chef`           | 运行 Chef 客户端进行配置管理         |
> | `puppet`         | 运行 Puppet 客户端进行配置管理       |
>
> ### **示例：Terraform 部署后发送 Slack 通知**
>
> ```
> hCopyEditresource "aws_instance" "example" {
>   ami           = "ami-12345678"
>   instance_type = "t2.micro"
> 
>   provisioner "local-exec" {
>     command = "curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"EC2 已创建!\"}' https://hooks.slack.com/services/xxx"
>   }
> }
> ```
>
> 🔹 **作用**：Terraform 创建 EC2 之后，发送 Slack 通知。
>
> **官方不推荐使用 Provisioners**，因为 Terraform 更倾向于声明式配置，而 Provisioners 属于命令式配置。如果可以，建议用 Ansible、Cloud-Init、User Data 等替代。



#### Terraform Module

Why Modules? 

Infrastructure Speclialists + Applicaiton Developers

> **Module（模块）** 是 Terraform 里的 **可复用代码单元**，可以用来组织、管理和复用 Terraform 配置。你可以把 Terraform Module 理解为 **函数或库**，它能：
>
> - **封装** 复杂资源，提供简单的接口
> - **提高代码复用性**，避免重复写相同配置
> - **简化维护**，让基础设施更易管理
> - **支持参数化**，通过变量自定义资源行为
>
> ### **Module 的基本结构**
>
> 一个 Terraform Module 其实就是 **一个包含 `.tf` 配置文件的目录**。比如，下面的目录结构就是一个 Module：
>
> ```
> perlCopyEditmy-module/
> │── main.tf      # 核心资源定义
> │── variables.tf # 定义输入变量
> │── outputs.tf   # 定义输出变量
> ```
>
> 在 Terraform 配置中，你可以 **调用** 这个 Module，并传递参数来创建资源。
>
> ## **什么时候使用 Module？**
>
> | 场景                                  | 适合使用 Module？            |
> | ------------------------------------- | ---------------------------- |
> | **多个环境（dev/prod）**              | ✅ 是，封装后只需改变量       |
> | **重复使用相同资源（VPC, EC2, RDS）** | ✅ 是，提高复用性             |
> | **团队协作**                          | ✅ 是，模块化结构更清晰       |
> | **简单单一的资源**                    | ❌ 否，可以直接写在 `main.tf` |

可以自己写Module，然后在主要tf中引用，也可以直接引用远程仓库中的module

设计Module就像设计软件开发中各种类对吧，需要好好思考

> Terraform Registry 是 Terraform 官方提供的 **模块和提供商（Providers）仓库**，你可以从这里下载和使用现成的 Terraform 资源。
>
> Terraform Registry 主要包含两大类：
>
> 1. **Providers（提供商）**：AWS、Azure、GCP、Kubernetes 等
> 2. **Modules（模块）**：VPC、EC2、EKS、RDS 等

你可以去下载Providers，也可以进一步下载相应的模块，你还可以自己上传Module到远端仓库



#### Terraform Workspaces

> **Terraform Workspaces** 允许你在 **同一套 Terraform 配置** 下管理多个独立的环境（如 `dev`、`staging`、`prod`）。它的作用类似于 **命名空间**，可以用来管理多个不同的 **状态（state）**。
>
> 在 Terraform 中，默认的状态文件（`terraform.tfstate`）存储资源信息。如果你需要多个环境（如 `dev` 和 `prod`），直接使用 `terraform.tfstate` 可能会导致环境混乱。
>
> **Workspaces 让你可以在同一份配置代码下，创建多个状态文件，分别管理不同的环境。**
>
> **每个 Workspace 都有独立的 `terraform.tfstate` 文件**，不会互相覆盖。

可以通过像git那样切换不同的环境

> ## **Workspaces vs. 变量文件（tfvars）**
>
> | 方式                     | 适用场景                                            | 优势                       | 缺点                           |
> | ------------------------ | --------------------------------------------------- | -------------------------- | ------------------------------ |
> | **Terraform Workspaces** | 管理多个环境（dev, staging, prod）                  | 同一份代码，多环境独立管理 | 在复杂项目中可能难以管理       |
> | **变量文件（tfvars）**   | 通过 `terraform apply -var-file` 传递不同环境的变量 | 灵活控制变量               | 需要额外维护多个 `tfvars` 文件 |
>
> 对于小型项目，**Workspaces 更方便**。
> 对于大型项目，推荐 **用 `tfvars` 文件管理环境**，如：



> [Terragrunt](https://github.com/gruntwork-io/terragrunt) 是 Terraform 的一个 **辅助工具**，它帮助管理 **多环境（multi-environment）基础设施**，解决 Terraform **代码重复** 和 **State 管理** 的问题。
>
> 👉 **简单来说，Terragrunt 是 Terraform 的一个 Wrapper，帮你优化代码结构和管理远程状态！**

用了很大篇幅讲file structure，相当于C#设计各种类，让项目更容易管理

#### Test and Develop 20250221

**Code Rot**（代码腐烂）：指的是**代码质量随着时间的推移逐渐恶化，变得难以理解、维护或扩展**，最终影响软件的稳定性和开发效率。

Static Check：比如用terraform fmt -check这个命令来检查

Manual Testing

Automated Testing: 可以用go语言写脚本测试..可以在脚本中模拟发送HTTP，就在不实际操作server的情况下完成单体测试？

至于开发，可以用Github Action，GitLab的CICD功能

Cloud Nuke: 一次性清除单一账号的所有云上资源的工具，避免被计费



你可以用Github Action

1. 写个terraform.yml
2. 设置连接AWS上的服务器测试
3. 在这个服务器上操作各种Terraform命令
4. 甚至还要调用go来测试



### Learn Terraform with Azure by Building a Dev Environment 20250331

[Link](https://www.youtube.com/watch?v=V53AHWun17s)

Install Azure CLI on Windows  => reopen vscode 就可以在terminal 运行 `az login` 等Azure的命令

查阅Azure Provider，用于Terraform调用

Terminal Command:

```shell
terraform fmt
terraform plan

terraform plan -destroy   #适用于检查销毁前的影响  
terraform apply -auto-approve #适用于自动化环境（如 CI/CD），但在生产环境要谨慎使用
terraform state list
terraform apply -refresh-only
terraform output
terraform console
```

main.tf:

```terraform 
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }
  required_version = ">= 1.0"
}

provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "example" {
  name     = "example-resources"
  location = "East US"
}

resource "azurerm_virtual_network" "example" {
  name                = "example-network"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
  address_space       = ["10.0.0.0/16"]
}

resource "azurerm_subnet" "example" {
  name                 = "example-subnet"
  resource_group_name  = azurerm_resource_group.example.name
  virtual_network_name = azurerm_virtual_network.example.name
  address_prefixes     = ["10.0.1.0/24"]
}

resource "azurerm_network_security_group" "example" {
  name                = "example-nsg"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name

  security_rule {
    name                       = "allow-ssh"
    priority                   = 1001
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "22"
    source_address_prefix      = "*"
    destination_address_prefix = "*"
  }
}

resource "azurerm_public_ip" "example" {
  name                = "example-public-ip"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
  allocation_method   = "Dynamic"
}

resource "azurerm_network_interface" "example" {
  name                = "example-nic"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name

  ip_configuration {
    name                          = "internal"
    subnet_id                     = azurerm_subnet.example.id
    private_ip_address_allocation = "Dynamic"
    public_ip_address_id          = azurerm_public_ip.example.id
  }
}

resource "azurerm_virtual_machine" "example" {
  name                  = "example-vm"
  location              = azurerm_resource_group.example.location
  resource_group_name   = azurerm_resource_group.example.name
  network_interface_ids = [azurerm_network_interface.example.id]
  vm_size               = "Standard_DS1_v2"

  storage_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "18.04-LTS"
    version   = "latest"
  }

  storage_os_disk {
    name              = "example-os-disk"
    caching           = "ReadWrite"
    create_option     = "FromImage"
    managed_disk_type = "Standard_LRS"
  }

  os_profile {
    computer_name  = "example-vm"
    admin_username = "adminuser"
  }
}
```

你可以对tf文件做一些修改，增加SSH登录功能，然后为VScode增加remote-SSH的扩展，你可以直接登录资源中的VM,是那种左侧都会出现Linux目录的那种深度登录哦。

> Terraform `.tpl` 文件通常用于 `templatefile()` 或 `data "template_file"` 方式，来动态填充变量。
>
> `.tpl` 文件本质上是一个文本文件，其中可以使用 `${}` 语法来插入变量。
>
> ```shell
> Hello, ${name}! Welcome to ${company}.
> ```

> 如果你需要在 VM 启动时执行某些初始化任务（如安装软件、配置用户），可以使用 `.tpl` 文件来动态生成 `cloud-init` 配置

你可以在 main.tf中使用 provisioner去调用tpl文件来为VM预置Docker等其它操作。

=>不太推荐provisioner，更多应该是Ansible的工作，因为Terraform无法探知provisioner所带来的状态变化，你需要 `terraform apply -replace resource_name`来使其provisioner生效





# Jenkins

[Jenkins自动化部署入门详细教程](https://www.cnblogs.com/wfd360/p/11314697.html)

## 尚硅谷Jenkins教程

[link](https://www.bilibili.com/video/BV1bS4y1471A?spm_id_from=333.788.videopod.episodes&vd_source=c13700902d2c98df282b6f1f2889c0cb&p=4)

### GitLab介绍及安装准备20241202

GitLab所在的服务器至少16G内存

> GitHub 本身是一个由 GitHub 公司托管的 **云服务**，并没有开放完整的自托管版本。GitHub 主要是提供云托管服务的，因此它不能像 GitLab 那样直接在自己的服务器上搭建和运行。
>
> 然而，**GitHub 提供了 GitHub Enterprise**，这是一款针对企业的私有部署版本，允许企业在自己的服务器上运行 GitHub。

> **GitLab.com** 是 GitLab 提供的云托管平台，类似于 GitHub.com，提供免费的基础功能和收费的企业级功能，适用于个人、小型团队到大型企业。
>
> 与 GitHub 相比，GitLab 在 **DevOps** 集成方面提供了更多功能，尤其在 **CI/CD** 和 **自动化管理** 上更为强大，适合需要全流程集成的团队

=>Github偏向于 云服务托管代码， GitLab偏向于给企业提供一个DevOps解决方案

不仅可以用yum直接安装在linux上，还可以利用docker安装GitLab：

> **快速部署**：Docker 提供了 GitLab 官方的镜像，使用 Docker 安装 GitLab 无需繁琐的手动配置和依赖管理。只需几个命令就可以启动 GitLab 实例，大大简化了安装过程。
>
> **预配置环境**：Docker 镜像已经包含了所有 GitLab 运行所需的组件（如数据库、Redis 等），无需额外配置。
>
> **跨平台兼容性**：Docker 可以在多种操作系统上运行，包括 Linux、Windows 和 macOS，因此不需要担心操作系统的兼容性问题。
>
> 总的来说，使用 Docker 部署 GitLab 不仅能减少安装、配置和维护的难度，还能提高灵活性和可扩展性，特别适合在 DevOps 或大规模开发环境中使用。

=>GItLab花钱的话可以直接CICD流程，而不用去额外用Jenkins

### Jenkins安装 及简单配置 20241204

需要JAVA安装环境 Java8(JRE或者JDK都行)

在一个linux上安装好，指定端口，就可以通过浏览器登录Jenkins界面了

同一台服务器再装个Maven便于之后编译Java项目代码



在Jenkins界面 输入 Repository URL （记得服务器上提前安装git哈）

Java 用Maven，如果是C/C++项目还可以有 CMake的插件配置 来构建项目



安装一个 Publish over SSH插件, 添加一台目标服务器用来运行构建好的 项目

=> 连 Exec command 这样的也可以在 Jenkins界面 上直接填... 还有 Remote Directory



总结而言有这么几天服务器： 自己的local，gitlab服务器，Jenkins服务器自动编译好，发送到test server用来启动

最简陋的CICD就构建好了，但是怎么配置一push就运行呢？





### Pre Steps 20241206

DashBoard

General,源码管理，构建触发器，Pre Steps，Build,Post Steps,构建设置，构建后操作



Pre Steps部分 可以对 目标服务器（测试用）进行一些操作，通常就在目标服务器上写个脚本,把占用着的端口的进程kill掉一类的，由Jenkins进行调用（Send file or execute commands over SSH）

### 构建触发器  20241208

在Jenkins构建触发器下 创造一个 URL，然后gitlab可以通过这个URL来调用Jenkins来触发自动构建，相当于在Jenkins上手动按下构建按钮

（有可能要安装 Plugin Manager的插件来解决验证问题）



提交代码就自动构建过于频繁，由于对于大项目而言，一般是在GitLab界面上合并分支后 自动构建

 利用GitLab Webhooks去觉察到 合并分支 这一事件 然后回调 Jenkins提供的URL 进行实现

GitLab：设置->Webhook => 有许多出发来源，包括 push，merge等事件



结论是 一push就运行的话 Jenkins必须结合GitLab来完成



上面所说的这种构建方式 其实是 Jenkins 的 触发远程构建（就是jenkins本身只提供一个API出去），此外还有其他构建方式，比如跟其他项目相关联，比如 Build after other projexts are built, 定期构建(Build periodically)， 以及Jenkins主动去查看git地址有没有变化进行构建(Poll SCM) 等



定时执行任务：Jenkins有建立在基本的cron表达式上加了Hash值的专用表达式，cron表达式就是年月日时分

在Jenkins界面上(Build periodically)打勾，日程表中填入cron表达式



Poll SCM：也需要输入一个cron表达式 去探测 GitLab上探测代码是否有变化



最后还可以在 DashBoard->Configure System中配置邮箱接受构建通知，比如从你的一个163邮箱发送到你的QQ邮箱

### 容器化构建 20241209

三种方式：

1. 外挂目录
2. jar包直接打包到镜像里：启动一个虚拟机测试docker；去docker官方市场里直接找支持jdk的镜像，在虚拟机某一路径创造dockerfile；在dockerfile中加入 jar包路径 然后启动docker
3. 在2的基础上生成新镜像，推送到docker私服中，比如Harbor，然后再由k8s集群自动管理部署容器。大型项目基本采用这个模式，是主流。小型项目说实话也不太用1与2，直接就不用容器了哈哈

dockerfile可以作为代码一部分推送到git上

在Jenkins中的Pre steps与Post Steps中相应改成停止以及启动docker等的设置

### Jenkins集群并发构建

集群化构建可以有效提升构建效率，尤其是团队项目比较多或是子项目比较多的时候，可以并发在多台机器上执行构建

Jenkins服务器多启动两台

在第一台Jenkins找到 Manage nodes and cloud这个界面，增加其他Jenkins服务器的节点；然后到General中找到 并发构建 按钮

### 流水线 pipeline 20241210

Jenkins的流水线配置文件Jenkinsfile就像docker的dockerfile

Dashboard->mypipeline: General,构建触发器，高级项目选项，流水线

流水线配置文件 是一个由特殊语法结构的运行于 Jenkins服务器上的 脚本

举例：

```json
pipeline {
    agent any  // 定义流水线的执行环境，any表示可以在任何可用的代理上运行

    environment {
        // 定义环境变量
        CC = 'gcc'  // C 编译器，设置为 GCC
        BUILD_DIR = 'build'  // 存放构建产物的目录
    }

    stages {
        // 阶段：准备环境
        stage('Prepare') {
            steps {
                echo 'Preparing environment...'
                script {
                    // 清理旧的构建文件
                    sh 'rm -rf ${BUILD_DIR}'  // 删除旧的 build 目录
                    sh 'mkdir ${BUILD_DIR}'  // 创建新的 build 目录
                }
            }
        }

        // 阶段：构建
        stage('Build') {
            steps {
                echo 'Building the project...'
                script {
                    // 编译 C 项目
                    sh '${CC} -o ${BUILD_DIR}/my_program src/*.c'  // 使用 GCC 编译 C 源文件
                }
            }
        }

        // 阶段：测试
        stage('Test') {
            steps {
                echo 'Running tests...'
                script {
                    // 假设有一个测试可执行文件，运行测试
                    sh './${BUILD_DIR}/my_program --test'  // 运行测试命令
                }
            }
        }

        // 阶段：部署
        stage('Deploy') {
            steps {
                echo 'Deploying the application...'
                script {
                    // 假设部署是将可执行文件上传到远程服务器
                    sh 'scp ${BUILD_DIR}/my_program user@server:/path/to/deploy'  // 使用 SCP 上传程序
                }
            }
        }
    }

    post {
        success {
            echo 'The build and deploy process succeeded!'  // 成功时输出
        }
        failure {
            echo 'The build or deploy process failed.'  // 失败时输出
        }
    }
}
```

> **Jenkins Pipeline**：
>
> - **Jenkins Pipeline** 是 Jenkins 提供的一种自动化构建和部署的方式，旨在帮助开发团队定义、执行、自动化整个 CI/CD 流程。
> - 它使用 **Jenkinsfile** 来定义工作流，支持更复杂的任务，例如构建、测试、部署、集成等，并且支持阶段性控制、并行执行、条件执行等特性。
>
> **一般的构建**：
>
> - **一般的构建** 通常是通过简单的 **Jenkins 构建任务** 来实现的。它通常由一系列预定义的步骤组成，直接在 Jenkins UI 中配置，不一定依赖于脚本或代码文件（例如 `Jenkinsfile`）。
> - 这种方式适用于简单的构建流程，通常用于没有复杂流程控制的场景。

=>Pipeline 在 任务并行，可视化(与Blue Ocean UI结合)，版本控制等方面更有优势

脚本可以由UI配置信息自动生成。 还有声明式流水线与脚本式流水线的区别，后者用 Groovy 脚本来编写，结构上可以更自由

Git上有多分支，可以为多个分支上传不同的 Jenkinsfile，达成可以为多分支并行构建的效果 

## Github Action 20241231

CI中还可以有 代码风格 check

在代码里 .github/workflows/文件夹下创建一个 yml文件就行

```yaml
#.github/workflows/ci.yml
name: C Project CI

on:
  # 当推送或创建 PR 到 main 分支时触发工作流
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build-and-check:
    runs-on: ubuntu-latest # 使用 Ubuntu 最新版本作为运行环境

    steps:
      # 步骤 1: 检出代码仓库
      - name: Checkout code
        uses: actions/checkout@v3

      # 步骤 2: 安装必要工具，包括构建工具和 MISRA 检查工具 (Cppcheck)
      - name: Install build tools and MISRA check tool
        run: sudo apt-get update && sudo apt-get install -y build-essential cppcheck

      # 步骤 3: 编译项目
      - name: Build project
        run: make

      # 步骤 4: 运行测试
      - name: Run tests
        run: ./build/project

      # 步骤 5: 执行 MISRA 检查
      - name: Run MISRA check
        run: cppcheck --enable=all --std=c99 --suppress=missingIncludeSystem --error-exitcode=1 src/ > cppcheck-report.txt

      # 步骤 6: 上传 MISRA 检查报告
      - name: Upload MISRA report
        uses: actions/upload-artifact@v3
        with:
          name: cppcheck-report
          path: cppcheck-report.txt
```



**GitHub Actions** 提供自己的 **云服务器**（也称为 **runner**）来运行工作流任务。

(如果需要更高的灵活性，可以设置自托管服务器（如企业内部服务器或专用云实例）)

## GitLab CICD 20250120

> ## **1. GitLab CI/CD 的核心概念**
>
> ### **1.1. GitLab Runner**
>
> GitLab CI/CD 依赖 **GitLab Runner** 来执行 CI/CD 任务。Runner 是一个独立的进程，可以运行在本地机器、服务器或 Kubernetes 集群上，支持多种执行环境（Docker、Shell、Kubernetes 等）。
>
> ### **1.2. `.gitlab-ci.yml` 配置文件**
>
> 所有 CI/CD 流程都由 `.gitlab-ci.yml` 文件控制，存放在项目根目录。它用于定义 **pipeline**（流水线）、**stages**（阶段）、**jobs**（作业）等内容。
>
> **示例：**
>
> ```yaml
> yamlCopyEditstages:
>   - build
>   - test
>   - deploy
> 
> build-job:
>   stage: build
>   script:
>     - echo "Building project..."
>     - make build
> 
> test-job:
>   stage: test
>   script:
>     - echo "Running tests..."
>     - make test
> 
> deploy-job:
>   stage: deploy
>   script:
>     - echo "Deploying..."
>     - make deploy
>   only:
>     - main
> ```
>
> - **stages**：定义流水线的不同阶段（如 `build`、`test`、`deploy`）。
> - **jobs**：每个 `job` 代表一个任务，属于某个 `stage`，可以执行 shell 命令。
> - **only**：指定在哪些分支触发此任务（如 `only: - main` 表示仅在 `main` 分支触发）。
>
> ------
>
> ## **2. GitLab CI/CD 工作流程**
>
> GitLab CI/CD 通过以下流程来自动化开发和部署：
>
> 1. **开发人员提交代码** 到 GitLab 仓库。
> 2. **GitLab 自动触发 Pipeline**，读取 `.gitlab-ci.yml` 进行构建、测试和部署。
> 3. **GitLab Runner 运行 CI/CD 作业**，执行 `build`、`test`、`deploy` 等任务。
> 4. **测试通过后**，可以自动或手动部署到测试/生产环境。
> 5. **GitLab 提供 CI/CD 可视化界面**，可以查看 Pipeline 运行状态、日志等。

 =>在GitLab界面中点击setting就可以看到Runner栏了，可以设置一个Runner，并为其设定target server的IP，所以界面中的Runner并不是正真的运行程序，只是一个指向性的设置，最终要去target server里安装这个runner才能运行

> ## **1. 服务器上的基本配置**
>
> 假设你要在服务器 `192.168.1.100` 上运行 GitLab Runner，需要进行以下设置：
>
> ### **1.1. 安装 GitLab Runner**
>
> 如果该服务器尚未安装 GitLab Runner，需要先安装：
>
> ```
> bashCopyEdit# 添加 GitLab Runner 仓库
> curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash
> 
> # 安装 GitLab Runner
> sudo apt-get install gitlab-runner -y   # Ubuntu/Debian
> # sudo yum install gitlab-runner -y     # CentOS/RHEL
> ```

=>配置release/gitlab-runner/config/config.toml，比如

```toml
[[runners]]
  name = "docker-runner"  # Runner 的名称，在 GitLab CI/CD 界面中可见
  url = "https://gitlab.example.com/"  # GitLab 服务器地址
  token = "your-runner-token"  # 该 Runner 连接到 GitLab 所需的 Token
  executor = "docker"  # 设定 Runner 以 Docker 模式执行 CI/CD 任务

  # 自定义构建目录（默认情况下，Runner 在 `/builds/` 目录执行 CI 任务）
  [runners.custom_build_dir]
  # 如果启用 `custom_build_dir`, 可以指定不同的路径来存放 CI/CD 任务的构建目录
  # 例如:
  # enabled = true
  # build_dir = "/my/custom/path"
  # 但一般情况下保持默认即可，不需要修改
  

  [runners.docker]
    tls_verify = false  # 是否开启 TLS 验证（一般用于私有 Docker Registry，默认 false）
    image = "python:3.9"  # 运行 CI/CD 任务时默认使用的 Docker 镜像
    privileged = true  # 允许 `privileged` 模式（若需要在 CI/CD 任务中运行 Docker in Docker，必须设为 true）
    disable_entrypoint_overwrite = false  # 允许覆盖 Docker 镜像的 Entrypoint，通常保持默认即可
    oom_kill_disable = false  # 是否禁用 OOM（内存不足）自动终止，默认 false，建议不修改
    disable_cache = false  # 是否禁用构建缓存，默认 false（开启缓存能加速任务）
    
    # 挂载卷（volumes）
    volumes = [
      "/cache",  # GitLab Runner 的缓存目录，保存构建中间结果，加快后续任务
      "/var/run/docker.sock:/var/run/docker.sock"  # 允许 Runner 访问宿主机 Docker（用于 `docker build`）
    ]
    
    shm_size = "256m"  # 共享内存大小，避免某些程序因共享内存不足崩溃（默认 64MB，建议设为至少 256MB）

  [runners.cache]  # 缓存配置
    [runners.cache.s3]  # S3 远程缓存（如果使用 AWS S3 作为构建缓存，可以在这里配置）
    [runners.cache.gcs]  # Google Cloud Storage 缓存（如果使用 GCS 存储 CI 任务的缓存，可以配置此项）

```

=>上面这个token就是要去GitLab界面注册的Runner里去寻找，这样的话两者就联系起来了

# Kubernetes

## 尚硅谷Kubernetes教程

[Link](https://www.bilibili.com/video/BV1w4411y7Go/?p=2&spm_id_from=pageDriver&vd_source=6fc477a8e79179a3fd30bed2e2ba5fbe)

2015年Google用Go语言开发了Kubernetes框架

Kubernetes属于PaaS

轻量级，开源，弹性收缩，负载均衡

Pod是Kubernetes管理上的最小单位，管理一定数量的Docker



### 知识图谱 20240920

1. 介绍说明：Kubernetes的前生今世
2. 基础概念：什么是Pod 控制器类型 网络通讯模式
3. 构建K8S集群
4. 资源清单： 资源清单的语法 编写Pod 掌握Pod的生命周期(**important**)
5. Pod控制器：掌握各种控制器的特点以及使用方式定义
6. Service原理：给顾客一个访问接口，掌握Service原理及其构架方式
7. 存储：掌握多种存储类型的特点
8. 调度器：能够根据要求把Pod定义到想要的节点运行
9. 集群安全机制：集群的认证，鉴权，访问控制 原理及其流程
10. HELM：类似Linux中yum，只是不安装包而是某个功能的集群
11. 运维：修改Kuberadm 达到证书可用期限为10年，构建高可用Kubernetes

### 组件说明 20241102

前身 Borg ： paxos是分布式一致性算法

K8S架构：

master 节点：scheduler,replication controller,api server,kubectl,etcd =>这些模块全部在同一个物理服务器上

kubectl： 命令行

api server： master 节点的中心组件，所有服务访问的统一入口

replication controller: 维持副本期望数目

scheduler:负责给Docker介绍任务，选择合适的节点进行分配任务

etcd ： 分布式键值存储数据库，为整个集群存储一些关键数据(持久化)。可以借助etcd进行数据恢复，采用HTTP协议



node 节点：kubelet， kube proxy, node(可以看成Docker(最主流) )

Kubelet：直接跟容器引擎交互实现容器的生命周期管理

Kube proxy: 负责写入规则至 IPTABLES, IPVS实现服务映射管理



其他插件：CoreDNS，Dashboard(给K8S集群提供B/S架构)...

### Pod概念

在同一个Pod中 许多容器 共享网络与存储卷，互相之间无需配置IP直接通信

> 在 Kubernetes（K8s）中，Pod 是一个基本的调度单位，用于部署和管理容器化应用。Pod 的概念可以理解为一组共享资源和网络的容器。以下是 Pod 的一些关键特点和概念：
>
> ### 1. **单个或多个容器**
>
> - 一个 Pod 可以包含一个或多个容器，这些容器共享相同的网络命名空间和存储卷。它们通常一起工作，以实现特定的功能。
>
> ### 2. **共享网络**
>
> - 所有在同一 Pod 中的容器共享一个 IP 地址和端口空间。这意味着它们可以通过 `localhost` 直接互相通信，而不需要通过网络进行路由。
>
> ### 3. **存储卷**
>
> - Pod 可以挂载一个或多个存储卷，以便容器可以共享数据。这使得数据在容器重启或替换时得以持久化。
>
> ### 4. **生命周期管理**
>
> - Kubernetes 管理 Pod 的生命周期，包括创建、调度、健康检查、重启等。Kubernetes 会根据配置自动处理 Pod 的状态。
>
> ### 5. **负载均衡**
>
> - 在使用服务（Service）时，Kubernetes 可以自动为 Pod 提供负载均衡，使流量均匀分配到多个 Pod 实例上。
>
> ### 6. **调度**
>
> - Kubernetes 的调度器负责将 Pod 分配到合适的节点上，以确保资源的高效利用和服务的可用性。
>
> ### 7. **标签和选择器**
>
> - Pod 可以使用标签（Labels）来标识和组织。通过选择器（Selectors），可以方便地管理和选择特定的 Pod 进行操作。
>
> ### 8. **用途**
>
> - Pod 通常用于运行微服务、任务队列、数据处理作业等。它们是构建应用程序的基本单元，允许将多个相关容器组合在一起。
>
> ### 总结
>
> Pod 是 Kubernetes 的核心概念之一，提供了一种灵活的方式来管理和协调多个容器的运行。理解 Pod 的工作原理有助于更好地利用 Kubernetes 来部署和管理容器化应用。

=> 每个Pod 一个IP，Pod内部的容器通信通过 localhost+端口通信

> 假设一个 Pod 内有两个容器：`web` 和 `app`，`web` 容器需要调用 `app` 容器提供的服务。可以通过以下方式进行通信：
>
> 1. **在 `app` 容器中启动一个服务**，监听某个端口（例如 8080）。
> 2. **在 `web` 容器中通过 `localhost:8080`** 发送请求：
>
> ```shell
> #localhost 就是 127.0.0.1
> curl http://localhost:8080
> ```

### 集群安装

> ### **集群（Cluster）**
>
> - **定义**：Kubernetes 集群是由一组计算资源（节点）组成的集合，这些节点可以是物理服务器或虚拟机。集群运行 Kubernetes 控制平面和应用程序的工作负载（即 Pod）。
>
> - 组成
>
>   ：
>
>   - **控制平面**：负责管理集群状态，调度 Pod，处理 API 请求等。主要组件包括 API Server、etcd、Controller Manager 和 Scheduler。
>   - **工作节点（Worker Nodes）**：运行实际的容器化应用（Pod）。每个节点都包含运行容器的必要组件，如 kubelet、kube-proxy 和容器运行时（如 Docker 或 containerd）。

=>Cluster分为控制平面和工作负载，后者的主体就是Pod

k8s集群举例: master01 node01 node02 Router Harbour

用VMWare创建5个虚拟机

在master01里进行如下操作：

1. 首先指定主机名，直接改hosts文件就行，安装依赖包
2. 设置防火墙为Iptables并设置空规则，关闭SE Linux，防止其影响k8s的运行效率
3. 调整内核参数，对于K8S. 关闭系统不需要的服务，配置日志服务
4. kube-proxy 开启ipvs的前置条件
5. 安装Docker 软件
6. 安装kubeadm （`kubeadm` 是 Kubernetes 提供的一个工具，用于简化 Kubernetes 集群的安装和管理。）
7. 初始化主节点, 加入工作节点，部署网络

再构建私有仓库Harbour

> Harbor 提供了一个集中式的地方来存储和管理 Docker 容器镜像，使得在 Kubernetes 中部署和更新应用时可以更加方便。持对镜像进行版本管理，可以轻松回滚到之前的版本，提高应用的可维护性。可以方便地与 CI/CD 工具链（如 Jenkins、GitLab CI 等）集成，实现持续集成和持续部署的自动化流程。Harbor 提供了一个用户友好的 Web 界面.

### 资源清单(manifest) 20241106

> Kubernetes（简称K8s）的资源清单（manifest）是用于定义和描述Kubernetes集群中资源对象的配置文件，通常是一个或多个YAML或JSON文件。这些资源对象可以包括Pod、Deployment、Service、ConfigMap、Secret等，几乎所有在K8s中运行的应用和服务都需要在资源清单中进行描述。

#### YAML的基本语法

YAML采用缩进表示层级关系，**通常使用空格**（而不是Tab）进行缩进。每一级缩进通常为两个空格。

支持的数据结构：对象(键值对)，数组，纯量(scalars)->不可再分割的量

> ### YAML的基本语法
>
> YAML的核心原则是简单、直观和易于理解，下面是一些基本语法规则和示例：
>
> #### 1. **基本结构**
>
> YAML采用缩进表示层级关系，通常使用空格（而不是Tab）进行缩进。每一级缩进通常为两个空格。
>
> ```yaml
> key: value
> ```
>
> 在YAML中，数据由键（key）和值（value）组成。键值之间使用冒号`:`分隔。
>
> #### 2. **键值对**
>
> 最简单的形式是键值对，每个键后跟一个冒号和一个值：
>
> ```yaml
> codename: Alice
> age: 30
> ```
>
> #### 3. **多行字符串**
>
> YAML允许用`|`和`>`来表示多行字符串，`|`保持换行，而`>`将换行符转换为空格。
>
> ```yaml
> address: |
>   123 Main St
>   Apt 4B
>   Springfield, IL
> ```
>
> 或者，使用`>`来把换行合并为单个空格：
>
> ```yaml
> description: >
>   This is a very long description
>   that will be wrapped into a single line.
> ```
>
> #### 4. **数组**
>
> YAML中的数组（列表）通过在每个元素前面加上破折号（`-`）表示：
>
> ```yaml
> fruits:
>   - Apple
>   - Banana
>   - Orange
> ```
>
> #### 5. **字典（Map）**
>
> 字典是由多个键值对组成的，**键值对之间用换行分隔**。字典嵌套也是常见的用法。
>
> ```yaml
> person:
>   name: Alice
>   age: 30
>   address:
>     street: 123 Main St
>     city: Springfield
>     zip: 62701
> ```
>
> #### 6. **引用**
>
> YAML支持引用和锚点。通过`&`创建一个锚点，再通过`*`引用它。
>
> ```yaml
> defaults: &defaults
>   name: Alice
>   age: 30
> 
> user1:
>   <<: *defaults
>   city: New York
> 
> user2:
>   <<: *defaults
>   city: Chicago
> ```
>
> 在上面的例子中，`user1`和`user2`引用了`defaults`中的数据，但可以覆盖或添加新的键值对。
>
> #### 7. **注释**
>
> YAML中的注释使用 `#` 符号。注释从 `#` 开始，直到行末为止：
>
> ```yaml
> # This is a comment
> name: Alice  # This is also a comment
> ```

> ### YAML与JSON的对比
>
> YAML与JSON非常相似，都可以用来表示结构化数据。YAML相较于JSON的主要优点是：
>
> - **更简洁**：YAML去除了JSON中的花括号、引号、逗号等符号，使得格式更加简洁。
> - **可读性更强**：YAML更接近自然语言，便于手动编辑和阅读。
> - **不需要引号**：YAML中，字符串值通常不需要用引号包裹，除非字符串本身包含特殊字符。
>
> 然而，JSON更严格，通常用于数据传输和API中，而YAML更适合配置文件、日志等场景。





一个简单的K8s Deployment资源清单

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-simple-pod
spec:
  containers:
  - name: my-container
    image: nginx:latest
    ports:
    - containerPort: 80
```

在master主机中可以进行部署

```shell
kubectl apply -f my-simple-pod.yaml
```



**Pod生命周期**中在 Main Container启动之前 有时会预先启动一些 init Container去检验运行环境以及配置一些文件

> 在 Kubernetes 中，**探针（Probe）** 是一种用于监控和管理容器健康状态的机制。探针通过周期性地检查容器的健康状况（如是否正在正常运行），帮助 Kubernetes 确定容器是否健康、是否就绪，以及是否需要重启。
>
> Kubernetes 中有三种类型的探针，分别是：
>
> 1. **Liveness Probe（存活探针）**
> 2. **Readiness Probe（就绪探针）**
> 3. **Startup Probe（启动探针）**

由kubelet去调用执行Probe

 Kubernetes Pod 示例，其中包括了 **Init Containers**、主容器、**探针（Probes）** 以及启动和退出的逻辑：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app-pod
spec:
  initContainers:
    - name: init-db
      image: busybox
      command: ['sh', '-c', 'echo "Initializing database..."; sleep 10']
      volumeMounts:
        - name: app-data
          mountPath: /data

  containers:
    - name: web-app
      image: my-web-app:latest
      ports:
        - containerPort: 80
      volumeMounts:
        - name: app-data
          mountPath: /app/data
      livenessProbe:
        httpGet:
          path: /healthz
          port: 80
        initialDelaySeconds: 10
        periodSeconds: 5
        failureThreshold: 3
        timeoutSeconds: 2
      readinessProbe:
        httpGet:
          path: /readiness
          port: 80
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 3
        timeoutSeconds: 2
      startupProbe:
        httpGet:
          path: /start
          port: 80
        initialDelaySeconds: 15
        periodSeconds: 5
        failureThreshold: 10

  volumes:
    - name: app-data
      emptyDir: {}
  
  restartPolicy: Always
```

> 通常情况下，你可能会用 `kind: Pod` 来定义单个容器的工作负载，例如：
>
> - 用于快速部署或测试容器化应用。
> - 用于运行不需要扩缩容的单一应用。
>
> 然而，Kubernetes 中更多的场景是使用 **控制器**（如 **Deployment**、**StatefulSet**、**DaemonSet** 等）来管理 Pod。控制器提供了更多的功能，比如自动扩展、滚动更新等。因此，虽然你可以直接使用 `Pod` 来运行应用，但在生产环境中，通常推荐使用 Deployment 或其他控制器来管理 Pod。

=>即不常用 kind ：Pod， 而是类似 kind:Deployment

### Pod 控制器 20241107

> **Pod 控制器**（Pod Controller）是 Kubernetes 中一种用于管理和控制 **Pod** 生命周期的控制器。
>
> ### 常见的 Pod 控制器类型
>
> 1. **ReplicaSet**
>    - **功能**：确保指定数量的 Pod 副本始终在集群中运行。如果某个 Pod 出现故障或被删除，ReplicaSet 会自动创建新的 Pod 以保持副本数。
>    - **应用场景**：用于保证某一应用的 Pod 数量和健康状态一致。
> 2. **Deployment**
>    - **功能**：Deployment 是 ReplicaSet 的高级抽象，它不仅管理 Pod 副本，还提供了滚动更新、回滚等功能。它在自动化部署、版本控制和系统更新中非常有用。
>    - **应用场景**：用于管理无状态的应用，提供版本控制和可扩展性。
> 3. **StatefulSet**
>    - **功能**：用于管理有状态的应用。它确保每个 Pod 都有一个稳定的标识符，并且在更新时保证 Pod 的顺序性和稳定性。StatefulSet 还确保应用中的每个 Pod 在重新调度时会保留持久化存储。
>    - **应用场景**：适用于有状态服务，比如数据库、缓存、队列等。
> 4. **DaemonSet**
>    - **功能**：DaemonSet 确保在每个节点上都运行一个 Pod 副本。如果添加了新节点，DaemonSet 会自动在新节点上启动一个 Pod 副本。
>    - **应用场景**：用于节点级别的服务，如日志收集、监控代理、网络插件等。
> 5. **Job**
>    - **功能**：Job 控制器负责一次性任务（如批处理任务）的管理。它确保任务完成指定次数，且所有 Pod 必须成功运行完任务。
>    - **应用场景**：用于处理定时任务、批处理、数据迁移等任务。
> 6. **CronJob**
>    - **功能**：CronJob 是 Job 控制器的扩展，允许用户按照指定的时间计划周期性地运行作业。它类似于 Linux 中的 cron 作业。
>    - **应用场景**：用于定期执行任务，如定时备份、定时数据同步等。

命令式编程 与 声明式编程

> **声明式编程**（Declarative Programming）是一种编程范式，其中程序员描述期望的“**做什么**”而不是“**怎么做**”。与命令式编程（Imperative Programming）不同，声明式编程专注于表达计算的结果或目标，而不是描述如何一步一步实现这个目标的具体过程。
>
> **配置语言（如 JSON、YAML、Terraform 等）**：
>
> - 配置文件通常用于声明性编程，描述希望系统达到的目标状态，而不涉及如何一步步实现。例如，使用 `YAML` 或 `JSON` 定义 Kubernetes 的 Pod、服务或虚拟机的配置，Kubernetes 系统会负责根据这些配置管理资源。

**大多数情况下**，**一个容器只有一个主进程**，并且该容器的生命周期和该主进程是紧密相关的。Docker 强调 **一个进程一个容器** 的理念，推荐将应用拆分成多个容器，每个容器运行一个单独的进程或服务。

=>所以所谓的Pod控制器，比如DaemonSet，可以跟单个的守护进程联系起来，即 Pod控制器=>进程属性

=>部署完Pod之后，控制器设置为Deployment, 还可以在master中进行扩容，回滚，更新镜像等命令行操作

Job 控制器示例：

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  completions: 1          # 任务成功执行的次数，默认值是 1
  parallelism: 1          # 同时运行的 Pod 数量，默认值是 1
  backoffLimit: 4         # 最大重试次数，失败后重试的最大次数
  activeDeadlineSeconds: 3600  # Job 运行的最大时间（秒），超过此时间任务将被终止
  template:
    spec:
      containers:
      - name: example-container
        image: busybox:1.35
        command: ["sh", "-c", "echo Hello World; sleep 30"]  # 执行的任务
      restartPolicy: Never  # 表示 Job 中的 Pod 失败后不会重启
```

=>上面就是启动一个Pod运行 Hello World，在master里再打一个 kubectl log xxx就能从Pod那边得到结果

### Service 20241108

> 在 **Kubernetes** 中，`Service` 是一种抽象的资源，它定义了一种访问和暴露一组 **Pod** 的方法。Kubernetes 的 `Service` 使得客户端可以访问后端的 Pod，而无需关心 Pod 的动态变化和 IP 地址的改变。
>
> ### `Service` 的作用
>
> - **负载均衡**：一个 `Service` 可以通过负载均衡的方式将请求分发到一组 Pod 上。即使 Pod 的 IP 地址变化，`Service` 也会保持对这些 Pod 的访问能力。
> - **抽象与稳定性**：`Service` 提供了一个稳定的访问入口。Pod 可能会被替换、重新调度或销毁，但 `Service` 的访问地址（即 DNS 名称或 IP）是固定的，不会改变。
> - **简化服务发现**：通过 `Service`，可以很容易地进行服务发现和访问 Kubernetes 集群内部或外部的服务。
>
> ### `Service` 的类型
>
> 在 Kubernetes 中，`Service` 有不同的类型，每种类型定义了不同的暴露方式。常见的 `Service` 类型包括：
>
> 1. **ClusterIP**（默认类型）
>    `ClusterIP` 类型的 `Service` 只在 Kubernetes 集群内部可访问。它为 Pod 提供了一个集群内部的 IP 地址，并通过该 IP 地址进行负载均衡。
> 2. **NodePort**
>    `NodePort` 类型的 `Service` 会将服务暴露到每个 Node 上的一个端口上。通过访问 `NodeIP:NodePort`，可以从外部访问该服务。
> 3. **LoadBalancer**
>    `LoadBalancer` 类型的 `Service` 会为服务创建一个外部负载均衡器（通常是云提供商的负载均衡服务，如 AWS ELB 或 GCP 的负载均衡器）。它允许外部流量通过负载均衡器访问 Kubernetes 集群中的服务。
> 4. **ExternalName**
>    `ExternalName` 类型的 `Service` 不会直接将请求路由到 Kubernetes 集群内部的 Pod。相反，它会将请求重定向到外部 DNS 名称。适用于将 Kubernetes 内部服务与外部服务进行集成的场景。

=>Service不是一个具体的服务器，它是通过apiserver 与 Kube-proxy 与Client所访问的iptables的 交互 而组成的 一套机制。 Service是 OSI第四层的 服务，有Kube-proxy 提供虚拟IP，据说用Ingress可以提供第7层的HTTP服务

> 举例实现 **ClusterIP** 的过程中，`apiserver`、`kube-proxy` 和 `iptables`（或 `IPVS`）起到了关键作用
>
> **API Server (`apiserver`)**：Kubernetes API Server 是集群的核心组件之一，负责接收并处理所有集群的 REST 请求，并与其它 Kubernetes 组件进行通信。
>
> **kube-proxy**：每个 Kubernetes 节点上都会运行 `kube-proxy`，它负责在节点上设置网络路由规则，确保来自客户端（例如，Pod、Service）请求的流量能够正确地转发到目标 Pod。
>
> **iptables**：`iptables` 是 Linux 系统中用于管理网络流量的工具，Kubernetes 使用它来创建规则，实现流量转发和负载均衡。

下面是一个简单的 `NodePort` 服务的 YAML 示例：

```
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80        # 集群内的服务端口
      targetPort: 8080 # 后端 Pod 上的端口（容器实际监听的端口）
      nodePort: 30007  # 暴露给外部客户端访问的端口
  type: NodePort
```

> Ingress 主要是处理 HTTP 和 HTTPS 流量的路由和负载均衡。它通过定义一些路由规则，将外部请求转发到集群内部的服务。Ingress 充当了一个**反向代理**，通过它你可以设置 URL 路由、SSL 终止、重定向等功能。
>
> Ingress 是由一个控制器（**Ingress Controller**）来实现的。比如**NGINX Ingress Controller**(基于 NGINX 的反向代理和负载均衡器实现)
>
> **Ingress Controller** 是一个运行在 Kubernetes 集群中的 **进程**，而不是一个独立的服务器。它负责实现和管理 **Ingress 资源** 中定义的路由规则，并将外部 HTTP/HTTPS 流量路由到集群内部的服务。

=>同样有配置Ingress 的YAML

### 存储 20241111

> 在 Kubernetes 中，**容器本身没有持久化存储**。容器是临时的、可替换的环境，一旦容器停止或被删除，容器中的所有数据都会丢失。因此，容器中的数据一般是**短暂的**，通常我们需要依赖外部存储来持久化数据。

configMap：配置文件

> `ConfigMap` 是 Kubernetes 用于存储配置信息的资源对象，它允许你存储一些不包含敏感数据的配置文件，如 JSON、YAML 或普通的文本配置文件。
>
> ```shell
> #命令行创建
> kubectl create configmap my-config --from-file=path/to/your/config/file
> ```
>
> 如果你的应用程序的配置本身就是 YAML 或 JSON 格式，你可以直接将这些格式的内容存储在 `ConfigMap` 中:
>
> ```yaml
> apiVersion: v1
> kind: ConfigMap
> metadata:
>   name: config-with-files
> data:
>   config.json: |
>     {
>       "database": "postgresql",
>       "host": "localhost",
>       "port": 5432
>     }
>   app.properties: |
>     server.port=8080
>     database.name=mydb
> ```
>
> 接着将 ConfigMap 挂载到 Pod上：
>
> ```yaml
> apiVersion: v1
> kind: Pod
> metadata:
>   name: example-pod
> spec:
>   containers:
>   - name: example-container
>     image: nginx
>     volumeMounts:
>     - name: config-volume
>       mountPath: /etc/config
>       readOnly: true
>   volumes:
>   - name: config-volume
>     configMap:
>       name: my-config
> ```

> `ConfigMap` 数据并不存储在某个单独的物理服务器上，而是存在于 Kubernetes **控制平面（Control Plane）** 的 **etcd** 数据库中。



Secret：`Secret` 和 `ConfigMap` 都是用于存储配置信息的资源对象，它们的主要区别在于存储的数据类型和用途。Secret存储敏感数据，如密码、API 密钥、令牌等。数据在 etcd 中存储时通常是经过 base64 编码的，且可以与外部加密工具（如 Vault）配合使用，以增强安全性。



Volume: 用于持久化数据和容器之间共享数据的机制。容器本身是短暂的，重启或重新调度时，容器内的数据可能会丢失。因此，Kubernetes 引入了 `Volume` 机制，以确保数据在容器生命周期之外依然存在，且可以在多个容器之间共享。

=>种类很多啊，有PersistentVolume (PV) 和 PersistentVolumeClaim (PVC)，有NFS (Network File System)，还有AWS EBS (Elastic Block Store)，Azure Disk...

> Kubernetes 中的 `Volume` 是容器持久化数据和共享数据的重要机制。它为容器化应用提供了多种存储选项，包括本地存储、云存储和分布式存储等。通过不同类型的 `Volume`，用户可以根据需求选择适合的存储方案。
>
> 常见的使用场景包括：
>
> - 临时存储：使用 `emptyDir`。
> - 持久化存储：使用 `PersistentVolume` 和 `PersistentVolumeClaim`。
> - 跨节点共享数据：使用 `NFS` 或 `GlusterFS`。
> - 云存储：使用 `AWS EBS`、`Azure Disk` 等。



PersistentVolume（PV）: PV 是集群中的存储资源，它可以是一个云存储卷（如 AWS EBS、Azure Disk）或网络存储卷（如 NFS、GlusterFS）。PV 是一个持久化的资源，生命周期独立于 Pod。它不会随着 Pod 的删除而删除，除非设置了回收策略。

PersistentVolumeClaim（PVC）:`PersistentVolumeClaim` 是对存储资源的请求，它由用户或应用程序创建。PVC 请求存储资源的大小、访问模式等属性，Kubernetes 会根据 PVC 的需求找到一个合适的 PV。PVC 的生命周期与 Pod 绑定，PVC 可以在 Pod 的声明中指定，Kubernetes 会自动将一个合适的 PV 绑定到 PVC 上。

> ### **PV 和 PVC 的工作原理**
>
> 1. **创建 PV**： 管理员创建 `PersistentVolume`，定义具体的存储后端（例如，本地存储、网络存储、云存储），并设置其大小、访问模式等属性。
> 2. **创建 PVC**： 用户创建 `PersistentVolumeClaim`，并声明所需的存储大小和访问模式（例如 `ReadWriteOnce` 或 `ReadWriteMany`）。PVC 会根据这些需求请求合适的 PV。
> 3. **PV 和 PVC 绑定**：
>    - Kubernetes 会根据 PVC 的请求找到一个符合条件的 PV，并将其与 PVC 绑定。
>    - 一旦 PV 和 PVC 被绑定，PVC 的生命周期就与绑定的 PV 绑定，PVC 的生命周期将在 Pod 被删除时结束。
> 4. **访问存储**：
>    - 绑定后，用户可以在 Pod 中通过 `PVC` 来访问 PV。Kubernetes 会自动把 PVC 挂载到容器的文件系统中，容器可以通过 `PVC` 使用对应的存储。

=>设计出这两个概念就是为了Pod与具体的存储 解耦啊，PV与PVC也是用YAML格式来定义，并且联系的，两者一对一映射, PVC 和 PV 根据一些原则自动绑定.

> ### **常见的 PV 和 PVC 使用场景**
>
> 1. **Cloud Storage**：例如使用 `AWS EBS` 或 `Azure Disk`，为每个 Pod 提供独立的存储。
> 2. **Network Storage**：例如使用 `NFS` 或 `CephFS`，为多个 Pod 提供共享的存储。
> 3. **Local Storage**：使用节点本地存储，例如 `hostPath` 或 `local` 卷，用于需要存储在特定节点上的数据。
> 4. **Stateful Applications**：像数据库（MySQL、PostgreSQL）这类有状态应用，通常需要持久化存储，可以使用 PVC 绑定到 PV。

> ### **完整示例**
>
> 假设我们已经有了 EBS 卷，以下是创建 EBS 存储卷并将其挂载到 Pod 的完整流程。
>
> 1.**创建 PersistentVolume（PV）**：
>
> ```yaml
> apiVersion: v1
> kind: PersistentVolume
> metadata:
>   name: ebs-pv
> spec:
>   capacity:
>     storage: 10Gi   # 需要与 EBS 卷大小匹配
>   accessModes:
>     - ReadWriteOnce   # EBS 卷通常仅能在单一节点上以 ReadWriteOnce 模式挂载
>   persistentVolumeReclaimPolicy: Retain   # PV 保留策略，当 PVC 删除时保留该 PV
>   awsElasticBlockStore:
>     volumeID: vol-xxxxxxxx   # 替换为实际的 EBS 卷 ID
>     fsType: ext4   # 文件系统类型，通常为 ext4 或 xfs
> ```
>
> 2.**创建 PersistentVolumeClaim（PVC）**：
>
> ```yaml
> apiVersion: v1
> kind: PersistentVolumeClaim
> metadata:
>   name: ebs-pvc
> spec:
>   accessModes:
>     - ReadWriteOnce  # 要求单一节点可以读写
>   resources:
>     requests:
>       storage: 10Gi  # 需要与 PV 的大小一致
> ```
>
> 3.创建 Pod，并挂载 PVC:
>
> ```yaml
> apiVersion: v1
> kind: Pod
> metadata:
>   name: ebs-pod
> spec:
>   containers:
>   - name: nginx-container
>     image: nginx
>     volumeMounts:
>     - name: ebs-storage
>       mountPath: /data  # 将 EBS 卷挂载到容器内的 /data 路径
>   volumes:
>   - name: ebs-storage
>     persistentVolumeClaim:
>       claimName: ebs-pvc  # PVC 的名称
> ```

### 集群调度 20241112

> 在 Kubernetes 中，集群调度（**Kubernetes Scheduling**）是指如何将一个 Pod 安排到集群中的节点上运行的过程。调度过程不仅要考虑资源的可用性，还要考虑节点的健康状态、拓扑结构、策略要求、调度约束等因素。Kubernetes 的调度器（**kube-scheduler**）负责根据一系列规则（如资源需求、节点亲和性、污点和容忍度等）决定将 Pod 部署到哪个节点上。
>
> ### **Kubernetes 调度过程概述**
>
> 1. **Pod 提交到 API 服务器**： 当用户创建 Pod 时，Pod 对象会被提交到 Kubernetes API 服务器。此时，Pod 并未立即运行，而是处于 `Pending` 状态。
> 2. **调度器调度 Pod**： Kubernetes 的调度器（`kube-scheduler`）会监控 API 服务器的 Pod 调度队列，查看哪些 Pod 处于 `Pending` 状态，且尚未绑定到节点。调度器将根据调度策略选择一个适合的节点来运行 Pod。
> 3. **节点选择**： 调度器会根据 Pod 的资源需求、节点的可用资源、节点亲和性、Pod 的亲和性、污点与容忍度等因素来选择合适的节点。
> 4. **绑定 Pod 和节点**： 一旦调度器选择了一个节点，它会将 Pod 绑定到该节点。绑定过程会更新 Pod 对象，使其与特定节点关联，Pod 将被分配到该节点并开始运行。
>
> ### **调度过程详细步骤**
>
> #### 1. **预选阶段（Predicates）**
>
> 在这个阶段，调度器会检查哪些节点符合 Pod 的基本需求。每个节点都会被检查以确定它是否能满足 Pod 的资源需求和其他约束条件。
>
> - **资源需求**：检查节点上是否有足够的 CPU、内存、存储等资源来满足 Pod 的要求。
> - **节点亲和性**（Node Affinity）：如果 Pod 有指定的节点亲和性（例如，必须运行在某些标签的节点上），则调度器会筛选符合条件的节点。
> - **污点和容忍度**（Taints and Tolerations）：如果节点上有污点，Pod 需要有相应的容忍度才能被调度到该节点。否则，该节点会被排除。
> - **Pod 亲和性**（Pod Affinity/Anti-Affinity）：指定 Pod 应该或不应该与哪些其他 Pod 一起运行。调度器会检查当前集群中是否有符合条件的其他 Pod。
> - **节点状态**：调度器还会检查节点的状态，例如是否是 `Ready` 状态，节点是否被标记为不可调度（`NoSchedule`）。
>
> #### 2. **优选阶段（Prioritization）**
>
> 在这个阶段，调度器会对符合条件的节点进行排序。它根据预定义的优先级规则为每个节点打分，分数较高的节点将被优先选中。
>
> 一些常见的优先级因素包括：
>
> - **资源需求优先级**：如节点上的资源使用情况和 Pod 的资源请求是否匹配。
> - **亲和性优先级**：例如，Pod 是否应该运行在某个特定区域的节点上，或者与其他 Pod 的位置要求。
> - **负载均衡**：调度器会考虑当前节点上的负载，避免某些节点过载。
> - **调度延迟**：某些情况可能希望 Pod 尽早调度到节点，调度器会尽量减少调度延迟。
>
> #### 3. **选择节点并绑定 Pod**
>
> 经过预选和优选后，调度器会选择一个最适合的节点，并将 Pod 绑定到该节点。此时，Pod 状态变为 `Running`，并开始在节点上启动容器。
>
> ### **调度器的核心功能**
>
> Kubernetes 调度器（`kube-scheduler`）执行以下主要任务：
>
> - **节点选择**：选择合适的节点以确保 Pod 在资源、拓扑、策略等方面的需求得到满足。
> - **Pod 绑定**：一旦选定节点，调度器将 Pod 绑定到该节点，并通过 API 服务器更新 Pod 对象。
> - **调度策略和插件**：调度器可以通过插件化的方式来扩展调度策略，允许集群管理员根据业务需求定制调度行为。
> - **资源管理**：调度器会确保 Pod 的资源请求（CPU、内存、存储等）符合节点的资源情况，避免资源争抢。
>
> ### **调度策略**
>
> #### 1. **节点亲和性（Node Affinity）**
>
> Kubernetes 提供了节点亲和性（Node Affinity），让用户能够指定 Pod 应该在哪些节点上运行。Node Affinity 是一个更灵活的方式，类似于标签选择器，但支持更复杂的逻辑。
>
> - **requiredDuringSchedulingIgnoredDuringExecution**：表示 Pod 必须被调度到满足条件的节点上。=>硬策略
> - **preferredDuringSchedulingIgnoredDuringExecution**：表示 Pod 会尽量被调度到满足条件的节点上，但不是强制要求。=>软策略
>
> 例如，要求 Pod 只在 `zone=us-west1` 的节点上运行：
>
> ```yaml
> affinity:
>   nodeAffinity:
>     requiredDuringSchedulingIgnoredDuringExecution:
>       nodeSelectorTerms:
>         - matchExpressions:
>             - key: "zone"
>               operator: In
>               values:
>                 - "us-west1"
> ```
>
> #### 2. **Pod 亲和性（Pod Affinity）与反亲和性（Anti-Affinity）**
>
> Pod 亲和性和反亲和性允许你指定 Pod 在选择节点时与其他 Pod 的位置关系。Pod 亲和性要求 Pod 在某些特定条件下被调度到离特定 Pod 很近的节点上，而反亲和性则要求 Pod 不要与某些 Pod 放置在同一个节点或相近的节点。
>
> 例如，要求某个 Pod 在同一个节点上与另一个特定标签的 Pod 一起调度：
>
> ```yaml
> affinity:
>   podAffinity:
>     requiredDuringSchedulingIgnoredDuringExecution:
>       - labelSelector:
>           matchExpressions:
>             - key: "app"
>               operator: In
>               values:
>                 - "frontend"
>         topologyKey: "kubernetes.io/hostname"
> ```
>
> =>Node Affinity 是跟某个具体节点（即服务器）关联，Pod Affinity则是跟另一个具体的Pod相关联
>
> #### 3. **污点和容忍度（Taints and Tolerations）**
>
> 污点（Taints）是节点的一种属性，用于标记节点上的限制，使得只有具有容忍度（Tolerations）标签的 Pod 可以被调度到这些节点上。这个特性有助于将某些 Pod 调度到专门的节点（例如，临时节点、特定硬件的节点等）。
>
> - **Taint**：污点是节点的一个属性，通常会被添加到节点上，表示该节点只接受具有特定容忍度的 Pod。
> - **Toleration**：容忍度是 Pod 的一个属性，用于声明该 Pod 可以被调度到带有特定污点的节点上。
>
> 例如，某个节点有污点 `key=value:NoSchedule`，只有带有 `key=value` 容忍度的 Pod 可以调度到该节点上：
>
> ```yaml
> tolerations:
> - key: "key"
>   operator: "Equal"
>   value: "value"
>   effect: "NoSchedule"
> ```
>
> =>像master节点天生就打了 NoSchedule 污点，所以分配Pod全去node节点还不回来master节点
>
> =>除以上之外，还可以为Pod直接指定Node...即固定节点调度
>
> ### **总结**
>
> Kubernetes 集群调度的核心任务是将 Pod 调度到合适的节点上，以确保资源的高效利用、应用的高可用性和服务的稳定性。调度器通过以下几个主要阶段完成任务：
>
> - **预选阶段**：检查节点是否满足 Pod 的基本需求。
> - **优选阶段**：对满足条件的节点进行打分，并选择最佳节点。
> - **Pod 绑定**：将 Pod 绑定到选择的节点，并启动 Pod。
>
> 通过使用 **节点亲和性**、**Pod 亲和性**、**污点和容忍度**等高级调度策略，Kubernetes 使得调度过程更加灵活，能够根据不同的业务需求进行精细的资源分配。

### 安全 20241113

Kubernetes API Server是所有请求的入口，K8S的安全机制也是围绕此来展开的。

身份验证（Authentication)：

> Kubernetes的API服务器（API Server）是整个集群的核心组件，它负责处理外部和内部的所有请求。为了确保客户端与API服务器之间的通信是安全的，Kubernetes默认要求通过**HTTPS**（即HTTP over TLS）来访问API。所有API请求，包括身份验证请求，都会使用HTTPS来进行加密。

K8s内部组件访问API Server也有两种可能，一种是本机访问，第二种也是需要HTTPS双向认证的

授权（Authorization）：

Authentication只是确定通信双方是可信的，至于请求方有哪些资源的权限则取决于Authorization 鉴权。

> Kubernetes的授权机制用于控制用户或服务帐户对资源的访问权限，确保只有被授权的实体能够执行特定的操作。
>
> - **RBAC（Role-Based Access Control）**：RBAC是Kubernetes的主要授权机制，它根据角色（Role）和角色绑定（RoleBinding）来定义用户对资源的访问权限。通过RBAC，可以细粒度地控制用户、服务帐户、甚至外部系统对K8s资源的访问。
> - **ABAC（Attribute-Based Access Control）**：基于属性的访问控制，允许通过用户属性（如部门、环境）来授权访问。
> - **Webhook授权**：可以通过Webhooks将K8s的授权机制扩展到外部系统，进行自定义授权。

Role-Based Access Control是最为流行的，也是K8s的默认设置

> 在 Kubernetes 中，`Role` 和 `ClusterRole` 都是 **RBAC (Role-Based Access Control)** 的核心概念，它们用于定义在集群中对不同资源的访问权限。二者的主要区别在于作用域和应用范围。
>
> `ClusterRole` 的权限是**集群级别**的。它可以跨所有命名空间，或者指定集群范围的资源（如 Node、PersistentVolume、Namespace 等）。

=>都是通过YAML文件来定义的

=>Role-Based Access Control与AWS的IAM机制差不多

### Helm 20241115

> **Helm** 是 Kubernetes（K8s）中的一个包管理工具，它简化了应用的部署和管理过程。类似于 Linux 中的包管理器（如 apt 或 yum），Helm 让你能够轻松地安装、升级、删除和管理 Kubernetes 应用。
>
> ### Helm 的核心功能：
>
> 1. **简化部署：** Helm 通过将 Kubernetes 资源（如 Pods、Services、Deployments 等）打包成一个称为 **Chart** 的包，简化了应用的部署过程。你只需要一个命令就可以部署一个复杂的应用，而不需要手动管理大量的 YAML 文件。
> 2. **应用版本管理：** Helm 使得应用的版本控制变得容易。你可以通过 Helm 安装应用，并且能够轻松地升级到新版本，或者回滚到旧版本。
> 3. **模板化配置：** Helm 使用 Go 模板来生成 Kubernetes 配置文件，这样就可以根据不同的环境需求调整配置。通过这种方式，同一个 Helm Chart 可以在多个环境中重复使用，只需要修改配置文件中的一些变量。
> 4. **依赖管理：** Helm 支持在安装一个应用时自动安装它的依赖。例如，如果一个应用需要数据库或缓存服务，Helm 可以自动安装并配置这些依赖服务。
> 5. **方便的命令行工具：** Helm 提供了一套命令行工具来帮助用户管理 Kubernetes 应用，比如 `helm install`、`helm upgrade`、`helm uninstall` 等。
>
> ### Helm 的核心概念：
>
> - **Chart：** Helm 中的核心概念是 **Chart**，它是一个包含 Kubernetes 资源定义的包。每个 Chart 可以包含多个 YAML 文件，描述了一个应用的不同部分（如 Deployment、Service、Ingress 等）。
> - **Release：** 每次使用 Helm 部署一个 Chart 时，会创建一个 **Release**。每个 Release 都有一个名称，表示这个 Chart 在 Kubernetes 集群中的一个具体实例。
> - **Repository：** Helm Chart 可以存放在一个 Helm 仓库中，用户可以从仓库中下载并安装公开或私有的 Charts。
>
> ### Helm 的工作流程：
>
> 1. **安装 Helm：**
>    首先需要安装 Helm 客户端，可以在本地执行 Helm 命令。
>
> 2. **添加 Helm 仓库：**
>    可以添加官方或私有 Helm 仓库，来获取公开的 Chart。例如：
>
>    ```bash
>    helm repo add stable https://charts.helm.sh/stable
>    ```
>
> 3. **安装应用：**
>    使用 Helm 安装应用。例如，安装 Nginx Ingress Controller：
>
>    ```bash
>    helm install my-nginx stable/nginx-ingress
>    ```
>
> 4. **管理和升级应用：**
>    可以使用 Helm 来查看、升级或回滚已安装的应用。例如：
>
>    ```bash
>    helm list           # 查看已安装的 Helm Releases
>    helm upgrade        # 升级应用
>    helm rollback       # 回滚到上一个版本
>    ```
>
> 5. **删除应用：**
>    使用 Helm 删除已部署的应用：
>
>    ```
>    helm uninstall my-nginx
>    ```
>
> ### 举个例子：
>
> 假设你想安装一个 Nginx 应用，使用 Helm 可以这样操作：
>
> ```
> helm install my-nginx stable/nginx-ingress
> ```
>
> 这条命令会将 Nginx Ingress Controller 安装到 Kubernetes 集群中，并自动为它创建相关资源（如 Deployment、Service 等）。
>
> ### 总结：
>
> Helm 大大简化了 Kubernetes 应用的部署和管理，尤其是在需要部署多个复杂应用时，Helm 的模板化和版本控制功能使得配置更加灵活和可维护。对于开发者和运维人员来说，Helm 是 Kubernetes 环境中非常有用的工具。

一个简单的Chart 的结构如下：

```
myapp/
├── Chart.yaml          # Chart 的元数据文件
├── values.yaml         # 默认的配置文件
├── charts/             # 可选的依赖包
├── templates/          # 存放 Kubernetes 资源模板文件
│   ├── deployment.yaml # Deployment 模板
│   ├── service.yaml    # Service 模板
│   └── ingress.yaml    # Ingress 模板
└── README.md           # Chart 的文档说明
```

> **Go 模板语法**在 Helm 中的作用是将静态的 YAML 文件模板化，使得你可以根据不同的配置、条件和环境动态生成 Kubernetes 资源文件。

=>可以配置一个Kubernetes的网页Dashboard，就像AWS的界面一样

> Prometheus 是一个强大的开源监控工具，专门为分布式系统、容器化应用以及微服务架构设计。它被广泛应用于 Kubernetes 集群、云原生架构、容器监控等领域，是现代 DevOps 和云原生环境中不可或缺的监控工具之一。

=>Prometheus 最终也是个 网页GUI来监视 容器的各种数据，CPU占有率啥的，好像需要跟**Grafana** 结合

> **EFK** 是一个广泛使用的日志收集和分析栈，通常用于 Kubernetes 集群的日志管理。EFK 由三个主要组件组成：
>
> - **Elasticsearch**：用于存储和查询日志数据。
> - **Fluentd**：日志收集器，负责从各个容器和节点中收集日志，通常用于将日志从 Kubernetes 集群推送到 Elasticsearch。
> - **Kibana**：一个可视化界面，用于显示 Elasticsearch 中存储的日志数据，提供查询和分析的功能。

=>也可以说领用Helm可以下载许多组件给Kubernete添加上dashboard，监视工具以及日志工具

### 高可用k8s构建 20241118

无非就是 loader Balancer增加副本容错 

构建各节点的时候如同Ansible项目中所说的对linux 有各种 防火墙 内核参数修改 等细微操作

然后就是建立多个master节点，依次安装各个模块以及Docker，可以预想与Ansible结合起来用，对linux的操作命令行要很熟

## Ericsson AKS Demo Session 20250122

**Agenda**

- Introduction to Azure Kubernetes Service
- Managed Istio
- Open Service Mesh
- Managed Nginx Ingress in AKS
- GitOps
- Prometheus
- Service Connector (Preview)
- KEDA Addon
- Custom Resource Definition
- GitHub Copilot for AKS
- Application Gateway for Containers
- Azure Load Testing



Load Balancer Service Ingress Controllers - Layer 7 Load Balancing

Istio Gateway

Service Mesh -  Istio add-on for AKS (AKS提供的 Istio 插件，用于管理微服务之间的通信，让服务间流量管理、安全认证等更容易控制和观察) =>与Azure Managed Grafana, Azure Managed Prometheus一起工作

NGINX Ingress Controller - App Routing Addon（应用路由插件）

=>AKS有更多可以集成的插件给Ingress Controller，即插即用，给developer更好的使用体验，不用担心版本冲突啥的

Application Gateway for Containers  => AKS除了kubernetes一般具有的流量均衡负载，还可以提供集成的firewall

> **GitOps**（Git Operations）是一种**基础设施自动化和应用交付**的方法，它使用 **Git 作为单一事实来源（Single Source of Truth）**，并结合 **CI/CD 流程** 来自动化部署和管理 Kubernetes 等云原生环境。
>
> 简单来说，GitOps **把 Git 当作配置管理工具**，让开发团队可以像管理代码一样管理基础设施和应用部署。

 =>VScode中下载GitHub Copilot for Azure

> GitHub Copilot 是**AI 代码助手**，可以帮助开发者**更快地编写 Kubernetes 相关的 YAML、Helm Charts 以及 CI/CD 脚本**。

# Docker

## 尚硅谷Docker实战教程

[Link](https://www.bilibili.com/video/BV1gr4y1U7CY/?spm_id_from=333.337.search-card.all.click&vd_source=c13700902d2c98df282b6f1f2889c0cb)

### 教程简介 20241127

1. Docker简介
2. Docker安装
3. Docker常用命令
4. Docker镜像
5. 本地镜像发布到阿里云
6. 本地镜像发布到私有库
7. Docker容器数据卷
8. Docker常规安装简介

进阶：

1. Docker复杂安装详说
2. DockerFile解析
3. Docker微服务实战
4. Docker网络
5. Docker-compose容器编排
6. Docker轻量级可视化工具Portainer
7. Docker容器监控之CADvisor+InfulxDB+Granfana



### Docker简介 20241211

系统环境平滑移植

从系统环境开始，自底至上打包应用。像虚拟机一样一个iso文件搞定一切。

=>先安装Docker，然后用这个工具运行iso文件就行

传统虚拟机 启动太慢，占用资源又多，从而优化发展出来 容器虚拟化技术

官网 + 镜像仓库

Dockers必须部署在Linux内核的系统上。如果Windows上部署Docker的方法都是先安装一个虚拟机

> Docker Desktop for Windows利用 WSL2 提供一个 Linux 内核环境，Docker 的容器和镜像实际运行在这个 WSL2 环境中。

Docker三要素：镜像image，容器container，仓库repository =>容器可以看成是镜像的实例化

> **WSL**：WSL通过操作系统级别的虚拟化来运行Linux环境。特别是WSL 1，它通过在Windows上模拟Linux内核的系统调用来运行Linux应用程序，而WSL 2引入了一个轻量级的虚拟机来运行一个完整的Linux内核，但它并不是模拟整个计算机硬件的虚拟机，而是一个虚拟化的Linux内核环境。
>
> **虚拟机**：虚拟机是硬件级别的虚拟化，每个虚拟机运行一个完整的操作系统（包括独立的内核），并且模拟了计算机硬件的所有部分，如CPU、内存、硬盘和网络等。
>
> **WSL更接近于操作系统级虚拟化，而虚拟机则是硬件级虚拟化**，二者的虚拟化方式不同。
>
> **WSL**（Windows Subsystem for Linux）更接近于**容器虚拟化技术**，尤其是WSL 2，它与容器化技术在一些方面有相似之处。

Docker就是专门用于CICD部署的，而WSL更偏向一个虚拟化开发环境

可以把容器看做一个简易版的Linux环境(包括root用户极限，进程空间，用户空间和网络空间等)和运行在其中的应用程序。

注意 Docker daemon的概念所在的位置，它在container之外

> Docker 的架构遵循了典型的客户端-服务器C/S模式，主要由 **Docker 客户端** 和 **Docker 守护进程（Daemon）** 两部分组成。

Docker运行run命令，现在本地寻找相关镜像，没有就去Hub上寻找并下载



Docker为什么比虚拟机快：

1. docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化，运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。
2. docker利用的是宿主机的内核，而不需要加载操作系统OS内核。新建虚拟机是几分钟级别的，而新建docker就是几秒钟

虚拟机：OS => Hypervisor => OS =>Appllication

Dockers：OS => Docker => Application



### Docker常用命令 20241213

```bash
#帮助启动类命令
sudo systemctl start docker
sudo systemctl stop docker
sudo systemctl restart docker
sudo systemctl enable docker
#镜像命令
docker images
docker pull <镜像名>:<标签>
#容器命令
docker run -d -p <宿主端口>:<容器端口> --name <容器名> <镜像名>
docker run -d -p 8080:80 --name my-nginx nginx #示例
docker ps
docker start <容器ID或名称> #启动已停止运行的容器
docker stop <容器ID或名称>
docker rm <容器ID或名称>
docker exec -it <容器ID或名称> /bin/bash #进入后台式容器
docker cp <容器ID>:<容器路径> <本地路径> #从容器中复制文件到本地
```

docker run之后就进入docker容器的交互行了，ls一下好像linux什么都齐全，但是比如vim这些命令肯定就没有了，不是Linux内核的东西docker不会装上去，尽可能瘦身。

两种退出方式：

1. exit，run进去容器，exit退出，容器停止
2. ctrl+p+q   run进去容器，ctrl+p+q退出，容器不停止

### Docker镜像 20241218

>  **UnionFS (Union File System)** 是一种分层文件系统，它是容器镜像的基础技术之一。

> ### **1. 什么是 bootfs 和 rootfs？**
>
> 在容器的文件系统中，有两个核心概念：
>
> - **bootfs (Boot File System)**：类似于传统 Linux 系统中的内核文件系统，包括内核以及初始化系统（initrd/initramfs）。它为容器提供最基本的运行环境。
> - **rootfs (Root File System)**：Linux 的根文件系统，包含 `/bin`, `/etc`, `/usr`, `/var` 等目录和文件。它是容器用户态的运行环境。
>
> 在容器启动时，**bootfs** 会加载内核和基本运行环境，随后会被卸载，而容器主要运行在 **rootfs** 上。
>
> ### **2. UnionFS 的简单结构**
>
> UnionFS 的核心是分层的文件系统结构。在 Docker 中：
>
> - 每个镜像是由多个层组成的，底层是基础镜像（如 `alpine` 或 `ubuntu`），上层是用户修改或添加的文件。
> - 容器启动后，会在镜像顶部创建一个**可写层**。
>
> 一个简单的 UnionFS 结构如下：
>
> ```
> /bootfs   (已卸载，仅用于启动)
> /rootfs
>  ├── Layer 1 (基础镜像层，read-only)
>  ├── Layer 2 (安装了软件包的增量层，read-only)
>  ├── Layer 3 (用户配置层，read-only)
>  └── Writeable Layer (容器的可写层)
> ```
>
> ### **3. 逐步分析一个简单的 UnionFS 案例**
>
> #### **(1) 基础镜像层**
>
> 假设一个容器以 `ubuntu` 镜像为基础：
>
> - 第一层是基础镜像，提供了最小化的根文件系统（如 `/bin/bash`）。
> - 这是一个只读层，不会因容器的运行而改变。
>
> 内容示例：
>
> ```
> bashCopy codeLayer 1:
>   /bin/bash
>   /etc/passwd
> ```
>
> #### **(2) 用户修改或安装层**
>
> 在构建镜像时，用户可能添加了新的软件或文件，这些更改会形成新的只读层：
>
> - 用户通过 `RUN apt-get install` 或其他命令安装了额外的软件。
> - 生成第二层，记录增量修改（如新文件或更改的文件）。
>
> 内容示例：
>
> ```
> rubyCopy codeLayer 2:
>   /usr/bin/python3
>   /etc/apt/sources.list
> ```
>
> #### **(3) 容器可写层**
>
> 当容器运行时：
>
> - 系统为镜像添加一个可写层，所有的写操作都会在这层执行。
> - 任何修改（如新增文件、修改配置）都只影响这一层，不会改变底层的只读层。
>
> 内容示例：
>
> ```
> bashCopy codeWriteable Layer:
>   /tmp/newfile.txt
>   /etc/hostname  (覆盖 Layer 1 的文件)
> ```
>
> ------
>
> ### **4. UnionFS 的合成文件系统**
>
> 容器内实际看到的文件系统是由 UnionFS 合成的：
>
> - 用户只会看到一个整体的文件系统。
> - 查找文件时，UnionFS 会从顶部向底层逐层搜索，找到第一个匹配的文件。
>
> 最终合成结果：
>
> ```
> bashCopy code/
> ├── /bin/bash          (来自 Layer 1)
> ├── /usr/bin/python3   (来自 Layer 2)
> ├── /tmp/newfile.txt   (来自 Writeable Layer)
> ├── /etc/passwd        (来自 Layer 1)
> └── /etc/hostname      (来自 Writeable Layer，覆盖 Layer 1)
> ```
>
> ------
>
> ### **5. 细节：写时复制（Copy-on-Write）**
>
> 当容器尝试修改一个来自只读层的文件时，UnionFS 会执行 **写时复制**：
>
> 1. 将目标文件从只读层复制到可写层。
> 2. 修改文件的副本，原始文件保持不变。
>
> 例如：
>
> - 修改 `/etc/hostname` 时，原始文件在 Layer 1 保持不变。
> - 可写层创建 `/etc/hostname` 的副本，容器只会看到副本的修改结果。
>
> ------
>
> ### **6. 总结**
>
> 通过 **bootfs** 和 **rootfs** 的分离，以及 UnionFS 的分层机制，Docker 实现了以下优势：
>
> 1. **快速启动**：bootfs 仅用于加载内核，随即卸载，rootfs 提供用户态环境。
> 2. **高效存储**：分层文件系统减少了重复存储。
> 3. **写时隔离**：容器的修改仅影响可写层，不会污染底层镜像。
>
> 如果有进一步的问题，可以深入讨论 UnionFS 的实现细节或扩展到具体文件系统（如 OverlayFS）的案例！

=>下载一个Ubuntu镜像，用apt下载vim，用docker commit提交容器副本使之成为本地的一个新的镜像，最简单的Docker镜像70M,加了一个vim就double了...跟git一样还可以docker push推送到阿里云上去



### Docker容器数据卷 20250116

> Docker 容器中的数据卷（Volumes）是 Docker 用于实现数据持久化和容器之间数据共享的一种机制。数据卷的主要特点是它独立于容器的生命周期，可以用于存储数据，即使容器被删除，数据卷中的数据依然保留。

将docker容器内的数据保存进宿主机的磁盘中

=> 数据卷可以挂载到宿主机某一个路径上，数据卷与docker互相反映数据变化，比如创建一个文件什么的，双向影响

=>Volumes 与其看作是个“盘”，不如说看成一种 挂载的 映射关系，所谓容器B继承容器A的Volumes其实就是获取这个 映射关系，所以即便容器A挂了，也不影响容器B继续使用这个 映射关系



dockerhub上搜索 tomcat,mysql，拉下来创建实例，配置端口，运行

### Dockerfile简介 20250129

相当于linux的shell脚本，作用与多次提交commit形成的新镜像一样 

> **Dockerfile** 是一个文本文件，包含一系列指令，用于自动化构建 Docker 镜像。通过 Dockerfile，用户可以定义镜像的构建步骤，包括基础镜像的选择、软件安装、文件复制、环境变量设置等。Docker 根据 Dockerfile 中的指令逐步构建镜像。

以下是一个简单的 Dockerfile 示例，用于构建一个运行 Python 应用的镜像：

```dockerfile
# 使用官方 Python 镜像作为基础镜像
FROM python:3.8-slim

# 设置工作目录，终端默认进入的落脚点
WORKDIR /app

# 复制当前目录内容到容器的 /app 目录
COPY . /app

# 安装依赖
# RUN 等同于，在终端操作的shell命令
RUN pip install --no-cache-dir -r requirements.txt

# 暴露端口
EXPOSE 80

# 设置环境变量
ENV NAME World

# 运行应用
CMD ["python", "app.py"]
```

使用以下命令构建镜像：

```shell
#docker build 是 Docker 提供的命令行工具，用于根据 Dockerfile 中的指令构建镜像。
docker build -t my-python-app
docker run -p 4000:80 my-python-app
```



常用保留字指令均是大写，如 FROM

每条指令都会创建一个新的镜像层对镜像进行提交

dockerhub上拉下来的镜像基本上最后都会有一个dockerfile作为理论支持

> `FROM` 指令中的镜像查找顺序如下：
>
> 1. 本地镜像缓存
> 2. Docker Hub（默认）
> 3. 私有镜像仓库（如果指定了仓库地址）
> 4. 其他公共镜像仓库（如 Quay.io、GCR、ECR 等）
> 5. 本地构建的镜像

ADD命令可以直接将本地下载好的比如jdk直接打入docker...牛逼

### Docker network 20250131

> ### **1. 什么是网络接口？**
>
> 网络接口（Network Interface）是设备（如计算机、服务器、路由器等）与网络之间进行数据传输的硬件或软件组件。它可以是物理的（如网卡）或虚拟的（如虚拟网桥、虚拟以太网接口）。
>
> - **物理网络接口**：通常是硬件设备，如以太网卡（`ens33`）、无线网卡（`wlp2s0`）。
> - **虚拟网络接口**：由软件实现，如虚拟网桥（`docker0`）、虚拟以太网接口（`veth`）、回环接口（`lo`）。
>
> ### **2. 网络接口的作用**
>
> 网络接口的主要作用是：
>
> 1. **数据传输**：在网络中发送和接收数据包。
> 2. **地址分配**：为设备分配 IP 地址和 MAC 地址。
> 3. **协议支持**：支持网络协议（如 TCP/IP）的实现。
> 4. **网络隔离**：通过虚拟接口实现网络隔离（如 Docker 容器网络）。
>
> ------
>
> ### **3. 网络接口的物理图例**
>
> 以下是一个简单的网络拓扑图，展示了不同类型的网络接口及其作用：
>
> ```
> +-------------------+       +-------------------+       +-------------------+
> |    Host Machine   |       |    Docker Host    |       |   Remote Server   |
> |                   |       |                   |       |                   |
> |  +-------------+  |       |  +-------------+  |       |  +-------------+  |
> |  |   ens33     |  |       |  |   docker0    |  |       |  |   eth0      |  |
> |  | (Physical)  |  |       |  | (Virtual)    |  |       |  | (Physical)  |  |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |  |   Switch    |  |       |  |   veth       |  |       |  |   Router    |  |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |  |   Internet  |  |       |  |   Container |  |       |  |   Internet  |  |
> |  +-------------+  |       |  +-------------+  |       |  +-------------+  |
> +-------------------+       +-------------------+       +-------------------+
> ```

ens33就是之前的额eth0，eth1一类的，当然不止这三种网络接口，用ipconfig打出来的信息可以看

> 在 Docker 容器网络中，`veth`（Virtual Ethernet Device，虚拟以太网设备）和 `docker0`（默认的虚拟网桥）一起工作，实现了容器与宿主机及其他容器的网络通信。它们的关系可以理解为 **“桥接网络中的虚拟网卡”** 和 **“连接这些网卡的交换机”**。
>
> `docker0` 是 Docker 默认创建的 **Linux 网桥（bridge）**，相当于一个二层交换机，负责连接所有 Docker 容器的网络接口，并提供它们之间的通信。
>
> - 默认情况下，`docker0` 具有一个分配的网段（例如 `172.17.0.1/16`）。
> - 容器之间的通信通过 `docker0` 进行转发，类似物理网络中的交换机。
> - 宿主机可以通过 `docker0` 访问所有连接到该网桥的容器。
>
> `veth` 是 **一对虚拟网卡（veth pair）**，用来连接 Linux 网络命名空间（Network Namespace）。它的特点是：
>
> - `veth` 总是成对出现（`vethX` 和 `vethY`），类似网线的两端。
> - 一端（`vethX`）连接到 `docker0` 网桥，另一端（`vethY`）放入容器的网络命名空间，充当容器的 `eth0`。
> - `veth` 设备可以传输数据，类似物理网卡+网线。



**Docker 网络模式**

| **网络模式** | **bridge**                           | **host**                 | **none**         |
| :----------- | :----------------------------------- | :----------------------- | :--------------- |
| **描述**     | 默认模式，使用 `docker0` 网桥        | 容器直接使用主机的网络栈 | 容器没有网络接口 |
| **网络隔离** | 是                                   | 否                       | 是               |
| **IP 地址**  | 容器分配独立的 IP（如 `172.17.0.2`） | 使用主机的 IP 地址       | 无 IP 地址       |

> - **`bridge` 模式**：
>   - 默认模式，容器通过 `docker0` 网桥与主机和其他容器通信。
>   - 需要手动映射端口（如 `-p 8080:80`）。
>   - 适用于多容器通信的场景。
> - **`host` 模式**：
>   - 容器直接使用主机的网络栈，性能较高。
>   - 不需要端口映射，容器端口直接绑定到主机端口。
>   - 适用于高性能需求的场景。
> - **`none` 模式**：
>   - 容器没有网络接口，完全隔离网络。
>   - 适用于不需要网络访问的场景。

> - `bridge` 模式是默认模式，适合大多数场景。
> - `host` 模式性能高，但安全性较低。
> - `none` 模式完全隔离网络，适合特殊需求。

=>bridge模式使用docker0网桥 是主流？

=>对一个docker容器而言，他有可能有两个网络接口，除了回环之外，eth0就是docker给它单独分配的！！这也是容器内部监视 *:5000 与 localhost:5000不同的原因！(2025.2.16)

```
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
```

还有container模式，就是多个container公用一个虚拟网卡eth0与bridge连接。

还可以自定义网络模式，好像可以指定主机名与ip对应，而不是像bridge那样自动分配

### Docker compose 20250203

> **Docker Compose** 是一个用于定义和运行多容器 Docker 应用程序的工具。它通过一个 **YAML 文件**（通常命名为 `docker-compose.yml`）来描述应用程序的 **服务、网络和数据卷**，并且可以使用 **一条命令** (`docker-compose up`) 来启动或管理所有容器。
>
> 以下是一个 `docker-compose.yml` 示例
>
> ```yaml
> version: '3.8'
> 
> services:
>   web:
>     image: python:3.9
>     container_name: flask_app
>     working_dir: /app
>     volumes:
>       - .:/app
>     ports:
>       - "5000:5000"
>     command: python app.py
>     depends_on:
>       - db
> 
>   db:
>     image: mysql:5.7
>     container_name: mysql_db
>     restart: always
>     environment:
>       MYSQL_ROOT_PASSWORD: root
>       MYSQL_DATABASE: mydb
>       MYSQL_USER: user
>       MYSQL_PASSWORD: password
>     ports:
>       - "3306:3306"
> ```
>
> 启动所有服务: docker-compose up



> **Docker Compose** 主要用于 **单机** 上管理多个 Docker 容器。
>
> **Kubernetes（K8s）** 主要用于 **分布式环境**，管理多个节点上的容器。

Docker Compose用于本地开发、测试环境，而K8s生产级容器编排



### Portainer 与 CIG 20250204

> Portainer 是一个轻量级的容器管理工具，旨在简化 Docker环境的管理。它提供了一个直观的 Web UI，使用户能够方便地部署、管理和监控容器应用，而无需使用复杂的命令行工具。

=>windows不是直接有gui界面嘛...

Portainer 就是轻量级，重量级的监控系统就要用CAdvisor+InfulxDB+Granfana

> ### **① cAdvisor（Container Advisor）**
>
> - **功能**：由 Google 开发的开源工具，专门用于收集容器的资源使用情况（CPU、内存、磁盘 I/O、网络流量等）。
>   - 轻量级，直接运行在宿主机上
>   - 原生支持 Docker，无需额外配置
>   - 提供 REST API，支持 Prometheus、InfluxDB 等多种后端存储
>
> ### **InfluxDB（时序数据库）**
>
> - **功能**：一个高效的时序数据库，专门用于存储时间序列数据，如指标、事件、日志等。
>   - 适用于高吞吐量写入的场景
>   - 支持 SQL 类似的查询语言（InfluxQL）
>   - 可与 cAdvisor、Telegraf、Grafana 轻松集成
>
> ### **③ Grafana（数据可视化）**
>
> - **功能**：一个强大的数据可视化工具，可连接多个数据源（InfluxDB、Prometheus、MySQL 等）并创建自定义监控面板。
>   - 直观的仪表盘（Dashboard）
>   - 灵活的数据查询和告警功能
>   - 多种插件支持（Graphite、InfluxDB、Prometheus、Elasticsearch）

cAdvisor只能存储2分钟的数据，所以需要InfluxDB

**cAdvisor、InfluxDB 和 Grafana** 都可以运行在容器中，通常使用 **Docker 容器** 部署它们，以实现快速安装、隔离环境和便捷管理。最后用 **Docker Compose** 或 **Kubernetes (K8s)** 来管理这些容器，使它们能够自动启动、重启并相互连接。

为Granfana配置来自InfluxDB的数据源...



# JIRA

> Jira 和 Azure DevOps Boards 都是支持敏捷开发的项目管理工具，提供如工作项管理、看板视图、冲刺规划、Backlog 管理等核心功能，适用于敏捷团队的日常协作。两者的主要区别在于侧重点不同：**Jira** 更灵活，支持高度自定义的工作流和丰富的报表，适合产品管理和跨部门协作；而 **Azure DevOps Boards** 深度集成代码库、构建、部署和测试，贴合开发流程，适合以开发为中心的团队，尤其是在使用 Azure Pipelines 或 Repos 的情况下更具优势。

## Azure DevOps Boards 20250407

Azure Boards 中的层级结构:

```
Epic  # 史诗,代表 大型目标或业务需求,通常跨越多个团队、多个冲刺甚至多个版本
└── Feature # Epic 的组成部分，代表一个可交付的、有业务价值的子目标
       └── Task # 进一步细化，用于开发、测试、设计等具体工作
```

左侧菜单（Boards、Backlogs、Work Items 等）其实是从不同角度来查看和管理工作项（Work Items）:

| 菜单项     | 作用       | 视角       | 适合人群               |
| ---------- | ---------- | ---------- | ---------------------- |
| Work Items | 快速查找   | 单个工作项 | 所有人                 |
| Boards     | 状态看板   | 状态流转   | 开发、测试             |
| Backlogs   | 层级规划   | 需求结构   | 产品经理、Scrum Master |
| Sprints    | 冲刺管理   | 任务执行   | Scrum 团队             |
| Queries    | 自定义搜索 | 灵活分析   | PM、QA、技术负责人     |

=>所以菜单项针对不同的人群哈，Work Items是最核心的，你增加一个Feature，可以没有父节点，即不必attach到某一个Epic上去，你在Backlogs可以用转到 Feature视角你就可以看到这个Feature的

=>另外不仅是Work Items，你在Backlogs，Sprints里均可以增加许多层级的任务，然后在其他菜单中也会关联显示出来。其底层都是各种任务，不同的菜单就是不同筛选不同视角的呈现而已！
