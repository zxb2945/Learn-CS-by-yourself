# 自动化运维学习

# Ansible

## 历史与安装

历史：

> Ansible 是由 Michael DeHaan 于 2012 年开发的。最初，Ansible 是作为一个个人项目启动的，旨在简化 IT 自动化和配置管理。2013 年，Ansible 被开源，并迅速受到社区的欢迎。
>
> 在随后的几年里，Ansible 逐渐发展，增加了许多功能，支持各种平台和服务。2015 年，Ansible 被 Red Hat 收购，并继续在其基础上发展，成为现代 IT 基础设施管理的重要工具之一。

如何安装:

Ubuntu

```
sudo apt update
sudo apt install ansible
ansible --version
```

CentOS/RHEL

```
sudo yum install epel-release
sudo yum install ansible
ansible --version
```

> Ansible 采用的是一种“无代理”（agentless）的架构，这意味着你不需要在目标服务器上安装 Ansible。Ansible 通过 SSH（对于 Linux/Unix 系统）或 WinRM（对于 Windows 系统）与目标机器通信，直接在控制节点（Ansible 所在的服务器）上执行命令和剧本。

## Ansible运维自动化

[Link](https://www.bilibili.com/video/BV1DY4y137Be?p=2&spm_id_from=pageDriver&vd_source=6fc477a8e79179a3fd30bed2e2ba5fbe)

ansible是新出现的自动化运维工具，基于Python开发，集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，实现了**批量**系统配置、**批量**程序部署、**批量**运行命令等功能。

### Ansible Inventory 20240903

在hosts文件里写各种主机的IP，可以分组，有专门的格式要求

```
[web]
172.16.1.5 
172.16.1.6
[db]
172.16.1.7
[nfs]
172.16.1.11
```

甚至可以将用户名和密码写进主机清单里，但是不推荐

```shell
#在hosts文件里写各种主机的IP，可以分组
#-i inventory list 指定主机清单
#-m 指定模块
#-a 指定动作
ansible -i hosts all -m ping

#从ansible所在主机远程操作
ssh 172.16.1.6 'lis -l .ssh'
```

### Ansible模块 20240904

| 命令/脚本模块 |                                                              |
| ------------- | ------------------------------------------------------------ |
| ping模块      | 检查ansible与主机连通性                                      |
| shell模块     | 批量执行shell命令                                            |
| command模块   | 不指定模块,默认就使用该模块，批量执行简单命令                |
| scripts模块   | 分发脚本并执行，在ansible上写个脚本分发到各个主机执行，比如安装Python |

调用具体模块要什么参数，查阅ansible官方网站

#### 

| 文件相关模块       |                                |                                                              |
| ------------------ | ------------------------------ | ------------------------------------------------------------ |
| file模块           | 文件目录创建，删除...          |                                                              |
| copy模块           | 远程复制，分发文件             | ansible -i hosts all -m copy -a 'src=/etc/hostname dest=/tmp/' |
| **服务管理模块**   |                                |                                                              |
| systemd模块        | 服务的开机自启动，服务开启关闭 |                                                              |
| service模块        |                                |                                                              |
| **软件包管理**     |                                |                                                              |
| yum_repository模块 | 给远端配置一个yum源文件        | ansible -i hosts lb -m yum_repository -a '...'               |
| yum模块            | 安装文件                       | ansible -i hosts lb -m yum -a 'name=nginx state=latest'      |
| get_url模块        |                                |                                                              |
| **系统管理**       |                                |                                                              |
| moute模块          | moute远程挂在nfs               | ansible -i hosts web -m mount -a 'fstype=nfs ...'            |
| cron模块           | 定时任务模块                   |                                                              |
| **用户管理**       |                                |                                                              |
| group管理          |                                |                                                              |
| user模块           |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |
|                    |                                |                                                              |



ansible现在有上千的模块...

跳板机/堡垒机: Jump Server, 通过跳板机管理其他所有机器

网站架构-运维角度：Developers => VPN通道 => 跳板机 => 批量管理：Ansible => 监控，日志收集



ansible返回的结果通常是json结构

### Ansible剧本(Playbook) 20240918

对于重复性操作与部署推荐使用剧本（脚本），使用模块+固定格式（对齐）

剧本格式：yaml格式，yml尾缀的文件

Playbook组成

Ansible Playbook 示例：

```yaml
---
- name: 安装和配置 Nginx
  hosts: webservers
  become: yes  # 以超级用户身份运行任务
  vars: #定义在 playbook 中使用的变量
    nginx_port: 80
    nginx_server_name: "example.com"
  tasks:
    - name: 确保 Nginx 已安装
      apt:
        name: nginx
        state: present
      notify:
        - 重启 Nginx

    - name: 配置 Nginx
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
      notify:
        - 重启 Nginx

    - name: 确保 Nginx 服务正在运行
      service:
        name: nginx
        state: started
        enabled: yes

  handlers:
    - name: 重启 Nginx
      service:
        name: nginx
        state: restarted

```

你可以使用 `ansible-playbook` 命令来运行这个 playbook，例如：`ansible-playbook -i inventory playbook.yml`



注意空格缩进，还不能用tab键，有些琐碎的格式要求

Playbook中可以定义变量和使用变量，除了自定义，还有内嵌变量facts，获取主机名,ip，cpu信息

可以将Playbook中这些变量抽出到一个独立文件vars_files中维护

> Ansible 中的 `register` 用于捕获任务的输出并将其存储在变量中，供后续任务使用。

总结：ansible中变量定义就是这三种：自定义，facts，register



剧本流程控制：

1. handler触发器 => 关键词 notify 与 handlers 配合，类似于C语言中的“goto”
2. when判断 => when一般与ansible facts变量一起使用，判断主机/判断系统系类
3. loop循环=> 关键词 with items, 批量启动服务，批量添加用户，感觉这里所谓的循环更像 批量操作，事先列出所有项目，所以无所谓某一条件的break

```yaml
---
- name: Install and configure Nginx on Ubuntu
  hosts: webservers
  become: true  # 以超级用户权限执行

  vars:
    nginx_conf_file: /etc/nginx/nginx.conf
    server_blocks:
      - server_name: example.com
        root: /var/www/example
      - server_name: example.org
        root: /var/www/example_org
    is_ubuntu: "{{ ansible_distribution == 'Ubuntu' }}"  # 声明 Ubuntu 条件

  tasks:
    - name: Install Nginx
      apt:
        name: nginx
        state: present
      notify: Restart Nginx  # 触发 handler
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Ensure the Nginx configuration file exists
      file:
        path: "{{ nginx_conf_file }}"
        state: touch
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Configure Nginx server blocks
      template:
        src: nginx.conf.j2  # 模板文件
        dest: "{{ nginx_conf_file }}"
      notify: Restart Nginx  # 触发 handler
      when: is_ubuntu  # 仅在 Ubuntu 上执行

    - name: Create document root directories
      file:
        path: "{{ item.root }}"
        state: directory
        owner: www-data
        group: www-data
        mode: '0755'
      with_items: "{{ server_blocks }}"
      when: is_ubuntu  # 仅在 Ubuntu 上执行

  handlers:
    - name: Restart Nginx
      service:
        name: nginx
        state: restarted
```

简单的 Nginx 配置模板nginx.conf.j2：

```
http {
    {% for block in server_blocks %}
    server {
        listen 80;
        server_name {{ block.server_name }};

        root {{ block.root }};
        index index.html;

        location / {
            try_files $uri $uri/ =404;
        }
    }
    {% endfor %}
}

```

> **Handler**：
>
> - 在 Playbook 的最后部分定义。名为 `Restart Nginx` 的 handler 会在其他任务通过 `notify` 指令触发时被执行。这里，它会重新启动 Nginx 服务
>
> **`with_items` 循环**：
>
> - `with_items` 用于循环创建多个文档根目录。这里通过 `server_blocks` 变量中的每个项目，创建对应的目录



对于比较复杂的剧本，可以进行拆分，分成记录主要步骤的主剧本 以及 一些yml小剧本，即所谓的 include机制

还可以用roles规则对这一些yml小剧本分门别类放在不同的路径中，使文件层次更为清晰



### Ansible项目  20241028

部署架构的时候，拆分不同的组件/模块，通过Ansible roles部署每个模块

1. 基础环境：yum源，用户，关闭防火墙，内核参数... （模板机做了）
2. 应用/服务环境：服务 nginx php 安装配置启动， 通用独立全面，根据不同的服务应用拆分 LNMP
3. 业务环境代码：比如部署Java的代码

=>可以对开发完成的项目一键部署，虽然没使用过，但是NTC现场完全可以用Ansbile来升级的



1通过服务器部署及配置流程-->转换为对应的步骤

2根据步骤转化成模块

3书写剧本，调试剧本

1. 安装：使用什么模块
2. 配置：使用什么模块
3. 启动与使用：使用什么模块
4. 最后书写剧本roles



# Terraform 

### Terraform Crash Course 20250208

[link]([The Terraform Crash Course](https://www.youtube.com/watch?v=4ukf6-S8wL4))

> Terraform 是一个 **基础设施即代码（Infrastructure as Code, IaC）** 工具，由 **HashiCorp** 开发。它用于**自动化管理云基础设施**，可以让你使用代码来定义、部署和管理基础设施资源（如服务器、数据库、网络等）。
>
> ### **Terraform 的核心特点**
>
> 1. **声明式（Declarative）**
>    - 你只需描述最终的基础设施状态，Terraform 会自动计算并执行必要的更改。
> 2. **跨云平台**
>    - 支持 AWS、Azure、Google Cloud、Kubernetes、VMware、阿里云等多种云平台，甚至可以用于本地数据中心。
> 3. **状态管理（State Management）**
>    - Terraform 维护一个 **状态文件（tfstate）** 来跟踪当前的基础设施状态，并确保所有更改都可预测和可追踪。
> 4. **模块化（Modular）**
>    - 可以使用 **模块（Modules）** 复用代码，提高管理效率。
> 5. **可审计 & 可版本化**
>    - 基础设施代码可以存入 Git 进行版本管理，方便审计和协作。
>
> 
>
> ### **Terraform 工作流程**
>
> 1. **编写配置文件**（.tf 文件）
>    - 使用 HCL（HashiCorp Configuration Language） 或 JSON 描述基础设施。
> 2. **初始化（terraform init）**
>    - 下载相关插件和依赖项。
> 3. **执行计划（terraform plan）**
>    - 预览 Terraform 计划做出的更改，不会真正修改基础设施。
> 4. **应用变更（terraform apply）**
>    - 执行变更，创建或更新基础设施。
> 5. **销毁资源（terraform destroy）**
>    - 删除所有配置的资源，释放云平台上的基础设施。
>
> 
>
> ### Terraform 示例
>
> 下面是一个创建 AWS EC2 实例的 Terraform 配置文件（`main.tf`）：
>
> ```
> provider "aws" {
>   region = "us-east-1"
> }
> 
> resource "aws_instance" "example" {
>   ami           = "ami-12345678"
>   instance_type = "t2.micro"
>   
>   tags = {
>     Name = "TerraformInstance"
>   }
> }
> ```
>
> 执行以下命令：
>
> ```shell
> terraform init      # 初始化 Terraform
> terraform plan      # 预览变更
> terraform apply     # 应用变更，创建实例
> terraform destroy   # 删除实例
> ```
>
> 
>
> ### **Terraform vs. 其他 IaC 工具**
>
> | 工具               | 语言                 | 主要特点           | 适用场景               |
> | ------------------ | -------------------- | ------------------ | ---------------------- |
> | **Terraform**      | HCL（声明式）        | 多云支持，状态管理 | 适用于大规模云基础设施 |
> | **Ansible**        | YAML（命令式）       | 偏向配置管理       | 配置服务器和应用部署   |
> | **CloudFormation** | JSON/YAML            | 仅支持 AWS         | 适用于 AWS 生态        |
> | **Pulumi**         | Python/Go/TypeScript | 编程式 IaC         | 适合 DevOps 和开发者   |
>
> 
>
> ### **Terraform 适用于哪些场景？**
>
> - **多云管理**（AWS + Azure + GCP）
> - **自动化基础设施部署**（创建服务器、数据库、网络等）
> - **DevOps CI/CD**（结合 GitOps，自动部署环境）
> - **Kubernetes 资源管理**
> - **灾难恢复（DR）**（一键恢复基础设施）



用vs编辑尾缀为tf的文件，加入相应插件

如何安装 Terraform，这个像一个编译器？terraform plan直接编译tf文件，甚至terraform apply运行它，然后可以得到一个tfstate的描述设施状态的文件

还有其他命令 terraform destroy

接下来Manage Resource on AWS:

创建一个 IAM 得到key和密码可以访问终端，在vs上用export命令注册它？

如果要创建一个EC2，你可以访问[link](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)，查看该如何定义它在tf文件中，定义resource，data等，所以这方面你都不用记忆什么东西，最后通用运行它



### Complete Terraform Course

[Link](https://www.youtube.com/watch?v=7xngnjfIlK4&t=8883s)

#### 简介 20250218

1. Evolution of Cloud + Infrastructure as Code
2. Terraform Overview + Setup
3. Basic Terraform Usage  =>展示了一个main.tf文件用于创建Router53 LoaderBalacner等一个后后端架构的创建
4. Variables and Outputs
5. Language Features =>HCL语言
6. Projest Organization + Modules
7. Mnaging Multiple Environments
8. Testing Terraform Code
9. Developer workflows

Declarative vs. Imperative

Provisioning + Config Management => Terraform+Ansible

Provisioning + Orchestration=> Terraform+Kubernetes

> Terraform 通过 **Provider**（提供者）与云平台进行交互，每个 Provider 都封装了云平台的 API，Terraform 通过声明式配置管理资源。

安装Terraform就是在vscode一行命令的事，拿到AWS的IAM，然后通过tf脚本就可以操控AWS了

#### Terraform State File

> Terraform **State File**（状态文件）是 Terraform 用来跟踪和管理基础设施资源的文件。它存储了 Terraform 管理的所有资源的当前状态，并在 `terraform apply` 运行时用于比较和决定哪些更改需要执行。
>
> State 文件是一个 **JSON 格式** 的文件，包含 Terraform 追踪的所有资源。例如：
>
> ```json
> {
>   "version": 4,
>   "terraform_version": "1.5.0",
>   "serial": 1,
>   "resources": [
>     {
>       "mode": "managed",
>       "type": "aws_instance",
>       "name": "web",
>       "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
>       "instances": [
>         {
>           "schema_version": 1,
>           "attributes": {
>             "id": "i-1234567890abcdef0",
>             "ami": "ami-0abcdef1234567890",
>             "instance_type": "t2.micro",
>             "private_ip": "10.0.1.25",
>             "public_ip": "34.203.120.12"
>           }
>         }
>       ]
>     }
>   ]
> }
> 
> ```
>
> 默认情况下，Terraform 将 `terraform.tfstate` 存储在 **本地目录**（即执行 `terraform apply` 的目录）。但对于团队协作，建议使用 **远程存储**，如：AWS S3 + DynamoDB

#### HCL语言 20250220

> HCL（HashiCorp Configuration Language）是一种专为基础设施即代码（Infrastructure as Code, IaC）设计的声明式配置语言，由 HashiCorp 开发。它主要用于 HashiCorp 的工具，如 **Terraform**、**Consul** 和 **Nomad**，用于定义和管理云基础设施、服务发现和编排任务。
>
> **声明式语法**：HCL 采用类似 JSON 的结构，但比 JSON 更具可读性，同时支持嵌套和复杂配置。

介绍了HCL语言中的Variable Types：Input Variables, Local Variables, Output Variables

tf采用的HCL语言还是有许多内容的...

可以把Sensitive Data存储在一个单独的tfvars文件中，通过pass的手段传递，这还有另外的好处，tf文件相当于一个函数，根据不同的参数可以deploy到不同的云设施中去

Meta-Arguments:

> 在 Terraform 的 HCL 语言中，**Meta-Arguments（元参数）** 是特殊的参数，它们可以用于 **控制资源的创建、依赖关系和循环逻辑**，从而增强 Terraform 配置的灵活性和可复用性。
>
> Terraform 中的 Meta-Arguments 主要包括以下几个：
>
> | Meta-Argument | 作用                                      |
> | ------------- | ----------------------------------------- |
> | `count`       | **创建多个相同资源**（基于整数值）        |
> | `for_each`    | **基于 map 或 set 迭代创建多个资源**      |
> | `depends_on`  | **显式定义资源之间的依赖关系**            |
> | `provider`    | **指定特定的 provider（云提供商）**       |
> | `lifecycle`   | **控制资源的生命周期（防止删除/更新等）** |
>
> 如果你在 CI/CD 和 Cloud 领域工作，Meta-Arguments 在 **自动化基础设施管理** 里非常重要：
>
> - `count` 和 `for_each` 适用于 **批量部署 Kubernetes、Lambda、VM**
> - `depends_on` 确保 **CI/CD 运行顺序正确**
> - `provider` 适用于 **多云环境**
> - `lifecycle` 可以防止 **意外破坏生产环境**

相当于HCL语句中的关键字

Provisioners：

> `Provisioners`（供应器）是 Terraform 用于在 **资源创建后** 或 **销毁前** 执行本地或远程命令的功能。它们通常用于：
>
> - **配置服务器**（如安装软件、修改系统设置）
> - **执行脚本**（如 Bash、PowerShell）
> - **文件传输**（如上传配置文件）
> - **处理资源删除逻辑**（如运行清理脚本）
>
>  **常见的 Provisioners**
>
> | Provisioner 类型 | 作用                                 |
> | ---------------- | ------------------------------------ |
> | `local-exec`     | 在 **本地主机** 上执行 Shell 命令    |
> | `remote-exec`    | 在 **远程主机** 上执行 Shell 命令    |
> | `file`           | 将 **本地文件或目录** 复制到远程主机 |
> | `chef`           | 运行 Chef 客户端进行配置管理         |
> | `puppet`         | 运行 Puppet 客户端进行配置管理       |
>
> ### **示例：Terraform 部署后发送 Slack 通知**
>
> ```
> hCopyEditresource "aws_instance" "example" {
>   ami           = "ami-12345678"
>   instance_type = "t2.micro"
> 
>   provisioner "local-exec" {
>     command = "curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"EC2 已创建!\"}' https://hooks.slack.com/services/xxx"
>   }
> }
> ```
>
> 🔹 **作用**：Terraform 创建 EC2 之后，发送 Slack 通知。
>
> **官方不推荐使用 Provisioners**，因为 Terraform 更倾向于声明式配置，而 Provisioners 属于命令式配置。如果可以，建议用 Ansible、Cloud-Init、User Data 等替代。



#### Terraform Module

Why Modules? 

Infrastructure Speclialists + Applicaiton Developers

> **Module（模块）** 是 Terraform 里的 **可复用代码单元**，可以用来组织、管理和复用 Terraform 配置。你可以把 Terraform Module 理解为 **函数或库**，它能：
>
> - **封装** 复杂资源，提供简单的接口
> - **提高代码复用性**，避免重复写相同配置
> - **简化维护**，让基础设施更易管理
> - **支持参数化**，通过变量自定义资源行为
>
> ### **Module 的基本结构**
>
> 一个 Terraform Module 其实就是 **一个包含 `.tf` 配置文件的目录**。比如，下面的目录结构就是一个 Module：
>
> ```
> perlCopyEditmy-module/
> │── main.tf      # 核心资源定义
> │── variables.tf # 定义输入变量
> │── outputs.tf   # 定义输出变量
> ```
>
> 在 Terraform 配置中，你可以 **调用** 这个 Module，并传递参数来创建资源。
>
> ## **什么时候使用 Module？**
>
> | 场景                                  | 适合使用 Module？            |
> | ------------------------------------- | ---------------------------- |
> | **多个环境（dev/prod）**              | ✅ 是，封装后只需改变量       |
> | **重复使用相同资源（VPC, EC2, RDS）** | ✅ 是，提高复用性             |
> | **团队协作**                          | ✅ 是，模块化结构更清晰       |
> | **简单单一的资源**                    | ❌ 否，可以直接写在 `main.tf` |

可以自己写Module，然后在主要tf中引用，也可以直接引用远程仓库中的module

设计Module就像设计软件开发中各种类对吧，需要好好思考

> Terraform Registry 是 Terraform 官方提供的 **模块和提供商（Providers）仓库**，你可以从这里下载和使用现成的 Terraform 资源。
>
> Terraform Registry 主要包含两大类：
>
> 1. **Providers（提供商）**：AWS、Azure、GCP、Kubernetes 等
> 2. **Modules（模块）**：VPC、EC2、EKS、RDS 等

你可以去下载Providers，也可以进一步下载相应的模块，你还可以自己上传Module到远端仓库



#### Terraform Workspaces

> **Terraform Workspaces** 允许你在 **同一套 Terraform 配置** 下管理多个独立的环境（如 `dev`、`staging`、`prod`）。它的作用类似于 **命名空间**，可以用来管理多个不同的 **状态（state）**。
>
> 在 Terraform 中，默认的状态文件（`terraform.tfstate`）存储资源信息。如果你需要多个环境（如 `dev` 和 `prod`），直接使用 `terraform.tfstate` 可能会导致环境混乱。
>
> **Workspaces 让你可以在同一份配置代码下，创建多个状态文件，分别管理不同的环境。**
>
> **每个 Workspace 都有独立的 `terraform.tfstate` 文件**，不会互相覆盖。

可以通过像git那样切换不同的环境

> ## **Workspaces vs. 变量文件（tfvars）**
>
> | 方式                     | 适用场景                                            | 优势                       | 缺点                           |
> | ------------------------ | --------------------------------------------------- | -------------------------- | ------------------------------ |
> | **Terraform Workspaces** | 管理多个环境（dev, staging, prod）                  | 同一份代码，多环境独立管理 | 在复杂项目中可能难以管理       |
> | **变量文件（tfvars）**   | 通过 `terraform apply -var-file` 传递不同环境的变量 | 灵活控制变量               | 需要额外维护多个 `tfvars` 文件 |
>
> 对于小型项目，**Workspaces 更方便**。
> 对于大型项目，推荐 **用 `tfvars` 文件管理环境**，如：



> [Terragrunt](https://github.com/gruntwork-io/terragrunt) 是 Terraform 的一个 **辅助工具**，它帮助管理 **多环境（multi-environment）基础设施**，解决 Terraform **代码重复** 和 **State 管理** 的问题。
>
> 👉 **简单来说，Terragrunt 是 Terraform 的一个 Wrapper，帮你优化代码结构和管理远程状态！**

用了很大篇幅讲file structure，相当于C#设计各种类，让项目更容易管理

#### Test and Develop 20250221

**Code Rot**（代码腐烂）：指的是**代码质量随着时间的推移逐渐恶化，变得难以理解、维护或扩展**，最终影响软件的稳定性和开发效率。

Static Check：比如用terraform fmt -check这个命令来检查

Manual Testing

Automated Testing: 可以用go语言写脚本测试..可以在脚本中模拟发送HTTP，就在不实际操作server的情况下完成单体测试？

至于开发，可以用Github Action，GitLab的CICD功能

Cloud Nuke: 一次性清除单一账号的所有云上资源的工具，避免被计费



你可以用Github Action

1. 写个terraform.yml
2. 设置连接AWS上的服务器测试
3. 在这个服务器上操作各种Terraform命令
4. 甚至还要调用go来测试



### Learn Terraform with Azure by Building a Dev Environment 20250331

[Link](https://www.youtube.com/watch?v=V53AHWun17s)

Install Azure CLI on Windows  => reopen vscode 就可以在terminal 运行 `az login` 等Azure的命令

查阅Azure Provider，用于Terraform调用

Terminal Command:

```shell
terraform fmt
terraform plan

terraform plan -destroy   #适用于检查销毁前的影响  
terraform apply -auto-approve #适用于自动化环境（如 CI/CD），但在生产环境要谨慎使用
terraform state list
terraform apply -refresh-only
terraform output
terraform console
```

main.tf:

```terraform 
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }
  required_version = ">= 1.0"
}

provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "example" {
  name     = "example-resources"
  location = "East US"
}

resource "azurerm_virtual_network" "example" {
  name                = "example-network"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
  address_space       = ["10.0.0.0/16"]
}

resource "azurerm_subnet" "example" {
  name                 = "example-subnet"
  resource_group_name  = azurerm_resource_group.example.name
  virtual_network_name = azurerm_virtual_network.example.name
  address_prefixes     = ["10.0.1.0/24"]
}

resource "azurerm_network_security_group" "example" {
  name                = "example-nsg"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name

  security_rule {
    name                       = "allow-ssh"
    priority                   = 1001
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "22"
    source_address_prefix      = "*"
    destination_address_prefix = "*"
  }
}

resource "azurerm_public_ip" "example" {
  name                = "example-public-ip"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name
  allocation_method   = "Dynamic"
}

resource "azurerm_network_interface" "example" {
  name                = "example-nic"
  location            = azurerm_resource_group.example.location
  resource_group_name = azurerm_resource_group.example.name

  ip_configuration {
    name                          = "internal"
    subnet_id                     = azurerm_subnet.example.id
    private_ip_address_allocation = "Dynamic"
    public_ip_address_id          = azurerm_public_ip.example.id
  }
}

resource "azurerm_virtual_machine" "example" {
  name                  = "example-vm"
  location              = azurerm_resource_group.example.location
  resource_group_name   = azurerm_resource_group.example.name
  network_interface_ids = [azurerm_network_interface.example.id]
  vm_size               = "Standard_DS1_v2"

  storage_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "18.04-LTS"
    version   = "latest"
  }

  storage_os_disk {
    name              = "example-os-disk"
    caching           = "ReadWrite"
    create_option     = "FromImage"
    managed_disk_type = "Standard_LRS"
  }

  os_profile {
    computer_name  = "example-vm"
    admin_username = "adminuser"
  }
}
```

你可以对tf文件做一些修改，增加SSH登录功能，然后为VScode增加remote-SSH的扩展，你可以直接登录资源中的VM,是那种左侧都会出现Linux目录的那种深度登录哦。

> Terraform `.tpl` 文件通常用于 `templatefile()` 或 `data "template_file"` 方式，来动态填充变量。
>
> `.tpl` 文件本质上是一个文本文件，其中可以使用 `${}` 语法来插入变量。
>
> ```shell
> Hello, ${name}! Welcome to ${company}.
> ```

> 如果你需要在 VM 启动时执行某些初始化任务（如安装软件、配置用户），可以使用 `.tpl` 文件来动态生成 `cloud-init` 配置

你可以在 main.tf中使用 provisioner去调用tpl文件来为VM预置Docker等其它操作。

=>不太推荐provisioner，更多应该是Ansible的工作，因为Terraform无法探知provisioner所带来的状态变化，你需要 `terraform apply -replace resource_name`来使其provisioner生效





# Jenkins

[Jenkins自动化部署入门详细教程](https://www.cnblogs.com/wfd360/p/11314697.html)

## 尚硅谷Jenkins教程

[link](https://www.bilibili.com/video/BV1bS4y1471A?spm_id_from=333.788.videopod.episodes&vd_source=c13700902d2c98df282b6f1f2889c0cb&p=4)

### GitLab介绍及安装准备20241202

GitLab所在的服务器至少16G内存

> GitHub 本身是一个由 GitHub 公司托管的 **云服务**，并没有开放完整的自托管版本。GitHub 主要是提供云托管服务的，因此它不能像 GitLab 那样直接在自己的服务器上搭建和运行。
>
> 然而，**GitHub 提供了 GitHub Enterprise**，这是一款针对企业的私有部署版本，允许企业在自己的服务器上运行 GitHub。

> **GitLab.com** 是 GitLab 提供的云托管平台，类似于 GitHub.com，提供免费的基础功能和收费的企业级功能，适用于个人、小型团队到大型企业。
>
> 与 GitHub 相比，GitLab 在 **DevOps** 集成方面提供了更多功能，尤其在 **CI/CD** 和 **自动化管理** 上更为强大，适合需要全流程集成的团队

=>Github偏向于 云服务托管代码， GitLab偏向于给企业提供一个DevOps解决方案

不仅可以用yum直接安装在linux上，还可以利用docker安装GitLab：

> **快速部署**：Docker 提供了 GitLab 官方的镜像，使用 Docker 安装 GitLab 无需繁琐的手动配置和依赖管理。只需几个命令就可以启动 GitLab 实例，大大简化了安装过程。
>
> **预配置环境**：Docker 镜像已经包含了所有 GitLab 运行所需的组件（如数据库、Redis 等），无需额外配置。
>
> **跨平台兼容性**：Docker 可以在多种操作系统上运行，包括 Linux、Windows 和 macOS，因此不需要担心操作系统的兼容性问题。
>
> 总的来说，使用 Docker 部署 GitLab 不仅能减少安装、配置和维护的难度，还能提高灵活性和可扩展性，特别适合在 DevOps 或大规模开发环境中使用。

=>GItLab花钱的话可以直接CICD流程，而不用去额外用Jenkins



#### mTLS 20250428 

> **mTLS（Mutual TLS）** 是 TLS（传输层安全协议）的扩展形式，不仅服务器需要提供证书让客户端验证，**客户端也需要提供证书让服务器验证**。这是一种 **双向认证机制**，可以实现更高安全等级的通信。

=>举例来说想要本地git操作 Rosetta GitLab，需要实现安装RCLI (Rosetta Command Line Interface), 通过 RCLI 获取服务端能够信任的 mTLS 客户端证书，这些东西就是mTLS相对于TLS的区别之处。`client.crt`是由客户端递给服务端的 名片，服务端那边也应该有ca.crt来验证你这个名片OK不，然后用名片中的公钥来验证你发过来消息中你的签名即被私钥加密过的签名对不对，OK的话服务端就能够确认是 值得信任的 客户端发过来的了。 而传统的TLS就没有这一步骤，只有客户端 用 服务端的公钥 来验证 服务端发过来的被私钥 加密过的 签名。

> 客户端证书通常包含：
>
> - `client.crt`（客户端公钥证书）
> - `client.key`（客户端私钥）
> - `ca.crt`（服务端信任的根 CA，用于校验证书是否有效）

标准 TLS/mTLS 握手流程简要顺序：

> | 阶段 | 发生内容                                                     |
> | ---- | ------------------------------------------------------------ |
> | 1    | 客户端发送 ClientHello（包含支持的算法等）                   |
> | 2    | 服务端返回 ServerHello，附带自己的证书（server.crt）及公钥   |
> | 3    | 客户端验证服务端证书（用本地信任的CA证书）                   |
> | 4    | 服务端请求客户端证书（**mTLS 特有步骤**）                    |
> | 5    | 客户端发送客户端证书（client.crt）                           |
> | 6    | 服务端验证客户端证书                                         |
> | 7    | 双方基于各自私钥和对方公钥交换临时密钥，共同计算**对称密钥** |
> | 8    | TLS握手完成，开始用对称密钥加密通信                          |

> **服务端的证书（含公钥）是在ServerHello消息里由服务端主动发送给客户端的**，客户端用它来验证服务端身份。
>
> **客户端证书（含客户端公钥）只有在服务端请求时才会发送给服务端（mTLS场景）**。
>
> **CA证书（根证书）一般双方事先配置在本地的信任仓库里，不通过网络传递。**

=>在 TLS 握手初期，包括客户端发送的 `ClientHello` 和服务端返回的 `ServerHello` 都是明文传输的，因为双方尚未协商出加密密钥。

> 对称加密使用同一个密钥进行加解密，而非对称加密使用一对公钥和私钥，公钥加密、私钥解密。

=>客户端与服务端 先用 非对称加密 协商出一个 密钥， 然后再用 对称加密通信



### Jenkins安装 及简单配置 20241204

需要JAVA安装环境 Java8(JRE或者JDK都行)

在一个linux上安装好，指定端口，就可以通过浏览器登录Jenkins界面了

同一台服务器再装个Maven便于之后编译Java项目代码



在Jenkins界面 输入 Repository URL （记得服务器上提前安装git哈）

Java 用Maven，如果是C/C++项目还可以有 CMake的插件配置 来构建项目



安装一个 Publish over SSH插件, 添加一台目标服务器用来运行构建好的 项目

=> 连 Exec command 这样的也可以在 Jenkins界面 上直接填... 还有 Remote Directory



总结而言有这么几天服务器： 自己的local，gitlab服务器，Jenkins服务器自动编译好，发送到test server用来启动

最简陋的CICD就构建好了，但是怎么配置一push就运行呢？





### Pre Steps 20241206

DashBoard

General,源码管理，构建触发器，Pre Steps，Build,Post Steps,构建设置，构建后操作



Pre Steps部分 可以对 目标服务器（测试用）进行一些操作，通常就在目标服务器上写个脚本,把占用着的端口的进程kill掉一类的，由Jenkins进行调用（Send file or execute commands over SSH）

### 构建触发器  20241208

在Jenkins构建触发器下 创造一个 URL，然后gitlab可以通过这个URL来调用Jenkins来触发自动构建，相当于在Jenkins上手动按下构建按钮

（有可能要安装 Plugin Manager的插件来解决验证问题）



提交代码就自动构建过于频繁，由于对于大项目而言，一般是在GitLab界面上合并分支后 自动构建

 利用GitLab Webhooks去觉察到 合并分支 这一事件 然后回调 Jenkins提供的URL 进行实现

GitLab：设置->Webhook => 有许多出发来源，包括 push，merge等事件



结论是 一push就运行的话 Jenkins必须结合GitLab来完成



上面所说的这种构建方式 其实是 Jenkins 的 触发远程构建（就是jenkins本身只提供一个API出去），此外还有其他构建方式，比如跟其他项目相关联，比如 Build after other projexts are built, 定期构建(Build periodically)， 以及Jenkins主动去查看git地址有没有变化进行构建(Poll SCM) 等



定时执行任务：Jenkins有建立在基本的cron表达式上加了Hash值的专用表达式，cron表达式就是年月日时分

在Jenkins界面上(Build periodically)打勾，日程表中填入cron表达式



Poll SCM：也需要输入一个cron表达式 去探测 GitLab上探测代码是否有变化



最后还可以在 DashBoard->Configure System中配置邮箱接受构建通知，比如从你的一个163邮箱发送到你的QQ邮箱

### 容器化构建 20241209

三种方式：

1. 外挂目录
2. jar包直接打包到镜像里：启动一个虚拟机测试docker；去docker官方市场里直接找支持jdk的镜像，在虚拟机某一路径创造dockerfile；在dockerfile中加入 jar包路径 然后启动docker
3. 在2的基础上生成新镜像，推送到docker私服中，比如Harbor，然后再由k8s集群自动管理部署容器。大型项目基本采用这个模式，是主流。小型项目说实话也不太用1与2，直接就不用容器了哈哈

dockerfile可以作为代码一部分推送到git上

在Jenkins中的Pre steps与Post Steps中相应改成停止以及启动docker等的设置

### Jenkins集群并发构建

集群化构建可以有效提升构建效率，尤其是团队项目比较多或是子项目比较多的时候，可以并发在多台机器上执行构建

Jenkins服务器多启动两台

在第一台Jenkins找到 Manage nodes and cloud这个界面，增加其他Jenkins服务器的节点；然后到General中找到 并发构建 按钮

### 流水线 pipeline 20241210

Jenkins的流水线配置文件Jenkinsfile就像docker的dockerfile

Dashboard->mypipeline: General,构建触发器，高级项目选项，流水线

流水线配置文件 是一个由特殊语法结构的运行于 Jenkins服务器上的 脚本

举例：

```json
pipeline {
    agent any  // 定义流水线的执行环境，any表示可以在任何可用的代理上运行

    environment {
        // 定义环境变量
        CC = 'gcc'  // C 编译器，设置为 GCC
        BUILD_DIR = 'build'  // 存放构建产物的目录
    }

    stages {
        // 阶段：准备环境
        stage('Prepare') {
            steps {
                echo 'Preparing environment...'
                script {
                    // 清理旧的构建文件
                    sh 'rm -rf ${BUILD_DIR}'  // 删除旧的 build 目录
                    sh 'mkdir ${BUILD_DIR}'  // 创建新的 build 目录
                }
            }
        }

        // 阶段：构建
        stage('Build') {
            steps {
                echo 'Building the project...'
                script {
                    // 编译 C 项目
                    sh '${CC} -o ${BUILD_DIR}/my_program src/*.c'  // 使用 GCC 编译 C 源文件
                }
            }
        }

        // 阶段：测试
        stage('Test') {
            steps {
                echo 'Running tests...'
                script {
                    // 假设有一个测试可执行文件，运行测试
                    sh './${BUILD_DIR}/my_program --test'  // 运行测试命令
                }
            }
        }

        // 阶段：部署
        stage('Deploy') {
            steps {
                echo 'Deploying the application...'
                script {
                    // 假设部署是将可执行文件上传到远程服务器
                    sh 'scp ${BUILD_DIR}/my_program user@server:/path/to/deploy'  // 使用 SCP 上传程序
                }
            }
        }
    }

    post {
        success {
            echo 'The build and deploy process succeeded!'  // 成功时输出
        }
        failure {
            echo 'The build or deploy process failed.'  // 失败时输出
        }
    }
}
```

> **Jenkins Pipeline**：
>
> - **Jenkins Pipeline** 是 Jenkins 提供的一种自动化构建和部署的方式，旨在帮助开发团队定义、执行、自动化整个 CI/CD 流程。
> - 它使用 **Jenkinsfile** 来定义工作流，支持更复杂的任务，例如构建、测试、部署、集成等，并且支持阶段性控制、并行执行、条件执行等特性。
>
> **一般的构建**：
>
> - **一般的构建** 通常是通过简单的 **Jenkins 构建任务** 来实现的。它通常由一系列预定义的步骤组成，直接在 Jenkins UI 中配置，不一定依赖于脚本或代码文件（例如 `Jenkinsfile`）。
> - 这种方式适用于简单的构建流程，通常用于没有复杂流程控制的场景。

=>Pipeline 在 任务并行，可视化(与Blue Ocean UI结合)，版本控制等方面更有优势

脚本可以由UI配置信息自动生成。 还有声明式流水线与脚本式流水线的区别，后者用 Groovy 脚本来编写，结构上可以更自由

Git上有多分支，可以为多个分支上传不同的 Jenkinsfile，达成可以为多分支并行构建的效果 

## Github Action 20241231

CI中还可以有 代码风格 check

在代码里 .github/workflows/文件夹下创建一个 yml文件就行

```yaml
#.github/workflows/ci.yml
name: C Project CI

on:
  # 当推送或创建 PR 到 main 分支时触发工作流
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build-and-check:
    runs-on: ubuntu-latest # 使用 Ubuntu 最新版本作为运行环境

    steps:
      # 步骤 1: 检出代码仓库
      - name: Checkout code
        uses: actions/checkout@v3

      # 步骤 2: 安装必要工具，包括构建工具和 MISRA 检查工具 (Cppcheck)
      - name: Install build tools and MISRA check tool
        run: sudo apt-get update && sudo apt-get install -y build-essential cppcheck

      # 步骤 3: 编译项目
      - name: Build project
        run: make

      # 步骤 4: 运行测试
      - name: Run tests
        run: ./build/project

      # 步骤 5: 执行 MISRA 检查
      - name: Run MISRA check
        run: cppcheck --enable=all --std=c99 --suppress=missingIncludeSystem --error-exitcode=1 src/ > cppcheck-report.txt

      # 步骤 6: 上传 MISRA 检查报告
      - name: Upload MISRA report
        uses: actions/upload-artifact@v3
        with:
          name: cppcheck-report
          path: cppcheck-report.txt
```



**GitHub Actions** 提供自己的 **云服务器**（也称为 **runner**）来运行工作流任务。

(如果需要更高的灵活性，可以设置自托管服务器（如企业内部服务器或专用云实例）)

## GitLab CICD 20250120

> ## **1. GitLab CI/CD 的核心概念**
>
> ### **1.1. GitLab Runner**
>
> GitLab CI/CD 依赖 **GitLab Runner** 来执行 CI/CD 任务。Runner 是一个独立的进程，可以运行在本地机器、服务器或 Kubernetes 集群上，支持多种执行环境（Docker、Shell、Kubernetes 等）。
>
> ### **1.2. `.gitlab-ci.yml` 配置文件**
>
> 所有 CI/CD 流程都由 `.gitlab-ci.yml` 文件控制，存放在项目根目录。它用于定义 **pipeline**（流水线）、**stages**（阶段）、**jobs**（作业）等内容。
>
> **示例：**
>
> ```yaml
> yamlCopyEditstages:
>   - build
>   - test
>   - deploy
> 
> build-job:
>   stage: build
>   script:
>     - echo "Building project..."
>     - make build
> 
> test-job:
>   stage: test
>   script:
>     - echo "Running tests..."
>     - make test
> 
> deploy-job:
>   stage: deploy
>   script:
>     - echo "Deploying..."
>     - make deploy
>   only:
>     - main
> ```
>
> - **stages**：定义流水线的不同阶段（如 `build`、`test`、`deploy`）。
> - **jobs**：每个 `job` 代表一个任务，属于某个 `stage`，可以执行 shell 命令。
> - **only**：指定在哪些分支触发此任务（如 `only: - main` 表示仅在 `main` 分支触发）。
>
> ------
>
> ## **2. GitLab CI/CD 工作流程**
>
> GitLab CI/CD 通过以下流程来自动化开发和部署：
>
> 1. **开发人员提交代码** 到 GitLab 仓库。
> 2. **GitLab 自动触发 Pipeline**，读取 `.gitlab-ci.yml` 进行构建、测试和部署。
> 3. **GitLab Runner 运行 CI/CD 作业**，执行 `build`、`test`、`deploy` 等任务。
> 4. **测试通过后**，可以自动或手动部署到测试/生产环境。
> 5. **GitLab 提供 CI/CD 可视化界面**，可以查看 Pipeline 运行状态、日志等。

 =>在GitLab界面中点击setting就可以看到Runner栏了，可以设置一个Runner，并为其设定target server的IP，所以界面中的Runner并不是正真的运行程序，只是一个指向性的设置，最终要去target server里安装这个runner才能运行

> ## **1. 服务器上的基本配置**
>
> 假设你要在服务器 `192.168.1.100` 上运行 GitLab Runner，需要进行以下设置：
>
> ### **1.1. 安装 GitLab Runner**
>
> 如果该服务器尚未安装 GitLab Runner，需要先安装：
>
> ```
> bashCopyEdit# 添加 GitLab Runner 仓库
> curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash
> 
> # 安装 GitLab Runner
> sudo apt-get install gitlab-runner -y   # Ubuntu/Debian
> # sudo yum install gitlab-runner -y     # CentOS/RHEL
> ```

=>配置release/gitlab-runner/config/config.toml，比如

```toml
[[runners]]
  name = "docker-runner"  # Runner 的名称，在 GitLab CI/CD 界面中可见
  url = "https://gitlab.example.com/"  # GitLab 服务器地址
  token = "your-runner-token"  # 该 Runner 连接到 GitLab 所需的 Token
  executor = "docker"  # 设定 Runner 以 Docker 模式执行 CI/CD 任务

  # 自定义构建目录（默认情况下，Runner 在 `/builds/` 目录执行 CI 任务）
  [runners.custom_build_dir]
  # 如果启用 `custom_build_dir`, 可以指定不同的路径来存放 CI/CD 任务的构建目录
  # 例如:
  # enabled = true
  # build_dir = "/my/custom/path"
  # 但一般情况下保持默认即可，不需要修改
  

  [runners.docker]
    tls_verify = false  # 是否开启 TLS 验证（一般用于私有 Docker Registry，默认 false）
    image = "python:3.9"  # 运行 CI/CD 任务时默认使用的 Docker 镜像
    privileged = true  # 允许 `privileged` 模式（若需要在 CI/CD 任务中运行 Docker in Docker，必须设为 true）
    disable_entrypoint_overwrite = false  # 允许覆盖 Docker 镜像的 Entrypoint，通常保持默认即可
    oom_kill_disable = false  # 是否禁用 OOM（内存不足）自动终止，默认 false，建议不修改
    disable_cache = false  # 是否禁用构建缓存，默认 false（开启缓存能加速任务）
    
    # 挂载卷（volumes）
    volumes = [
      "/cache",  # GitLab Runner 的缓存目录，保存构建中间结果，加快后续任务
      "/var/run/docker.sock:/var/run/docker.sock"  # 允许 Runner 访问宿主机 Docker（用于 `docker build`）
    ]
    
    shm_size = "256m"  # 共享内存大小，避免某些程序因共享内存不足崩溃（默认 64MB，建议设为至少 256MB）

  [runners.cache]  # 缓存配置
    [runners.cache.s3]  # S3 远程缓存（如果使用 AWS S3 作为构建缓存，可以在这里配置）
    [runners.cache.gcs]  # Google Cloud Storage 缓存（如果使用 GCS 存储 CI 任务的缓存，可以配置此项）

```

=>上面这个token就是要去GitLab界面注册的Runner里去寻找，这样的话两者就联系起来了

# Kubernetes

## 尚硅谷Kubernetes教程

[Link](https://www.bilibili.com/video/BV1w4411y7Go/?p=2&spm_id_from=pageDriver&vd_source=6fc477a8e79179a3fd30bed2e2ba5fbe)

2015年Google用Go语言开发了Kubernetes框架

Kubernetes属于PaaS

轻量级，开源，弹性收缩，负载均衡

Pod是Kubernetes管理上的最小单位，管理一定数量的Docker



### 知识图谱 20240920

1. 介绍说明：Kubernetes的前生今世
2. 基础概念：什么是Pod 控制器类型 网络通讯模式
3. 构建K8S集群
4. 资源清单： 资源清单的语法 编写Pod 掌握Pod的生命周期(**important**)
5. Pod控制器：掌握各种控制器的特点以及使用方式定义
6. Service原理：给顾客一个访问接口，掌握Service原理及其构架方式
7. 存储：掌握多种存储类型的特点
8. 调度器：能够根据要求把Pod定义到想要的节点运行
9. 集群安全机制：集群的认证，鉴权，访问控制 原理及其流程
10. HELM：类似Linux中yum，只是不安装包而是某个功能的集群
11. 运维：修改Kuberadm 达到证书可用期限为10年，构建高可用Kubernetes

### 组件说明 20241102

前身 Borg ： paxos是分布式一致性算法

K8S架构：

master 节点：scheduler,replication controller,api server,kubectl,etcd =>这些模块全部在同一个物理服务器上

kubectl： 命令行

api server： master 节点的中心组件，所有服务访问的统一入口

replication controller: 维持副本期望数目

scheduler:负责给Docker介绍任务，选择合适的节点进行分配任务

etcd ： 分布式键值存储数据库，为整个集群存储一些关键数据(持久化)。可以借助etcd进行数据恢复，采用HTTP协议



node 节点：kubelet， kube proxy, node(可以看成Docker(最主流) )

Kubelet：直接跟容器引擎交互实现容器的生命周期管理

Kube proxy: 负责写入规则至 IPTABLES, IPVS实现服务映射管理



其他插件：CoreDNS，Dashboard(给K8S集群提供B/S架构)...

### Pod概念

在同一个Pod中 许多容器 共享网络与存储卷，互相之间无需配置IP直接通信

> 在 Kubernetes（K8s）中，Pod 是一个基本的调度单位，用于部署和管理容器化应用。Pod 的概念可以理解为一组共享资源和网络的容器。以下是 Pod 的一些关键特点和概念：
>
> ### 1. **单个或多个容器**
>
> - 一个 Pod 可以包含一个或多个容器，这些容器共享相同的网络命名空间和存储卷。它们通常一起工作，以实现特定的功能。
>
> ### 2. **共享网络**
>
> - 所有在同一 Pod 中的容器共享一个 IP 地址和端口空间。这意味着它们可以通过 `localhost` 直接互相通信，而不需要通过网络进行路由。
>
> ### 3. **存储卷**
>
> - Pod 可以挂载一个或多个存储卷，以便容器可以共享数据。这使得数据在容器重启或替换时得以持久化。
>
> ### 4. **生命周期管理**
>
> - Kubernetes 管理 Pod 的生命周期，包括创建、调度、健康检查、重启等。Kubernetes 会根据配置自动处理 Pod 的状态。
>
> ### 5. **负载均衡**
>
> - 在使用服务（Service）时，Kubernetes 可以自动为 Pod 提供负载均衡，使流量均匀分配到多个 Pod 实例上。
>
> ### 6. **调度**
>
> - Kubernetes 的调度器负责将 Pod 分配到合适的节点上，以确保资源的高效利用和服务的可用性。
>
> ### 7. **标签和选择器**
>
> - Pod 可以使用标签（Labels）来标识和组织。通过选择器（Selectors），可以方便地管理和选择特定的 Pod 进行操作。
>
> ### 8. **用途**
>
> - Pod 通常用于运行微服务、任务队列、数据处理作业等。它们是构建应用程序的基本单元，允许将多个相关容器组合在一起。
>
> ### 总结
>
> Pod 是 Kubernetes 的核心概念之一，提供了一种灵活的方式来管理和协调多个容器的运行。理解 Pod 的工作原理有助于更好地利用 Kubernetes 来部署和管理容器化应用。

=> 每个Pod 一个IP，Pod内部的容器通信通过 localhost+端口通信

> 假设一个 Pod 内有两个容器：`web` 和 `app`，`web` 容器需要调用 `app` 容器提供的服务。可以通过以下方式进行通信：
>
> 1. **在 `app` 容器中启动一个服务**，监听某个端口（例如 8080）。
> 2. **在 `web` 容器中通过 `localhost:8080`** 发送请求：
>
> ```shell
> #localhost 就是 127.0.0.1
> curl http://localhost:8080
> ```

### 集群安装

> ### **集群（Cluster）**
>
> - **定义**：Kubernetes 集群是由一组计算资源（节点）组成的集合，这些节点可以是物理服务器或虚拟机。集群运行 Kubernetes 控制平面和应用程序的工作负载（即 Pod）。
>
> - 组成
>
>   ：
>
>   - **控制平面**：负责管理集群状态，调度 Pod，处理 API 请求等。主要组件包括 API Server、etcd、Controller Manager 和 Scheduler。
>   - **工作节点（Worker Nodes）**：运行实际的容器化应用（Pod）。每个节点都包含运行容器的必要组件，如 kubelet、kube-proxy 和容器运行时（如 Docker 或 containerd）。

=>Cluster分为控制平面和工作负载，后者的主体就是Pod

k8s集群举例: master01 node01 node02 Router Harbour

用VMWare创建5个虚拟机

在master01里进行如下操作：

1. 首先指定主机名，直接改hosts文件就行，安装依赖包
2. 设置防火墙为Iptables并设置空规则，关闭SE Linux，防止其影响k8s的运行效率
3. 调整内核参数，对于K8S. 关闭系统不需要的服务，配置日志服务
4. kube-proxy 开启ipvs的前置条件
5. 安装Docker 软件
6. 安装kubeadm （`kubeadm` 是 Kubernetes 提供的一个工具，用于简化 Kubernetes 集群的安装和管理。）
7. 初始化主节点, 加入工作节点，部署网络

再构建私有仓库Harbour

> Harbor 提供了一个集中式的地方来存储和管理 Docker 容器镜像，使得在 Kubernetes 中部署和更新应用时可以更加方便。持对镜像进行版本管理，可以轻松回滚到之前的版本，提高应用的可维护性。可以方便地与 CI/CD 工具链（如 Jenkins、GitLab CI 等）集成，实现持续集成和持续部署的自动化流程。Harbor 提供了一个用户友好的 Web 界面.

### 资源清单(manifest) 20241106

> Kubernetes（简称K8s）的资源清单（manifest）是用于定义和描述Kubernetes集群中资源对象的配置文件，通常是一个或多个YAML或JSON文件。这些资源对象可以包括Pod、Deployment、Service、ConfigMap、Secret等，几乎所有在K8s中运行的应用和服务都需要在资源清单中进行描述。

#### YAML的基本语法

YAML采用缩进表示层级关系，**通常使用空格**（而不是Tab）进行缩进。每一级缩进通常为两个空格。

支持的数据结构：对象(键值对)，数组，纯量(scalars)->不可再分割的量

> ### YAML的基本语法
>
> YAML的核心原则是简单、直观和易于理解，下面是一些基本语法规则和示例：
>
> #### 1. **基本结构**
>
> YAML采用缩进表示层级关系，通常使用空格（而不是Tab）进行缩进。每一级缩进通常为两个空格。
>
> ```yaml
> key: value
> ```
>
> 在YAML中，数据由键（key）和值（value）组成。键值之间使用冒号`:`分隔。
>
> #### 2. **键值对**
>
> 最简单的形式是键值对，每个键后跟一个冒号和一个值：
>
> ```yaml
> codename: Alice
> age: 30
> ```
>
> #### 3. **多行字符串**
>
> YAML允许用`|`和`>`来表示多行字符串，`|`保持换行，而`>`将换行符转换为空格。
>
> ```yaml
> address: |
>   123 Main St
>   Apt 4B
>   Springfield, IL
> ```
>
> 或者，使用`>`来把换行合并为单个空格：
>
> ```yaml
> description: >
>   This is a very long description
>   that will be wrapped into a single line.
> ```
>
> #### 4. **数组**
>
> YAML中的数组（列表）通过在每个元素前面加上破折号（`-`）表示：
>
> ```yaml
> fruits:
>   - Apple
>   - Banana
>   - Orange
> ```
>
> #### 5. **字典（Map）**
>
> 字典是由多个键值对组成的，**键值对之间用换行分隔**。字典嵌套也是常见的用法。
>
> ```yaml
> person:
>   name: Alice
>   age: 30
>   address:
>     street: 123 Main St
>     city: Springfield
>     zip: 62701
> ```
>
> #### 6. **引用**
>
> YAML支持引用和锚点。通过`&`创建一个锚点，再通过`*`引用它。
>
> ```yaml
> defaults: &defaults
>   name: Alice
>   age: 30
> 
> user1:
>   <<: *defaults
>   city: New York
> 
> user2:
>   <<: *defaults
>   city: Chicago
> ```
>
> 在上面的例子中，`user1`和`user2`引用了`defaults`中的数据，但可以覆盖或添加新的键值对。
>
> #### 7. **注释**
>
> YAML中的注释使用 `#` 符号。注释从 `#` 开始，直到行末为止：
>
> ```yaml
> # This is a comment
> name: Alice  # This is also a comment
> ```

> ### YAML与JSON的对比
>
> YAML与JSON非常相似，都可以用来表示结构化数据。YAML相较于JSON的主要优点是：
>
> - **更简洁**：YAML去除了JSON中的花括号、引号、逗号等符号，使得格式更加简洁。
> - **可读性更强**：YAML更接近自然语言，便于手动编辑和阅读。
> - **不需要引号**：YAML中，字符串值通常不需要用引号包裹，除非字符串本身包含特殊字符。
>
> 然而，JSON更严格，通常用于数据传输和API中，而YAML更适合配置文件、日志等场景。





一个简单的K8s Deployment资源清单

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-simple-pod
spec:
  containers:
  - name: my-container
    image: nginx:latest
    ports:
    - containerPort: 80
```

在master主机中可以进行部署

```shell
kubectl apply -f my-simple-pod.yaml
```



**Pod生命周期**中在 Main Container启动之前 有时会预先启动一些 init Container去检验运行环境以及配置一些文件

> 在 Kubernetes 中，**探针（Probe）** 是一种用于监控和管理容器健康状态的机制。探针通过周期性地检查容器的健康状况（如是否正在正常运行），帮助 Kubernetes 确定容器是否健康、是否就绪，以及是否需要重启。
>
> Kubernetes 中有三种类型的探针，分别是：
>
> 1. **Liveness Probe（存活探针）**
> 2. **Readiness Probe（就绪探针）**
> 3. **Startup Probe（启动探针）**

由kubelet去调用执行Probe

 Kubernetes Pod 示例，其中包括了 **Init Containers**、主容器、**探针（Probes）** 以及启动和退出的逻辑：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app-pod
spec:
  initContainers:
    - name: init-db
      image: busybox
      command: ['sh', '-c', 'echo "Initializing database..."; sleep 10']
      volumeMounts:
        - name: app-data
          mountPath: /data

  containers:
    - name: web-app
      image: my-web-app:latest
      ports:
        - containerPort: 80
      volumeMounts:
        - name: app-data
          mountPath: /app/data
      livenessProbe:
        httpGet:
          path: /healthz
          port: 80
        initialDelaySeconds: 10
        periodSeconds: 5
        failureThreshold: 3
        timeoutSeconds: 2
      readinessProbe:
        httpGet:
          path: /readiness
          port: 80
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 3
        timeoutSeconds: 2
      startupProbe:
        httpGet:
          path: /start
          port: 80
        initialDelaySeconds: 15
        periodSeconds: 5
        failureThreshold: 10

  volumes:
    - name: app-data
      emptyDir: {}
  
  restartPolicy: Always
```

> 通常情况下，你可能会用 `kind: Pod` 来定义单个容器的工作负载，例如：
>
> - 用于快速部署或测试容器化应用。
> - 用于运行不需要扩缩容的单一应用。
>
> 然而，Kubernetes 中更多的场景是使用 **控制器**（如 **Deployment**、**StatefulSet**、**DaemonSet** 等）来管理 Pod。控制器提供了更多的功能，比如自动扩展、滚动更新等。因此，虽然你可以直接使用 `Pod` 来运行应用，但在生产环境中，通常推荐使用 Deployment 或其他控制器来管理 Pod。

=>即不常用 kind ：Pod， 而是类似 kind:Deployment

### Pod 控制器 20241107

> **Pod 控制器**（Pod Controller）是 Kubernetes 中一种用于管理和控制 **Pod** 生命周期的控制器。
>
> ### 常见的 Pod 控制器类型
>
> 1. **ReplicaSet**
>    - **功能**：确保指定数量的 Pod 副本始终在集群中运行。如果某个 Pod 出现故障或被删除，ReplicaSet 会自动创建新的 Pod 以保持副本数。
>    - **应用场景**：用于保证某一应用的 Pod 数量和健康状态一致。
> 2. **Deployment**
>    - **功能**：Deployment 是 ReplicaSet 的高级抽象，它不仅管理 Pod 副本，还提供了滚动更新、回滚等功能。它在自动化部署、版本控制和系统更新中非常有用。
>    - **应用场景**：用于管理无状态的应用，提供版本控制和可扩展性。
> 3. **StatefulSet**
>    - **功能**：用于管理有状态的应用。它确保每个 Pod 都有一个稳定的标识符，并且在更新时保证 Pod 的顺序性和稳定性。StatefulSet 还确保应用中的每个 Pod 在重新调度时会保留持久化存储。
>    - **应用场景**：适用于有状态服务，比如数据库、缓存、队列等。
> 4. **DaemonSet**
>    - **功能**：DaemonSet 确保在每个节点上都运行一个 Pod 副本。如果添加了新节点，DaemonSet 会自动在新节点上启动一个 Pod 副本。
>    - **应用场景**：用于节点级别的服务，如日志收集、监控代理、网络插件等。
> 5. **Job**
>    - **功能**：Job 控制器负责一次性任务（如批处理任务）的管理。它确保任务完成指定次数，且所有 Pod 必须成功运行完任务。
>    - **应用场景**：用于处理定时任务、批处理、数据迁移等任务。
> 6. **CronJob**
>    - **功能**：CronJob 是 Job 控制器的扩展，允许用户按照指定的时间计划周期性地运行作业。它类似于 Linux 中的 cron 作业。
>    - **应用场景**：用于定期执行任务，如定时备份、定时数据同步等。

命令式编程 与 声明式编程

> **声明式编程**（Declarative Programming）是一种编程范式，其中程序员描述期望的“**做什么**”而不是“**怎么做**”。与命令式编程（Imperative Programming）不同，声明式编程专注于表达计算的结果或目标，而不是描述如何一步一步实现这个目标的具体过程。
>
> **配置语言（如 JSON、YAML、Terraform 等）**：
>
> - 配置文件通常用于声明性编程，描述希望系统达到的目标状态，而不涉及如何一步步实现。例如，使用 `YAML` 或 `JSON` 定义 Kubernetes 的 Pod、服务或虚拟机的配置，Kubernetes 系统会负责根据这些配置管理资源。

**大多数情况下**，**一个容器只有一个主进程**，并且该容器的生命周期和该主进程是紧密相关的。Docker 强调 **一个进程一个容器** 的理念，推荐将应用拆分成多个容器，每个容器运行一个单独的进程或服务。

=>所以所谓的Pod控制器，比如DaemonSet，可以跟单个的守护进程联系起来，即 Pod控制器=>进程属性

=>部署完Pod之后，控制器设置为Deployment, 还可以在master中进行扩容，回滚，更新镜像等命令行操作

Job 控制器示例：

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  completions: 1          # 任务成功执行的次数，默认值是 1
  parallelism: 1          # 同时运行的 Pod 数量，默认值是 1
  backoffLimit: 4         # 最大重试次数，失败后重试的最大次数
  activeDeadlineSeconds: 3600  # Job 运行的最大时间（秒），超过此时间任务将被终止
  template:
    spec:
      containers:
      - name: example-container
        image: busybox:1.35
        command: ["sh", "-c", "echo Hello World; sleep 30"]  # 执行的任务
      restartPolicy: Never  # 表示 Job 中的 Pod 失败后不会重启
```

=>上面就是启动一个Pod运行 Hello World，在master里再打一个 kubectl log xxx就能从Pod那边得到结果

### Service 20241108

> 在 **Kubernetes** 中，`Service` 是一种抽象的资源，它定义了一种访问和暴露一组 **Pod** 的方法。Kubernetes 的 `Service` 使得客户端可以访问后端的 Pod，而无需关心 Pod 的动态变化和 IP 地址的改变。
>
> ### `Service` 的作用
>
> - **负载均衡**：一个 `Service` 可以通过负载均衡的方式将请求分发到一组 Pod 上。即使 Pod 的 IP 地址变化，`Service` 也会保持对这些 Pod 的访问能力。
> - **抽象与稳定性**：`Service` 提供了一个稳定的访问入口。Pod 可能会被替换、重新调度或销毁，但 `Service` 的访问地址（即 DNS 名称或 IP）是固定的，不会改变。
> - **简化服务发现**：通过 `Service`，可以很容易地进行服务发现和访问 Kubernetes 集群内部或外部的服务。
>
> ### `Service` 的类型
>
> 在 Kubernetes 中，`Service` 有不同的类型，每种类型定义了不同的暴露方式。常见的 `Service` 类型包括：
>
> 1. **ClusterIP**（默认类型）
>    `ClusterIP` 类型的 `Service` 只在 Kubernetes 集群内部可访问。它为 Pod 提供了一个集群内部的 IP 地址，并通过该 IP 地址进行负载均衡。
> 2. **NodePort**
>    `NodePort` 类型的 `Service` 会将服务暴露到每个 Node 上的一个端口上。通过访问 `NodeIP:NodePort`，可以从外部访问该服务。
> 3. **LoadBalancer**
>    `LoadBalancer` 类型的 `Service` 会为服务创建一个外部负载均衡器（通常是云提供商的负载均衡服务，如 AWS ELB 或 GCP 的负载均衡器）。它允许外部流量通过负载均衡器访问 Kubernetes 集群中的服务。
> 4. **ExternalName**
>    `ExternalName` 类型的 `Service` 不会直接将请求路由到 Kubernetes 集群内部的 Pod。相反，它会将请求重定向到外部 DNS 名称。适用于将 Kubernetes 内部服务与外部服务进行集成的场景。

=>Service不是一个具体的服务器，它是通过apiserver 与 Kube-proxy 与Client所访问的iptables的 交互 而组成的 一套机制。 Service是 OSI第四层的 服务，有Kube-proxy 提供虚拟IP，据说用Ingress可以提供第7层的HTTP服务

> 举例实现 **ClusterIP** 的过程中，`apiserver`、`kube-proxy` 和 `iptables`（或 `IPVS`）起到了关键作用
>
> **API Server (`apiserver`)**：Kubernetes API Server 是集群的核心组件之一，负责接收并处理所有集群的 REST 请求，并与其它 Kubernetes 组件进行通信。
>
> **kube-proxy**：每个 Kubernetes 节点上都会运行 `kube-proxy`，它负责在节点上设置网络路由规则，确保来自客户端（例如，Pod、Service）请求的流量能够正确地转发到目标 Pod。
>
> **iptables**：`iptables` 是 Linux 系统中用于管理网络流量的工具，Kubernetes 使用它来创建规则，实现流量转发和负载均衡。

下面是一个简单的 `NodePort` 服务的 YAML 示例：

```
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80        # 集群内的服务端口
      targetPort: 8080 # 后端 Pod 上的端口（容器实际监听的端口）
      nodePort: 30007  # 暴露给外部客户端访问的端口
  type: NodePort
```

> Ingress 主要是处理 HTTP 和 HTTPS 流量的路由和负载均衡。它通过定义一些路由规则，将外部请求转发到集群内部的服务。Ingress 充当了一个**反向代理**，通过它你可以设置 URL 路由、SSL 终止、重定向等功能。
>
> Ingress 是由一个控制器（**Ingress Controller**）来实现的。比如**NGINX Ingress Controller**(基于 NGINX 的反向代理和负载均衡器实现)
>
> **Ingress Controller** 是一个运行在 Kubernetes 集群中的 **进程**，而不是一个独立的服务器。它负责实现和管理 **Ingress 资源** 中定义的路由规则，并将外部 HTTP/HTTPS 流量路由到集群内部的服务。

=>同样有配置Ingress 的YAML

### 存储 20241111

> 在 Kubernetes 中，**容器本身没有持久化存储**。容器是临时的、可替换的环境，一旦容器停止或被删除，容器中的所有数据都会丢失。因此，容器中的数据一般是**短暂的**，通常我们需要依赖外部存储来持久化数据。

configMap：配置文件

> `ConfigMap` 是 Kubernetes 用于存储配置信息的资源对象，它允许你存储一些不包含敏感数据的配置文件，如 JSON、YAML 或普通的文本配置文件。
>
> ```shell
> #命令行创建
> kubectl create configmap my-config --from-file=path/to/your/config/file
> ```
>
> 如果你的应用程序的配置本身就是 YAML 或 JSON 格式，你可以直接将这些格式的内容存储在 `ConfigMap` 中:
>
> ```yaml
> apiVersion: v1
> kind: ConfigMap
> metadata:
>   name: config-with-files
> data:
>   config.json: |
>     {
>       "database": "postgresql",
>       "host": "localhost",
>       "port": 5432
>     }
>   app.properties: |
>     server.port=8080
>     database.name=mydb
> ```
>
> 接着将 ConfigMap 挂载到 Pod上：
>
> ```yaml
> apiVersion: v1
> kind: Pod
> metadata:
>   name: example-pod
> spec:
>   containers:
>   - name: example-container
>     image: nginx
>     volumeMounts:
>     - name: config-volume
>       mountPath: /etc/config
>       readOnly: true
>   volumes:
>   - name: config-volume
>     configMap:
>       name: my-config
> ```

> `ConfigMap` 数据并不存储在某个单独的物理服务器上，而是存在于 Kubernetes **控制平面（Control Plane）** 的 **etcd** 数据库中。



Secret：`Secret` 和 `ConfigMap` 都是用于存储配置信息的资源对象，它们的主要区别在于存储的数据类型和用途。Secret存储敏感数据，如密码、API 密钥、令牌等。数据在 etcd 中存储时通常是经过 base64 编码的，且可以与外部加密工具（如 Vault）配合使用，以增强安全性。



Volume: 用于持久化数据和容器之间共享数据的机制。容器本身是短暂的，重启或重新调度时，容器内的数据可能会丢失。因此，Kubernetes 引入了 `Volume` 机制，以确保数据在容器生命周期之外依然存在，且可以在多个容器之间共享。

=>种类很多啊，有PersistentVolume (PV) 和 PersistentVolumeClaim (PVC)，有NFS (Network File System)，还有AWS EBS (Elastic Block Store)，Azure Disk...

> Kubernetes 中的 `Volume` 是容器持久化数据和共享数据的重要机制。它为容器化应用提供了多种存储选项，包括本地存储、云存储和分布式存储等。通过不同类型的 `Volume`，用户可以根据需求选择适合的存储方案。
>
> 常见的使用场景包括：
>
> - 临时存储：使用 `emptyDir`。
> - 持久化存储：使用 `PersistentVolume` 和 `PersistentVolumeClaim`。
> - 跨节点共享数据：使用 `NFS` 或 `GlusterFS`。
> - 云存储：使用 `AWS EBS`、`Azure Disk` 等。



PersistentVolume（PV）: PV 是集群中的存储资源，它可以是一个云存储卷（如 AWS EBS、Azure Disk）或网络存储卷（如 NFS、GlusterFS）。PV 是一个持久化的资源，生命周期独立于 Pod。它不会随着 Pod 的删除而删除，除非设置了回收策略。

PersistentVolumeClaim（PVC）:`PersistentVolumeClaim` 是对存储资源的请求，它由用户或应用程序创建。PVC 请求存储资源的大小、访问模式等属性，Kubernetes 会根据 PVC 的需求找到一个合适的 PV。PVC 的生命周期与 Pod 绑定，PVC 可以在 Pod 的声明中指定，Kubernetes 会自动将一个合适的 PV 绑定到 PVC 上。

> ### **PV 和 PVC 的工作原理**
>
> 1. **创建 PV**： 管理员创建 `PersistentVolume`，定义具体的存储后端（例如，本地存储、网络存储、云存储），并设置其大小、访问模式等属性。
> 2. **创建 PVC**： 用户创建 `PersistentVolumeClaim`，并声明所需的存储大小和访问模式（例如 `ReadWriteOnce` 或 `ReadWriteMany`）。PVC 会根据这些需求请求合适的 PV。
> 3. **PV 和 PVC 绑定**：
>    - Kubernetes 会根据 PVC 的请求找到一个符合条件的 PV，并将其与 PVC 绑定。
>    - 一旦 PV 和 PVC 被绑定，PVC 的生命周期就与绑定的 PV 绑定，PVC 的生命周期将在 Pod 被删除时结束。
> 4. **访问存储**：
>    - 绑定后，用户可以在 Pod 中通过 `PVC` 来访问 PV。Kubernetes 会自动把 PVC 挂载到容器的文件系统中，容器可以通过 `PVC` 使用对应的存储。

=>设计出这两个概念就是为了Pod与具体的存储 解耦啊，PV与PVC也是用YAML格式来定义，并且联系的，两者一对一映射, PVC 和 PV 根据一些原则自动绑定.

> ### **常见的 PV 和 PVC 使用场景**
>
> 1. **Cloud Storage**：例如使用 `AWS EBS` 或 `Azure Disk`，为每个 Pod 提供独立的存储。
> 2. **Network Storage**：例如使用 `NFS` 或 `CephFS`，为多个 Pod 提供共享的存储。
> 3. **Local Storage**：使用节点本地存储，例如 `hostPath` 或 `local` 卷，用于需要存储在特定节点上的数据。
> 4. **Stateful Applications**：像数据库（MySQL、PostgreSQL）这类有状态应用，通常需要持久化存储，可以使用 PVC 绑定到 PV。

> ### **完整示例**
>
> 假设我们已经有了 EBS 卷，以下是创建 EBS 存储卷并将其挂载到 Pod 的完整流程。
>
> 1.**创建 PersistentVolume（PV）**：
>
> ```yaml
> apiVersion: v1
> kind: PersistentVolume
> metadata:
>   name: ebs-pv
> spec:
>   capacity:
>     storage: 10Gi   # 需要与 EBS 卷大小匹配
>   accessModes:
>     - ReadWriteOnce   # EBS 卷通常仅能在单一节点上以 ReadWriteOnce 模式挂载
>   persistentVolumeReclaimPolicy: Retain   # PV 保留策略，当 PVC 删除时保留该 PV
>   awsElasticBlockStore:
>     volumeID: vol-xxxxxxxx   # 替换为实际的 EBS 卷 ID
>     fsType: ext4   # 文件系统类型，通常为 ext4 或 xfs
> ```
>
> 2.**创建 PersistentVolumeClaim（PVC）**：
>
> ```yaml
> apiVersion: v1
> kind: PersistentVolumeClaim
> metadata:
>   name: ebs-pvc
> spec:
>   accessModes:
>     - ReadWriteOnce  # 要求单一节点可以读写
>   resources:
>     requests:
>       storage: 10Gi  # 需要与 PV 的大小一致
> ```
>
> 3.创建 Pod，并挂载 PVC:
>
> ```yaml
> apiVersion: v1
> kind: Pod
> metadata:
>   name: ebs-pod
> spec:
>   containers:
>   - name: nginx-container
>     image: nginx
>     volumeMounts:
>     - name: ebs-storage
>       mountPath: /data  # 将 EBS 卷挂载到容器内的 /data 路径
>   volumes:
>   - name: ebs-storage
>     persistentVolumeClaim:
>       claimName: ebs-pvc  # PVC 的名称
> ```

### 集群调度 20241112

> 在 Kubernetes 中，集群调度（**Kubernetes Scheduling**）是指如何将一个 Pod 安排到集群中的节点上运行的过程。调度过程不仅要考虑资源的可用性，还要考虑节点的健康状态、拓扑结构、策略要求、调度约束等因素。Kubernetes 的调度器（**kube-scheduler**）负责根据一系列规则（如资源需求、节点亲和性、污点和容忍度等）决定将 Pod 部署到哪个节点上。
>
> ### **Kubernetes 调度过程概述**
>
> 1. **Pod 提交到 API 服务器**： 当用户创建 Pod 时，Pod 对象会被提交到 Kubernetes API 服务器。此时，Pod 并未立即运行，而是处于 `Pending` 状态。
> 2. **调度器调度 Pod**： Kubernetes 的调度器（`kube-scheduler`）会监控 API 服务器的 Pod 调度队列，查看哪些 Pod 处于 `Pending` 状态，且尚未绑定到节点。调度器将根据调度策略选择一个适合的节点来运行 Pod。
> 3. **节点选择**： 调度器会根据 Pod 的资源需求、节点的可用资源、节点亲和性、Pod 的亲和性、污点与容忍度等因素来选择合适的节点。
> 4. **绑定 Pod 和节点**： 一旦调度器选择了一个节点，它会将 Pod 绑定到该节点。绑定过程会更新 Pod 对象，使其与特定节点关联，Pod 将被分配到该节点并开始运行。
>
> ### **调度过程详细步骤**
>
> #### 1. **预选阶段（Predicates）**
>
> 在这个阶段，调度器会检查哪些节点符合 Pod 的基本需求。每个节点都会被检查以确定它是否能满足 Pod 的资源需求和其他约束条件。
>
> - **资源需求**：检查节点上是否有足够的 CPU、内存、存储等资源来满足 Pod 的要求。
> - **节点亲和性**（Node Affinity）：如果 Pod 有指定的节点亲和性（例如，必须运行在某些标签的节点上），则调度器会筛选符合条件的节点。
> - **污点和容忍度**（Taints and Tolerations）：如果节点上有污点，Pod 需要有相应的容忍度才能被调度到该节点。否则，该节点会被排除。
> - **Pod 亲和性**（Pod Affinity/Anti-Affinity）：指定 Pod 应该或不应该与哪些其他 Pod 一起运行。调度器会检查当前集群中是否有符合条件的其他 Pod。
> - **节点状态**：调度器还会检查节点的状态，例如是否是 `Ready` 状态，节点是否被标记为不可调度（`NoSchedule`）。
>
> #### 2. **优选阶段（Prioritization）**
>
> 在这个阶段，调度器会对符合条件的节点进行排序。它根据预定义的优先级规则为每个节点打分，分数较高的节点将被优先选中。
>
> 一些常见的优先级因素包括：
>
> - **资源需求优先级**：如节点上的资源使用情况和 Pod 的资源请求是否匹配。
> - **亲和性优先级**：例如，Pod 是否应该运行在某个特定区域的节点上，或者与其他 Pod 的位置要求。
> - **负载均衡**：调度器会考虑当前节点上的负载，避免某些节点过载。
> - **调度延迟**：某些情况可能希望 Pod 尽早调度到节点，调度器会尽量减少调度延迟。
>
> #### 3. **选择节点并绑定 Pod**
>
> 经过预选和优选后，调度器会选择一个最适合的节点，并将 Pod 绑定到该节点。此时，Pod 状态变为 `Running`，并开始在节点上启动容器。
>
> ### **调度器的核心功能**
>
> Kubernetes 调度器（`kube-scheduler`）执行以下主要任务：
>
> - **节点选择**：选择合适的节点以确保 Pod 在资源、拓扑、策略等方面的需求得到满足。
> - **Pod 绑定**：一旦选定节点，调度器将 Pod 绑定到该节点，并通过 API 服务器更新 Pod 对象。
> - **调度策略和插件**：调度器可以通过插件化的方式来扩展调度策略，允许集群管理员根据业务需求定制调度行为。
> - **资源管理**：调度器会确保 Pod 的资源请求（CPU、内存、存储等）符合节点的资源情况，避免资源争抢。
>
> ### **调度策略**
>
> #### 1. **节点亲和性（Node Affinity）**
>
> Kubernetes 提供了节点亲和性（Node Affinity），让用户能够指定 Pod 应该在哪些节点上运行。Node Affinity 是一个更灵活的方式，类似于标签选择器，但支持更复杂的逻辑。
>
> - **requiredDuringSchedulingIgnoredDuringExecution**：表示 Pod 必须被调度到满足条件的节点上。=>硬策略
> - **preferredDuringSchedulingIgnoredDuringExecution**：表示 Pod 会尽量被调度到满足条件的节点上，但不是强制要求。=>软策略
>
> 例如，要求 Pod 只在 `zone=us-west1` 的节点上运行：
>
> ```yaml
> affinity:
>   nodeAffinity:
>     requiredDuringSchedulingIgnoredDuringExecution:
>       nodeSelectorTerms:
>         - matchExpressions:
>             - key: "zone"
>               operator: In
>               values:
>                 - "us-west1"
> ```
>
> #### 2. **Pod 亲和性（Pod Affinity）与反亲和性（Anti-Affinity）**
>
> Pod 亲和性和反亲和性允许你指定 Pod 在选择节点时与其他 Pod 的位置关系。Pod 亲和性要求 Pod 在某些特定条件下被调度到离特定 Pod 很近的节点上，而反亲和性则要求 Pod 不要与某些 Pod 放置在同一个节点或相近的节点。
>
> 例如，要求某个 Pod 在同一个节点上与另一个特定标签的 Pod 一起调度：
>
> ```yaml
> affinity:
>   podAffinity:
>     requiredDuringSchedulingIgnoredDuringExecution:
>       - labelSelector:
>           matchExpressions:
>             - key: "app"
>               operator: In
>               values:
>                 - "frontend"
>         topologyKey: "kubernetes.io/hostname"
> ```
>
> =>Node Affinity 是跟某个具体节点（即服务器）关联，Pod Affinity则是跟另一个具体的Pod相关联
>
> #### 3. **污点和容忍度（Taints and Tolerations）**
>
> 污点（Taints）是节点的一种属性，用于标记节点上的限制，使得只有具有容忍度（Tolerations）标签的 Pod 可以被调度到这些节点上。这个特性有助于将某些 Pod 调度到专门的节点（例如，临时节点、特定硬件的节点等）。
>
> - **Taint**：污点是节点的一个属性，通常会被添加到节点上，表示该节点只接受具有特定容忍度的 Pod。
> - **Toleration**：容忍度是 Pod 的一个属性，用于声明该 Pod 可以被调度到带有特定污点的节点上。
>
> 例如，某个节点有污点 `key=value:NoSchedule`，只有带有 `key=value` 容忍度的 Pod 可以调度到该节点上：
>
> ```yaml
> tolerations:
> - key: "key"
>   operator: "Equal"
>   value: "value"
>   effect: "NoSchedule"
> ```
>
> =>像master节点天生就打了 NoSchedule 污点，所以分配Pod全去node节点还不回来master节点
>
> =>除以上之外，还可以为Pod直接指定Node...即固定节点调度
>
> ### **总结**
>
> Kubernetes 集群调度的核心任务是将 Pod 调度到合适的节点上，以确保资源的高效利用、应用的高可用性和服务的稳定性。调度器通过以下几个主要阶段完成任务：
>
> - **预选阶段**：检查节点是否满足 Pod 的基本需求。
> - **优选阶段**：对满足条件的节点进行打分，并选择最佳节点。
> - **Pod 绑定**：将 Pod 绑定到选择的节点，并启动 Pod。
>
> 通过使用 **节点亲和性**、**Pod 亲和性**、**污点和容忍度**等高级调度策略，Kubernetes 使得调度过程更加灵活，能够根据不同的业务需求进行精细的资源分配。

### 安全 20241113

Kubernetes API Server是所有请求的入口，K8S的安全机制也是围绕此来展开的。

身份验证（Authentication)：

> Kubernetes的API服务器（API Server）是整个集群的核心组件，它负责处理外部和内部的所有请求。为了确保客户端与API服务器之间的通信是安全的，Kubernetes默认要求通过**HTTPS**（即HTTP over TLS）来访问API。所有API请求，包括身份验证请求，都会使用HTTPS来进行加密。

K8s内部组件访问API Server也有两种可能，一种是本机访问，第二种也是需要HTTPS双向认证的

授权（Authorization）：

Authentication只是确定通信双方是可信的，至于请求方有哪些资源的权限则取决于Authorization 鉴权。

> Kubernetes的授权机制用于控制用户或服务帐户对资源的访问权限，确保只有被授权的实体能够执行特定的操作。
>
> - **RBAC（Role-Based Access Control）**：RBAC是Kubernetes的主要授权机制，它根据角色（Role）和角色绑定（RoleBinding）来定义用户对资源的访问权限。通过RBAC，可以细粒度地控制用户、服务帐户、甚至外部系统对K8s资源的访问。
> - **ABAC（Attribute-Based Access Control）**：基于属性的访问控制，允许通过用户属性（如部门、环境）来授权访问。
> - **Webhook授权**：可以通过Webhooks将K8s的授权机制扩展到外部系统，进行自定义授权。

Role-Based Access Control是最为流行的，也是K8s的默认设置

> 在 Kubernetes 中，`Role` 和 `ClusterRole` 都是 **RBAC (Role-Based Access Control)** 的核心概念，它们用于定义在集群中对不同资源的访问权限。二者的主要区别在于作用域和应用范围。
>
> `ClusterRole` 的权限是**集群级别**的。它可以跨所有命名空间，或者指定集群范围的资源（如 Node、PersistentVolume、Namespace 等）。

=>都是通过YAML文件来定义的

=>Role-Based Access Control与AWS的IAM机制差不多

### Helm 20241115

> **Helm** 是 Kubernetes（K8s）中的一个包管理工具，它简化了应用的部署和管理过程。类似于 Linux 中的包管理器（如 apt 或 yum），Helm 让你能够轻松地安装、升级、删除和管理 Kubernetes 应用。
>
> ### Helm 的核心功能：
>
> 1. **简化部署：** Helm 通过将 Kubernetes 资源（如 Pods、Services、Deployments 等）打包成一个称为 **Chart** 的包，简化了应用的部署过程。你只需要一个命令就可以部署一个复杂的应用，而不需要手动管理大量的 YAML 文件。
> 2. **应用版本管理：** Helm 使得应用的版本控制变得容易。你可以通过 Helm 安装应用，并且能够轻松地升级到新版本，或者回滚到旧版本。
> 3. **模板化配置：** Helm 使用 Go 模板来生成 Kubernetes 配置文件，这样就可以根据不同的环境需求调整配置。通过这种方式，同一个 Helm Chart 可以在多个环境中重复使用，只需要修改配置文件中的一些变量。
> 4. **依赖管理：** Helm 支持在安装一个应用时自动安装它的依赖。例如，如果一个应用需要数据库或缓存服务，Helm 可以自动安装并配置这些依赖服务。
> 5. **方便的命令行工具：** Helm 提供了一套命令行工具来帮助用户管理 Kubernetes 应用，比如 `helm install`、`helm upgrade`、`helm uninstall` 等。
>
> ### Helm 的核心概念：
>
> - **Chart：** Helm 中的核心概念是 **Chart**，它是一个包含 Kubernetes 资源定义的包。每个 Chart 可以包含多个 YAML 文件，描述了一个应用的不同部分（如 Deployment、Service、Ingress 等）。
> - **Release：** 每次使用 Helm 部署一个 Chart 时，会创建一个 **Release**。每个 Release 都有一个名称，表示这个 Chart 在 Kubernetes 集群中的一个具体实例。
> - **Repository：** Helm Chart 可以存放在一个 Helm 仓库中，用户可以从仓库中下载并安装公开或私有的 Charts。
>
> ### Helm 的工作流程：
>
> 1. **安装 Helm：**
>    首先需要安装 Helm 客户端，可以在本地执行 Helm 命令。
>
> 2. **添加 Helm 仓库：**
>    可以添加官方或私有 Helm 仓库，来获取公开的 Chart。例如：
>
>    ```bash
>    helm repo add stable https://charts.helm.sh/stable
>    ```
>
> 3. **安装应用：**
>    使用 Helm 安装应用。例如，安装 Nginx Ingress Controller：
>
>    ```bash
>    helm install my-nginx stable/nginx-ingress
>    ```
>
> 4. **管理和升级应用：**
>    可以使用 Helm 来查看、升级或回滚已安装的应用。例如：
>
>    ```bash
>    helm list           # 查看已安装的 Helm Releases
>    helm upgrade        # 升级应用
>    helm rollback       # 回滚到上一个版本
>    ```
>
> 5. **删除应用：**
>    使用 Helm 删除已部署的应用：
>
>    ```
>    helm uninstall my-nginx
>    ```
>
> ### 举个例子：
>
> 假设你想安装一个 Nginx 应用，使用 Helm 可以这样操作：
>
> ```
> helm install my-nginx stable/nginx-ingress
> ```
>
> 这条命令会将 Nginx Ingress Controller 安装到 Kubernetes 集群中，并自动为它创建相关资源（如 Deployment、Service 等）。
>
> ### 总结：
>
> Helm 大大简化了 Kubernetes 应用的部署和管理，尤其是在需要部署多个复杂应用时，Helm 的模板化和版本控制功能使得配置更加灵活和可维护。对于开发者和运维人员来说，Helm 是 Kubernetes 环境中非常有用的工具。

一个简单的Chart 的结构如下：

```
myapp/
├── Chart.yaml          # Chart 的元数据文件
├── values.yaml         # 默认的配置文件
├── charts/             # 可选的依赖包
├── templates/          # 存放 Kubernetes 资源模板文件
│   ├── deployment.yaml # Deployment 模板
│   ├── service.yaml    # Service 模板
│   └── ingress.yaml    # Ingress 模板
└── README.md           # Chart 的文档说明
```

> **Go 模板语法**在 Helm 中的作用是将静态的 YAML 文件模板化，使得你可以根据不同的配置、条件和环境动态生成 Kubernetes 资源文件。

=>可以配置一个Kubernetes的网页Dashboard，就像AWS的界面一样

> Prometheus 是一个强大的开源监控工具，专门为分布式系统、容器化应用以及微服务架构设计。它被广泛应用于 Kubernetes 集群、云原生架构、容器监控等领域，是现代 DevOps 和云原生环境中不可或缺的监控工具之一。

=>Prometheus 最终也是个 网页GUI来监视 容器的各种数据，CPU占有率啥的，好像需要跟**Grafana** 结合

> **EFK** 是一个广泛使用的日志收集和分析栈，通常用于 Kubernetes 集群的日志管理。EFK 由三个主要组件组成：
>
> - **Elasticsearch**：用于存储和查询日志数据。
> - **Fluentd**：日志收集器，负责从各个容器和节点中收集日志，通常用于将日志从 Kubernetes 集群推送到 Elasticsearch。
> - **Kibana**：一个可视化界面，用于显示 Elasticsearch 中存储的日志数据，提供查询和分析的功能。

=>也可以说领用Helm可以下载许多组件给Kubernete添加上dashboard，监视工具以及日志工具

### 高可用k8s构建 20241118

无非就是 loader Balancer增加副本容错 

构建各节点的时候如同Ansible项目中所说的对linux 有各种 防火墙 内核参数修改 等细微操作

然后就是建立多个master节点，依次安装各个模块以及Docker，可以预想与Ansible结合起来用，对linux的操作命令行要很熟

## Ericsson AKS Demo Session 20250122

**Agenda**

- Introduction to Azure Kubernetes Service
- Managed Istio
- Open Service Mesh
- Managed Nginx Ingress in AKS
- GitOps
- Prometheus
- Service Connector (Preview)
- KEDA Addon
- Custom Resource Definition
- GitHub Copilot for AKS
- Application Gateway for Containers
- Azure Load Testing



Load Balancer Service Ingress Controllers - Layer 7 Load Balancing

Istio Gateway

Service Mesh -  Istio add-on for AKS (AKS提供的 Istio 插件，用于管理微服务之间的通信，让服务间流量管理、安全认证等更容易控制和观察) =>与Azure Managed Grafana, Azure Managed Prometheus一起工作

NGINX Ingress Controller - App Routing Addon（应用路由插件）

=>AKS有更多可以集成的插件给Ingress Controller，即插即用，给developer更好的使用体验，不用担心版本冲突啥的

Application Gateway for Containers  => AKS除了kubernetes一般具有的流量均衡负载，还可以提供集成的firewall

> **GitOps**（Git Operations）是一种**基础设施自动化和应用交付**的方法，它使用 **Git 作为单一事实来源（Single Source of Truth）**，并结合 **CI/CD 流程** 来自动化部署和管理 Kubernetes 等云原生环境。
>
> 简单来说，GitOps **把 Git 当作配置管理工具**，让开发团队可以像管理代码一样管理基础设施和应用部署。

 =>VScode中下载GitHub Copilot for Azure

> GitHub Copilot 是**AI 代码助手**，可以帮助开发者**更快地编写 Kubernetes 相关的 YAML、Helm Charts 以及 CI/CD 脚本**。

## Cloud Native 20250628

1. 什么是Cloud Native ：

   > **Cloud Native（云原生）** 是一种设计和运行应用程序的方法，目标是：
   >
   > - 弹性伸缩（scalable）
   > - 自动化运维（automated management）
   > - 微服务架构（microservices）
   > - 快速交付（fast release）
   >
   > Cloud Native 不是指“在云上运行”，而是指**以云的方式构建**，强调以下特征：
   >
   > | 特征                       | 说明                                     |
   > | -------------------------- | ---------------------------------------- |
   > | 容器化（Containerization） | 每个服务封装为可移植的容器。             |
   > | 微服务架构                 | 功能被拆解成独立服务，松耦合、独立部署。 |
   > | 动态编排（Orchestration）  | 系统根据负载动态部署与扩缩容。           |
   > | 可观察性                   | 日志、监控、追踪统一管理。               |
   > | 自动化 CI/CD               | 从构建、测试到部署的流水线全自动。       |

2. Kubernetes 与 Cloud Native的关系

> Kubernetes 是 Cloud Native 架构的核心基础设施和执行引擎。
>
> Kubernetes 实际上**提供了 Cloud Native 应用的运行时平台**，让用户可以在任意云环境下，像在云上一样运行服务。
>
> 没有 Kubernetes，Cloud Native 很难落地；有了 Kubernetes，Cloud Native 的理念才能真正实现。

3. Cloud与Cloud Native的区别

> **云（Cloud）是平台，Cloud Native 是方法。**
>  云提供了资源，**Cloud Native 定义了如何高效使用这些资源来构建现代应用。**
>
> ## 🌥️ 什么是“云”？
>
> - 指通过互联网提供的计算资源（如服务器、数据库、存储等）。
> - 有三种主要模式：IaaS、PaaS、SaaS。
> - 云是“**你不再拥有，而是租用并远程使用**”的计算平台。
>
> 想象你开了一家餐厅：
>
> - **云**：你租了一整套厨房设备（别人已经建好，只需使用）；
> - **Cloud Native**：你用自动点菜、智能配送、分工合作、实时监控的方式高效运行这家餐厅。
>
> 云是 **“在哪里运行”**，Cloud Native 是 **“如何构建和运行”**。|
>
> > 拥有“云”并不意味着你就“云原生”了。只有当你的系统具备弹性、自愈、自动化能力，才真正走向 Cloud Native。

4. CNCF

> **CNCF** 全称是 **Cloud Native Computing Foundation**，中文名为**云原生计算基金会**。它是一个隶属于 Linux 基金会的非营利性开源组织，成立于 **2015 年**，旨在推动云原生技术的普及与标准化。
>
> ## 🧩 CNCF 的主要职责
>
> 1. **孵化和维护开源项目**
>     如：
>    - 🌐 **Kubernetes**（容器编排）
>    - 📦 **Prometheus**（监控）
>    - 🕸️ **Envoy**（服务代理）
>    - 🔀 **gRPC**（远程调用）
>    - 🛡️ **OPA**（策略控制）等。
> 2. **推动云原生理念的发展**
>    - 支持微服务、容器化、可观测性、自动化运维等实践。
>    - 提倡使用**可弹性伸缩、易于管理的现代应用架构**。
> 3. **举办社区活动和认证**
>    - KubeCon 是 CNCF 主办的全球最大 Kubernetes 会议。
>    - 提供认证项目：
>      - CKA（Kubernetes 管理员认证）
>      - CKAD（应用开发者认证）
>      - KCNA（云原生入门认证）等。

# Docker

## 尚硅谷Docker实战教程

[Link](https://www.bilibili.com/video/BV1gr4y1U7CY/?spm_id_from=333.337.search-card.all.click&vd_source=c13700902d2c98df282b6f1f2889c0cb)

### 教程简介 20241127

1. Docker简介
2. Docker安装
3. Docker常用命令
4. Docker镜像
5. 本地镜像发布到阿里云
6. 本地镜像发布到私有库
7. Docker容器数据卷
8. Docker常规安装简介

进阶：

1. Docker复杂安装详说
2. DockerFile解析
3. Docker微服务实战
4. Docker网络
5. Docker-compose容器编排
6. Docker轻量级可视化工具Portainer
7. Docker容器监控之CADvisor+InfulxDB+Granfana



### Docker简介 20241211

系统环境平滑移植

从系统环境开始，自底至上打包应用。像虚拟机一样一个iso文件搞定一切。

=>先安装Docker，然后用这个工具运行iso文件就行

传统虚拟机 启动太慢，占用资源又多，从而优化发展出来 容器虚拟化技术

官网 + 镜像仓库

Dockers必须部署在Linux内核的系统上。如果Windows上部署Docker的方法都是先安装一个虚拟机

> Docker Desktop for Windows利用 WSL2 提供一个 Linux 内核环境，Docker 的容器和镜像实际运行在这个 WSL2 环境中。

Docker三要素：镜像image，容器container，仓库repository =>容器可以看成是镜像的实例化

> **WSL**：WSL通过操作系统级别的虚拟化来运行Linux环境。特别是WSL 1，它通过在Windows上模拟Linux内核的系统调用来运行Linux应用程序，而WSL 2引入了一个轻量级的虚拟机来运行一个完整的Linux内核，但它并不是模拟整个计算机硬件的虚拟机，而是一个虚拟化的Linux内核环境。
>
> **虚拟机**：虚拟机是硬件级别的虚拟化，每个虚拟机运行一个完整的操作系统（包括独立的内核），并且模拟了计算机硬件的所有部分，如CPU、内存、硬盘和网络等。
>
> **WSL更接近于操作系统级虚拟化，而虚拟机则是硬件级虚拟化**，二者的虚拟化方式不同。
>
> **WSL**（Windows Subsystem for Linux）更接近于**容器虚拟化技术**，尤其是WSL 2，它与容器化技术在一些方面有相似之处。

Docker就是专门用于CICD部署的，而WSL更偏向一个虚拟化开发环境

可以把容器看做一个简易版的Linux环境(包括root用户极限，进程空间，用户空间和网络空间等)和运行在其中的应用程序。

注意 Docker daemon的概念所在的位置，它在container之外

> Docker 的架构遵循了典型的客户端-服务器C/S模式，主要由 **Docker 客户端** 和 **Docker 守护进程（Daemon）** 两部分组成。

Docker运行run命令，现在本地寻找相关镜像，没有就去Hub上寻找并下载



Docker为什么比虚拟机快：

1. docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化，运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。
2. docker利用的是宿主机的内核，而不需要加载操作系统OS内核。新建虚拟机是几分钟级别的，而新建docker就是几秒钟

虚拟机：OS => Hypervisor => OS =>Appllication

Dockers：OS => Docker => Application



### Docker常用命令 20241213

```bash
#帮助启动类命令
sudo systemctl start docker
sudo systemctl stop docker
sudo systemctl restart docker
sudo systemctl enable docker
#镜像命令
docker images
docker pull <镜像名>:<标签>
#容器命令
docker run -d -p <宿主端口>:<容器端口> --name <容器名> <镜像名>
docker run -d -p 8080:80 --name my-nginx nginx #示例
docker ps
docker start <容器ID或名称> #启动已停止运行的容器
docker stop <容器ID或名称>
docker rm <容器ID或名称>
docker exec -it <容器ID或名称> /bin/bash #进入后台式容器
docker cp <容器ID>:<容器路径> <本地路径> #从容器中复制文件到本地
```

docker run之后就进入docker容器的交互行了，ls一下好像linux什么都齐全，但是比如vim这些命令肯定就没有了，不是Linux内核的东西docker不会装上去，尽可能瘦身。

两种退出方式：

1. exit，run进去容器，exit退出，容器停止
2. ctrl+p+q   run进去容器，ctrl+p+q退出，容器不停止

### Docker镜像 20241218

>  **UnionFS (Union File System)** 是一种分层文件系统，它是容器镜像的基础技术之一。

> ### **1. 什么是 bootfs 和 rootfs？**
>
> 在容器的文件系统中，有两个核心概念：
>
> - **bootfs (Boot File System)**：类似于传统 Linux 系统中的内核文件系统，包括内核以及初始化系统（initrd/initramfs）。它为容器提供最基本的运行环境。
> - **rootfs (Root File System)**：Linux 的根文件系统，包含 `/bin`, `/etc`, `/usr`, `/var` 等目录和文件。它是容器用户态的运行环境。
>
> 在容器启动时，**bootfs** 会加载内核和基本运行环境，随后会被卸载，而容器主要运行在 **rootfs** 上。
>
> ### **2. UnionFS 的简单结构**
>
> UnionFS 的核心是分层的文件系统结构。在 Docker 中：
>
> - 每个镜像是由多个层组成的，底层是基础镜像（如 `alpine` 或 `ubuntu`），上层是用户修改或添加的文件。
> - 容器启动后，会在镜像顶部创建一个**可写层**。
>
> 一个简单的 UnionFS 结构如下：
>
> ```
> /bootfs   (已卸载，仅用于启动)
> /rootfs
>  ├── Layer 1 (基础镜像层，read-only)
>  ├── Layer 2 (安装了软件包的增量层，read-only)
>  ├── Layer 3 (用户配置层，read-only)
>  └── Writeable Layer (容器的可写层)
> ```
>
> ### **3. 逐步分析一个简单的 UnionFS 案例**
>
> #### **(1) 基础镜像层**
>
> 假设一个容器以 `ubuntu` 镜像为基础：
>
> - 第一层是基础镜像，提供了最小化的根文件系统（如 `/bin/bash`）。
> - 这是一个只读层，不会因容器的运行而改变。
>
> 内容示例：
>
> ```
> bashCopy codeLayer 1:
>   /bin/bash
>   /etc/passwd
> ```
>
> #### **(2) 用户修改或安装层**
>
> 在构建镜像时，用户可能添加了新的软件或文件，这些更改会形成新的只读层：
>
> - 用户通过 `RUN apt-get install` 或其他命令安装了额外的软件。
> - 生成第二层，记录增量修改（如新文件或更改的文件）。
>
> 内容示例：
>
> ```
> rubyCopy codeLayer 2:
>   /usr/bin/python3
>   /etc/apt/sources.list
> ```
>
> #### **(3) 容器可写层**
>
> 当容器运行时：
>
> - 系统为镜像添加一个可写层，所有的写操作都会在这层执行。
> - 任何修改（如新增文件、修改配置）都只影响这一层，不会改变底层的只读层。
>
> 内容示例：
>
> ```
> bashCopy codeWriteable Layer:
>   /tmp/newfile.txt
>   /etc/hostname  (覆盖 Layer 1 的文件)
> ```
>
> ------
>
> ### **4. UnionFS 的合成文件系统**
>
> 容器内实际看到的文件系统是由 UnionFS 合成的：
>
> - 用户只会看到一个整体的文件系统。
> - 查找文件时，UnionFS 会从顶部向底层逐层搜索，找到第一个匹配的文件。
>
> 最终合成结果：
>
> ```
> bashCopy code/
> ├── /bin/bash          (来自 Layer 1)
> ├── /usr/bin/python3   (来自 Layer 2)
> ├── /tmp/newfile.txt   (来自 Writeable Layer)
> ├── /etc/passwd        (来自 Layer 1)
> └── /etc/hostname      (来自 Writeable Layer，覆盖 Layer 1)
> ```
>
> ------
>
> ### **5. 细节：写时复制（Copy-on-Write）**
>
> 当容器尝试修改一个来自只读层的文件时，UnionFS 会执行 **写时复制**：
>
> 1. 将目标文件从只读层复制到可写层。
> 2. 修改文件的副本，原始文件保持不变。
>
> 例如：
>
> - 修改 `/etc/hostname` 时，原始文件在 Layer 1 保持不变。
> - 可写层创建 `/etc/hostname` 的副本，容器只会看到副本的修改结果。
>
> ------
>
> ### **6. 总结**
>
> 通过 **bootfs** 和 **rootfs** 的分离，以及 UnionFS 的分层机制，Docker 实现了以下优势：
>
> 1. **快速启动**：bootfs 仅用于加载内核，随即卸载，rootfs 提供用户态环境。
> 2. **高效存储**：分层文件系统减少了重复存储。
> 3. **写时隔离**：容器的修改仅影响可写层，不会污染底层镜像。
>
> 如果有进一步的问题，可以深入讨论 UnionFS 的实现细节或扩展到具体文件系统（如 OverlayFS）的案例！

=>下载一个Ubuntu镜像，用apt下载vim，用docker commit提交容器副本使之成为本地的一个新的镜像，最简单的Docker镜像70M,加了一个vim就double了...跟git一样还可以docker push推送到阿里云上去



### Docker容器数据卷 20250116

> Docker 容器中的数据卷（Volumes）是 Docker 用于实现数据持久化和容器之间数据共享的一种机制。数据卷的主要特点是它独立于容器的生命周期，可以用于存储数据，即使容器被删除，数据卷中的数据依然保留。

将docker容器内的数据保存进宿主机的磁盘中

=> 数据卷可以挂载到宿主机某一个路径上，数据卷与docker互相反映数据变化，比如创建一个文件什么的，双向影响

=>Volumes 与其看作是个“盘”，不如说看成一种 挂载的 映射关系，所谓容器B继承容器A的Volumes其实就是获取这个 映射关系，所以即便容器A挂了，也不影响容器B继续使用这个 映射关系



dockerhub上搜索 tomcat,mysql，拉下来创建实例，配置端口，运行

### Dockerfile简介 20250129

相当于linux的shell脚本，作用与多次提交commit形成的新镜像一样 

> **Dockerfile** 是一个文本文件，包含一系列指令，用于自动化构建 Docker 镜像。通过 Dockerfile，用户可以定义镜像的构建步骤，包括基础镜像的选择、软件安装、文件复制、环境变量设置等。Docker 根据 Dockerfile 中的指令逐步构建镜像。

以下是一个简单的 Dockerfile 示例，用于构建一个运行 Python 应用的镜像：

```dockerfile
# 使用官方 Python 镜像作为基础镜像
FROM python:3.8-slim

# 设置工作目录，终端默认进入的落脚点
WORKDIR /app

# 复制当前目录内容到容器的 /app 目录
COPY . /app

# 安装依赖
# RUN 等同于，在终端操作的shell命令
RUN pip install --no-cache-dir -r requirements.txt

# 暴露端口
EXPOSE 80

# 设置环境变量
ENV NAME World

# 运行应用
CMD ["python", "app.py"]
```

使用以下命令构建镜像：

```shell
#docker build 是 Docker 提供的命令行工具，用于根据 Dockerfile 中的指令构建镜像。
docker build -t my-python-app
docker run -p 4000:80 my-python-app
```



常用保留字指令均是大写，如 FROM

每条指令都会创建一个新的镜像层对镜像进行提交

dockerhub上拉下来的镜像基本上最后都会有一个dockerfile作为理论支持

> `FROM` 指令中的镜像查找顺序如下：
>
> 1. 本地镜像缓存
> 2. Docker Hub（默认）
> 3. 私有镜像仓库（如果指定了仓库地址）
> 4. 其他公共镜像仓库（如 Quay.io、GCR、ECR 等）
> 5. 本地构建的镜像

ADD命令可以直接将本地下载好的比如jdk直接打入docker...牛逼

### Docker network 20250131

> ### **1. 什么是网络接口？**
>
> 网络接口（Network Interface）是设备（如计算机、服务器、路由器等）与网络之间进行数据传输的硬件或软件组件。它可以是物理的（如网卡）或虚拟的（如虚拟网桥、虚拟以太网接口）。
>
> - **物理网络接口**：通常是硬件设备，如以太网卡（`ens33`）、无线网卡（`wlp2s0`）。
> - **虚拟网络接口**：由软件实现，如虚拟网桥（`docker0`）、虚拟以太网接口（`veth`）、回环接口（`lo`）。
>
> ### **2. 网络接口的作用**
>
> 网络接口的主要作用是：
>
> 1. **数据传输**：在网络中发送和接收数据包。
> 2. **地址分配**：为设备分配 IP 地址和 MAC 地址。
> 3. **协议支持**：支持网络协议（如 TCP/IP）的实现。
> 4. **网络隔离**：通过虚拟接口实现网络隔离（如 Docker 容器网络）。
>
> ------
>
> ### **3. 网络接口的物理图例**
>
> 以下是一个简单的网络拓扑图，展示了不同类型的网络接口及其作用：
>
> ```
> +-------------------+       +-------------------+       +-------------------+
> |    Host Machine   |       |    Docker Host    |       |   Remote Server   |
> |                   |       |                   |       |                   |
> |  +-------------+  |       |  +-------------+  |       |  +-------------+  |
> |  |   ens33     |  |       |  |   docker0    |  |       |  |   eth0      |  |
> |  | (Physical)  |  |       |  | (Virtual)    |  |       |  | (Physical)  |  |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |  |   Switch    |  |       |  |   veth       |  |       |  |   Router    |  |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |        |          |       |        |          |       |        |          |
> |  +-----+-------+  |       |  +-----+-------+  |       |  +-----+-------+  |
> |  |   Internet  |  |       |  |   Container |  |       |  |   Internet  |  |
> |  +-------------+  |       |  +-------------+  |       |  +-------------+  |
> +-------------------+       +-------------------+       +-------------------+
> ```

ens33就是之前的额eth0，eth1一类的，当然不止这三种网络接口，用ipconfig打出来的信息可以看

> 在 Docker 容器网络中，`veth`（Virtual Ethernet Device，虚拟以太网设备）和 `docker0`（默认的虚拟网桥）一起工作，实现了容器与宿主机及其他容器的网络通信。它们的关系可以理解为 **“桥接网络中的虚拟网卡”** 和 **“连接这些网卡的交换机”**。
>
> `docker0` 是 Docker 默认创建的 **Linux 网桥（bridge）**，相当于一个二层交换机，负责连接所有 Docker 容器的网络接口，并提供它们之间的通信。
>
> - 默认情况下，`docker0` 具有一个分配的网段（例如 `172.17.0.1/16`）。
> - 容器之间的通信通过 `docker0` 进行转发，类似物理网络中的交换机。
> - 宿主机可以通过 `docker0` 访问所有连接到该网桥的容器。
>
> `veth` 是 **一对虚拟网卡（veth pair）**，用来连接 Linux 网络命名空间（Network Namespace）。它的特点是：
>
> - `veth` 总是成对出现（`vethX` 和 `vethY`），类似网线的两端。
> - 一端（`vethX`）连接到 `docker0` 网桥，另一端（`vethY`）放入容器的网络命名空间，充当容器的 `eth0`。
> - `veth` 设备可以传输数据，类似物理网卡+网线。



**Docker 网络模式**

| **网络模式** | **bridge**                           | **host**                 | **none**         |
| :----------- | :----------------------------------- | :----------------------- | :--------------- |
| **描述**     | 默认模式，使用 `docker0` 网桥        | 容器直接使用主机的网络栈 | 容器没有网络接口 |
| **网络隔离** | 是                                   | 否                       | 是               |
| **IP 地址**  | 容器分配独立的 IP（如 `172.17.0.2`） | 使用主机的 IP 地址       | 无 IP 地址       |

> - **`bridge` 模式**：
>   - 默认模式，容器通过 `docker0` 网桥与主机和其他容器通信。
>   - 需要手动映射端口（如 `-p 8080:80`）。
>   - 适用于多容器通信的场景。
> - **`host` 模式**：
>   - 容器直接使用主机的网络栈，性能较高。
>   - 不需要端口映射，容器端口直接绑定到主机端口。
>   - 适用于高性能需求的场景。
> - **`none` 模式**：
>   - 容器没有网络接口，完全隔离网络。
>   - 适用于不需要网络访问的场景。

> - `bridge` 模式是默认模式，适合大多数场景。
> - `host` 模式性能高，但安全性较低。
> - `none` 模式完全隔离网络，适合特殊需求。

=>bridge模式使用docker0网桥 是主流？

=>对一个docker容器而言，他有可能有两个网络接口，除了回环之外，eth0就是docker给它单独分配的！！这也是容器内部监视 *:5000 与 localhost:5000不同的原因！(2025.2.16)

```
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
```

还有container模式，就是多个container公用一个虚拟网卡eth0与bridge连接。

还可以自定义网络模式，好像可以指定主机名与ip对应，而不是像bridge那样自动分配

### Docker compose 20250203

> **Docker Compose** 是一个用于定义和运行多容器 Docker 应用程序的工具。它通过一个 **YAML 文件**（通常命名为 `docker-compose.yml`）来描述应用程序的 **服务、网络和数据卷**，并且可以使用 **一条命令** (`docker-compose up`) 来启动或管理所有容器。
>
> 以下是一个 `docker-compose.yml` 示例
>
> ```yaml
> version: '3.8'
> 
> services:
>   web:
>     image: python:3.9
>     container_name: flask_app
>     working_dir: /app
>     volumes:
>       - .:/app
>     ports:
>       - "5000:5000"
>     command: python app.py
>     depends_on:
>       - db
> 
>   db:
>     image: mysql:5.7
>     container_name: mysql_db
>     restart: always
>     environment:
>       MYSQL_ROOT_PASSWORD: root
>       MYSQL_DATABASE: mydb
>       MYSQL_USER: user
>       MYSQL_PASSWORD: password
>     ports:
>       - "3306:3306"
> ```
>
> 启动所有服务: docker-compose up



> **Docker Compose** 主要用于 **单机** 上管理多个 Docker 容器。
>
> **Kubernetes（K8s）** 主要用于 **分布式环境**，管理多个节点上的容器。

Docker Compose用于本地开发、测试环境，而K8s生产级容器编排



### Portainer 与 CIG 20250204

> Portainer 是一个轻量级的容器管理工具，旨在简化 Docker环境的管理。它提供了一个直观的 Web UI，使用户能够方便地部署、管理和监控容器应用，而无需使用复杂的命令行工具。

=>windows不是直接有gui界面嘛...

Portainer 就是轻量级，重量级的监控系统就要用CAdvisor+InfulxDB+Granfana

> ### **① cAdvisor（Container Advisor）**
>
> - **功能**：由 Google 开发的开源工具，专门用于收集容器的资源使用情况（CPU、内存、磁盘 I/O、网络流量等）。
>   - 轻量级，直接运行在宿主机上
>   - 原生支持 Docker，无需额外配置
>   - 提供 REST API，支持 Prometheus、InfluxDB 等多种后端存储
>
> ### **InfluxDB（时序数据库）**
>
> - **功能**：一个高效的时序数据库，专门用于存储时间序列数据，如指标、事件、日志等。
>   - 适用于高吞吐量写入的场景
>   - 支持 SQL 类似的查询语言（InfluxQL）
>   - 可与 cAdvisor、Telegraf、Grafana 轻松集成
>
> ### **③ Grafana（数据可视化）**
>
> - **功能**：一个强大的数据可视化工具，可连接多个数据源（InfluxDB、Prometheus、MySQL 等）并创建自定义监控面板。
>   - 直观的仪表盘（Dashboard）
>   - 灵活的数据查询和告警功能
>   - 多种插件支持（Graphite、InfluxDB、Prometheus、Elasticsearch）

cAdvisor只能存储2分钟的数据，所以需要InfluxDB

**cAdvisor、InfluxDB 和 Grafana** 都可以运行在容器中，通常使用 **Docker 容器** 部署它们，以实现快速安装、隔离环境和便捷管理。最后用 **Docker Compose** 或 **Kubernetes (K8s)** 来管理这些容器，使它们能够自动启动、重启并相互连接。

为Granfana配置来自InfluxDB的数据源...



# JIRA

> Jira 和 Azure DevOps Boards 都是支持敏捷开发的项目管理工具，提供如工作项管理、看板视图、冲刺规划、Backlog 管理等核心功能，适用于敏捷团队的日常协作。两者的主要区别在于侧重点不同：**Jira** 更灵活，支持高度自定义的工作流和丰富的报表，适合产品管理和跨部门协作；而 **Azure DevOps Boards** 深度集成代码库、构建、部署和测试，贴合开发流程，适合以开发为中心的团队，尤其是在使用 Azure Pipelines 或 Repos 的情况下更具优势。

## Azure DevOps Boards 20250407

Azure Boards 中的层级结构:

```
Epic  # 史诗,代表 大型目标或业务需求,通常跨越多个团队、多个冲刺甚至多个版本
└── Feature # Epic 的组成部分，代表一个可交付的、有业务价值的子目标
       └── Task # 进一步细化，用于开发、测试、设计等具体工作
```

左侧菜单（Boards、Backlogs、Work Items 等）其实是从不同角度来查看和管理工作项（Work Items）:

| 菜单项     | 作用       | 视角       | 适合人群               |
| ---------- | ---------- | ---------- | ---------------------- |
| Work Items | 快速查找   | 单个工作项 | 所有人                 |
| Boards     | 状态看板   | 状态流转   | 开发、测试             |
| Backlogs   | 层级规划   | 需求结构   | 产品经理、Scrum Master |
| Sprints    | 冲刺管理   | 任务执行   | Scrum 团队             |
| Queries    | 自定义搜索 | 灵活分析   | PM、QA、技术负责人     |

=>所以菜单项针对不同的人群哈，Work Items是最核心的，你增加一个Feature，可以没有父节点，即不必attach到某一个Epic上去，你在Backlogs可以用转到 Feature视角你就可以看到这个Feature的

=>另外不仅是Work Items，你在Backlogs，Sprints里均可以增加许多层级的任务，然后在其他菜单中也会关联显示出来。其底层都是各种任务，不同的菜单就是不同筛选不同视角的呈现而已！
